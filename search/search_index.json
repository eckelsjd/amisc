{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"changelog/","title":"Changelog","text":"<p>Changes should automatically show up here when using <code>cz bump</code>.</p>"},{"location":"changelog/#v040-2024-08-29","title":"v0.4.0 (2024-08-29)","text":""},{"location":"changelog/#feat","title":"Feat","text":"<ul> <li>migrate to copier-numpy template</li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing-to-amisc","title":"Contributing to amisc","text":"<p>You might be here if you want to:</p> <ul> <li>Report a bug</li> <li>Discuss the current state of the code</li> <li>Submit a fix</li> <li>Propose a new feature</li> <li>Write unit tests</li> <li>Add to the documentation.</li> </ul> <p>We use Github to host code and documentation, to track issues and feature requests, and to accept pull requests.</p>"},{"location":"contributing/#submitting-pull-requests","title":"Submitting pull requests","text":"<p>Pull requests are the best way to propose changes to the codebase (bug fixes, new features, docs, etc.)</p> <ol> <li>Fork the repo and create a branch from <code>main</code>.</li> <li>If you are adding a feature or making major changes, first create the issue in Github.</li> <li>If you've added code that should be tested, add to <code>/tests</code>.</li> <li>If you've made major changes, update the <code>/docs</code>.</li> <li>Ensure the test suite passes (<code>pdm run test</code>).</li> <li>Follow Conventional commits guidelines when adding a commit message.</li> <li>Ensure all <code>pre-commit</code> checks pass. Pro tip: use <code>pdm lint</code> to help.</li> <li>Issue the pull request!</li> </ol> <p>Use pdm to set up your development environment. An example contribution workflow is shown here:</p> <pre><code># Fork the repo on Github\ngit clone https://github.com/&lt;your-user-name&gt;/amisc.git\ncd amisc\npdm install\ngit checkout -b &lt;your-branch-name&gt;\n\n# Make local changes\n\npdm run test  # make sure tests pass\ngit add -A\ngit commit -m \"fix: adding a bugfix\"\ngit push -u origin &lt;your-branch-name&gt;\n\n# Go to Github and \"Compare &amp; Pull Request\" on your fork\n# For your PR to be merged:\n  # squash all your commits on your branch (interactively in an IDE most likely)\n  # rebase to the top of origin/main to include new changes from others\n\ngit fetch\ngit rebase -i main your-branch  # for example\n\n# Resolve any conflicts\n# Your history now looks something like this:\n#              o your-branch\n#             /\n# ---o---o---o main\n\n# You can delete the branch and fork when your PR has been merged!\n</code></pre> <p>You can also find a good tutorial here.</p>"},{"location":"contributing/#report-bugs-using-issues","title":"Report bugs using issues","text":"<p>Open a new issue and describe your problem using the template. Provide screenshots where possible and example log files. Add labels to help categorize and describe your issue.</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the GPL-3.0 license.</p>"},{"location":"coverage/","title":"Coverage report","text":""},{"location":"how-to-guides/","title":"How-to Guides","text":""},{"location":"how-to-guides/#specifying-model-inputs-and-outputs","title":"Specifying model inputs and outputs","text":"<p>Coming soon.</p>"},{"location":"how-to-guides/#defining-a-component","title":"Defining a component","text":"<p>Coming soon.</p>"},{"location":"how-to-guides/#making-a-model-wrapper-function","title":"Making a model wrapper function","text":"<p>The examples in the tutorial use the simple function call signatures <code>ret = func(x)</code>, where <code>x</code> is an <code>np.ndarray</code> and  <code>ret</code> is a dictionary with the required <code>y=output</code> key-value pair. If your model must be executed outside of Python (such as in a separate <code>.exe</code> file), then you can write a Python wrapper function with the same call signature as above and make any external calls you need inside the function (such as with <code>os.popen()</code>). You then pass the wrapper function to <code>ComponentSpec</code> and <code>SystemSurrogate</code>.</p> <p>Requirements for your wrapper function</p> <ul> <li>First argument <code>x</code> must be an <code>np.ndarray</code> of the model inputs whose last dimension is the number of inputs, i.e. <code>x.shape[-1] = x_dim</code>.</li> <li>You can choose to handle as many other dimensions as you want, i.e. <code>x.shape[:-1]</code>. The surrogate will handle the same number    of dimensions you give to your wrapper function (so that <code>model(x)</code> and <code>surrogate(x)</code> are functionally equivalent). We recommend you handle at least    1 extra dimension, i.e. <code>x.shape = (N, x_dim)</code>. So your wrapper must handle <code>N</code> total sets of inputs at a time. The easiest way is to just    write a for loop over <code>N</code> and run your model for a single set of inputs at a time.</li> <li>Your wrapper function must expect the <code>x_dim</code> inputs in a specific order according to how you defined your system. All   system-level exogenous inputs (i.e. those in <code>system.exo_vars</code>) must be first and in the order you specified for   <code>ComponentSpec(exo_in=[first, second, ...])</code>. All coupling inputs that come from the outputs of other models are next.   Regardless of what order you chose in <code>ComponentSpec(coupling_in=[one, two, three,...]</code>, your wrapper must expect them   in sorted order according to <code>system.coupling_vars</code>. For example, if <code>system.coupling_vars = [a, b, c]</code> and    <code>comp = ComponentSpec(wrapper, coupling_in=[c, a], exo_in=[d, e], coupling_out=[f])</code>, then <code>x_dim = 4</code> and your <code>wrapper</code> function   should expect the inputs in <code>x</code> to be ordered as <code>[d, e, a, c]</code>.</li> <li>If you want to pass in model fidelity indices (see \\(\\alpha\\) in theory for details), they must be in the form of a <code>tuple</code>,   and your wrapper function should accept the <code>alpha=...</code> keyword argument. Specifying <code>alpha</code> allows managing a hierarchy of modeling fidelities, if applicable.</li> <li>You can pass any number of additional positional arguments. Specify these with <code>ComponentSpec(model_args=...)</code>.</li> <li>You can pass any number of keyword arguments. Specify these with <code>ComponentSpec(model_kwargs=...)</code>.</li> <li>If you want to save and keep track of the full output of your model (i.e. if it writes result files to disk), then   you can specify <code>ComponentSpec(save_output=True)</code>. When you do this, you must also specify <code>SystemSurrogate(..., save_dir='path/to/save/dir')</code>.   You will then get a folder called <code>save_dir/amisc_timestamp/components/&lt;your_model_name&gt;</code>. This folder will be passed to your   wrapper function as the keyword argument <code>output_dir=&lt;your_model_dir&gt;</code>. Make sure your <code>wrapper</code> accepts this keyword (no need to specify it in <code>ComponentSpec(model_kwargs=...)</code>; this is done automatically).   You can then have your model write whatever it wants to this folder. You must then pass back the names of the files   you created via <code>ret=dict(files=[your output files, ...])</code>. The filenames must be in a list and match the order in   which the samples in <code>x</code> were executed by the model.</li> <li>To assist the adaptive training procedure, you can also optionally have your model compute and return its computational cost via   <code>ret=dict(cost=cpu_cost)</code>. The computational cost should be expressed in units of seconds of CPU time (not walltime!) for one model evaluation.   If your model makes use of <code>n</code> CPUs in parallel, then the total CPU time would be <code>n</code> times the wall clock time.</li> <li>The return dictionary of your wrapper can include anything else you want outside of the three fields <code>(y, files, cost)</code> discussed here.   Any extra return values will be ignored by the system.</li> </ul> <p>Example</p> <pre><code>def wrapper_func(x, *args, alpha=(0,), output_dir=None, **kwargs):\n    print(x.shape)  # (..., x_dim)\n\n    # Your code here, for example:\n    output = x ** 2\n    output_files = ['output_1.json', 'output2.json', ...]\n    cpu_time = 42  # seconds for one model evaluation\n\n    ret = dict(y=output, files=output_files, cost=cpu_time)\n\n    return ret\n</code></pre> <p>Warning</p> <p>Always specify the model at a global scope, i.e. don't use <code>lambda</code> or nested functions. When saving to file, only a symbolic reference to the function signature will be saved, which must be globally defined when loading back from that save file.</p>"},{"location":"how-to-guides/#putting-it-all-together","title":"Putting it all together","text":"<p>Coming soon.</p>"},{"location":"start/","title":"Getting started","text":"<p>Efficient framework for building surrogates of multidisciplinary systems using the adaptive multi-index stochastic collocation (AMISC)  technique.</p>"},{"location":"start/#installation","title":"\u2699\ufe0f Installation","text":"<p><pre><code>pip install amisc\n</code></pre> If you are using pdm in your own project, then you can use: <pre><code>pdm add amisc\n\n# Or in editable mode from a local clone...\npdm add -e ./amisc --dev\n</code></pre></p>"},{"location":"start/#quickstart","title":"\ud83d\udccd Quickstart","text":"<pre><code>import numpy as np\n\nfrom amisc.system import SystemSurrogate, ComponentSpec\nfrom amisc.rv import UniformRV\n\ndef fun1(x):\n    return dict(y=x * np.sin(np.pi * x))\n\ndef fun2(x):\n    return dict(y=1 / (1 + 25 * x ** 2))\n\nx = UniformRV(0, 1, 'x')\ny = UniformRV(0, 1, 'y')\nz = UniformRV(0, 1, 'z')\nmodel1 = ComponentSpec(fun1, exo_in=x, coupling_out=y)\nmodel2 = ComponentSpec(fun2, coupling_in=y, coupling_out=z)\n\ninputs = x\noutputs = [y, z]\nsystem = SystemSurrogate([model1, model2], inputs, outputs)\nsystem.fit()\n\nx_test = system.sample_inputs(10)\ny_test = system.predict(x_test)\n</code></pre>"},{"location":"start/#contributing","title":"\ud83c\udfd7\ufe0f Contributing","text":"<p>See the contribution guidelines.</p>"},{"location":"start/#reference","title":"\ud83d\udcd6 Reference","text":"<p>AMISC paper [1].</p> <p><sup><sub>Made with the copier-numpy template.</sub></sup></p>"},{"location":"theory/","title":"Overview","text":"<p>Coming soon.</p>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#single-component-example","title":"Single component example","text":"<p>Here is an example of interpolating a simple quadratic function. amisc.examples.tutorial.py<pre><code>from amisc.rv import UniformRV\nfrom amisc.system import ComponentSpec, SystemSurrogate\n\ndef fun(x):\n    return dict(y=x ** 2)\n\nx = UniformRV(-1, 1)\ny = UniformRV(0, 1)\ncomponent = ComponentSpec(fun)\nsystem = SystemSurrogate([component], x, y)\n\nsystem.fit()\nsystem.predict(0.5)  # 0.25\n</code></pre></p>"},{"location":"tutorials/#two-component-system","title":"Two component system","text":"<p>Here is a simple example of a two-component multidisciplinary system. amisc.examples.tutorial.py<pre><code>import numpy as np\n\nfrom amisc.rv import UniformRV\nfrom amisc.system import ComponentSpec, SystemSurrogate\n\ndef fun1(x):\n    return dict(y=x * np.sin(np.pi * x))\n\ndef fun2(x):\n    return dict(y=1 / (1 + 25 * x ** 2))\n\nx = UniformRV(0, 1, 'x')\ny = UniformRV(0, 1, 'y')\nz = UniformRV(0, 1, 'z')\nmodel1 = ComponentSpec(fun1, exo_in=x, coupling_out=y)\nmodel2 = ComponentSpec(fun2, coupling_in=y, coupling_out=z)\n\ninputs = x\noutputs = [y, z]\nsystem = SystemSurrogate([model1, model2], inputs, outputs)\nsystem.fit()\n\nx_test = system.sample_inputs(10)\ny_test = system.predict(x_test)\n</code></pre> The first component computes \\(y=x\\sin(\\pi x)\\). The second component takes the output of the first and computes \\(z=1 / (1 + 25y^2)\\). The system-level input is \\(x\\) and the system-level outputs are \\(y\\) and \\(z\\). </p> <p>Note</p> <p>Each component always locally returns a dictionary with the output saved as <code>y=value</code>. This is not to be confused with the  system-level <code>y</code> variable in this example.</p>"},{"location":"tutorials/#fire-detection-satellite","title":"Fire detection satellite","text":"<p>Here is an example of a three-component fire detection satellite system from Chauduri (2018). amisc.examples.tutorial.py<pre><code>import numpy as np\n\nfrom amisc.examples.models import fire_sat_system\n\nsystem = fire_sat_system()\n\nxtest = system.sample_inputs(100, use_pdf=True)     # --&gt; (100, xdim)\nytest = system(xtest, use_model='best')             # --&gt; (100, ydim)\nuse_idx = ~np.any(np.isnan(ytest), axis=-1)\nxtest = xtest[use_idx, :]\nytest = ytest[use_idx, :]\ntest_set = {'xt': xtest, 'yt': ytest}\n\nsystem.fit(max_iter=10, test_set=test_set, n_jobs=-1, num_refine=1000)\n\nprint(f'Inputs: {system.exo_vars}')\nprint(f'Outputs: {system.coupling_vars}')\n\n# Plots\ninput_vars = ['H', 'Po']\noutput_vars = ['Vsat', 'Asa']\nsystem.plot_allocation()\nsystem.plot_slice(input_vars, output_vars, show_model=['best', 'worst'], random_walk=True, N=10)\n</code></pre> We first generate a test set using the ground truth model predictions (and filter any bad values out). Then we train the  surrogate in 10 iterations, and finally plot some results. Here is the output of <code>plot_slice()</code>: </p>"},{"location":"reference/","title":"<code>amisc</code>","text":"<p>Efficient framework for building surrogates of multidisciplinary systems using the adaptive multi-index stochastic collocation (AMISC) technique.</p> <ul> <li>Author - Joshua Eckels (eckelsjd@umich.edu)</li> <li>License - GPL-3.0</li> </ul> <p>The <code>amisc</code> package takes an object-oriented approach to building a surrogate of a multidisciplinary system. From the bottom up, you have:</p> <ul> <li>variables that serve as inputs and outputs for the models,</li> <li>interpolators that define a specific input \u2192 output mathematical relationship to interpolate a function,</li> <li>components that wrap a model for a single discipline, and a</li> <li>system that defines the connections between components in a multidisciplinary system.</li> </ul> <p>The variables, interpolators, and components all have abstract base classes, so that the system is ultimately independent of the specific models, interpolation methods, or underlying variables. As such, the primary top-level object that users of the <code>amisc</code> package will interact with is the <code>SystemSurrogate</code>.</p> <p>Note</p> <p>There are already pretty good implementations of the other abstractions that most users will not need to worry about, but they are provided in this API reference for completeness. The abstractions allow new interpolation (i.e. function approximation) methods to be implemented if desired, such as neural networks, kriging, etc.</p> <p>Here is a class diagram summary of this workflow:</p> <p><pre><code>classDiagram\n    namespace Core {\n        class SystemSurrogate {\n          +list[BaseRV] exo_vars\n          +list[BaseRV] coupling_vars\n          +int refine_level\n          +fit()\n          +predict(x)\n          +sample_inputs(size)\n          +insert_component(comp)\n        }\n        class ComponentSurrogate {\n          &lt;&lt;abstract&gt;&gt;\n          +IndexSet index_set\n          +IndexSet candidate_set\n          +list[BaseRV] x_vars\n          +dict[str: BaseInterpolator] surrogates\n          +dict[str: float] misc_coeff\n          +predict(x)\n          +activate_index(alpha, beta)\n          +add_surrogate(alpha, beta)\n          +update_misc_coeff()\n        }\n        class BaseInterpolator {\n          &lt;&lt;abstract&gt;&gt;\n          +tuple beta\n          +list[BaseRV] x_vars\n          +np.ndarray xi\n          +np.ndarray yi\n          +set_yi()\n          +refine()\n          +__call__(x)\n        }\n    }\n    class SparseGridSurrogate {\n      +np.ndarray x_grids\n      +dict xi_map\n      +dict yi_map\n      +get_tensor_grid(alpha, beta)\n    }\n    class LagrangeInterpolator {\n      +np.ndarray x_grids\n      +np.ndarray weights\n      +get_grid_sizes()\n      +leja_1d()\n    }\n    class BaseRV {\n      &lt;&lt;abstract&gt;&gt;\n      +tuple bounds\n      +str units\n      +float nominal\n      +pdf(x)\n      +sample(size)\n    }\n    class UniformRV {\n      +str type\n      +get_uniform_bounds(nominal)\n    }\n    SystemSurrogate o-- \"1..n\" ComponentSurrogate\n    ComponentSurrogate o-- \"1..n\" BaseInterpolator\n    direction LR\n    ComponentSurrogate &lt;|-- SparseGridSurrogate\n    BaseInterpolator &lt;|-- LagrangeInterpolator\n    SparseGridSurrogate ..&gt; LagrangeInterpolator\n    BaseRV &lt;|-- UniformRV</code></pre> Note how the <code>SystemSurrogate</code> aggregates the <code>ComponentSurrogate</code>, which aggregates the <code>BaseInterpolator</code>. In other words, interpolators can act independently of components, and components can act independently of systems. All three make use of the random variables (these connections and some RVs are not shown for visual clarity). Currently, the only underlying surrogate method that is implemented here is Lagrange polynomial interpolation (i.e. the <code>LagrangeInterpolator</code>). If one wanted to use neural networks instead, the only change required is a new implementation of <code>BaseInterpolator</code>.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> amisc</li> <li> component</li> <li> interpolator</li> <li> rv</li> <li> system</li> <li> utils</li> </ul>"},{"location":"reference/component/","title":"component","text":""},{"location":"reference/component/#amisc.component","title":"<code>amisc.component</code>","text":"<p>A Component is an <code>amisc</code> wrapper around a single discipline model. It manages surrogate construction and optionally a hierarchy of modeling fidelities that may be available. Concrete component classes all inherit from the base <code>ComponentSurrogate</code> class provided here. Components manage an array of <code>BaseInterpolator</code> objects to form a multifidelity hierarchy.</p> <p>Includes:</p> <ul> <li><code>ComponentSurrogate</code>: the base class that is fundamental to the adaptive multi-index stochastic collocation strategy</li> <li><code>SparseGridSurrogate</code>: an AMISC component that manages a hierarchy of <code>LagrangeInterpolator</code> objects</li> <li><code>AnalyticalSurrogate</code>: a light wrapper around a single discipline model that does not require surrogate approximation</li> </ul>"},{"location":"reference/component/#amisc.component.ComponentSurrogate","title":"<code>ComponentSurrogate(x_vars, model, multi_index=None, truth_alpha=(), max_alpha=(), max_beta=(), log_file=None, executor=None, model_args=(), model_kwargs=None)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>The base multi-index stochastic collocation (MISC) surrogate class for a single discipline component model.</p> <p>Multi-indices</p> <p>A multi-index is a tuple of natural numbers, each specifying a level of fidelity. You will frequently see two multi-indices: <code>alpha</code> and <code>beta</code>. The <code>alpha</code> (or \\(\\alpha\\)) indices specify physical model fidelity and get passed to the model as an additional argument (e.g. things like discretization level, time step size, etc.). The <code>beta</code> (or \\(\\beta\\)) indices specify surrogate refinement level, so typically an indication of the amount of training data used. Each fidelity index in \\(\\alpha\\) and \\(\\beta\\) increase in refinement from \\(0\\) up to <code>max_alpha</code> and <code>max_beta</code>. From the surrogate's perspective, the concatenation of \\((\\alpha, \\beta)\\) fully specifies a single fidelity \"level\". The <code>ComponentSurrogate</code> forms an approximation of the model by summing up over many of these concatenated sets of \\((\\alpha, \\beta)\\). These lists are stored in a data structure of <code>list[ tuple[ tuple, tuple ], ...]</code>. When \\(\\alpha\\) or \\(\\beta\\) are used as keys in a <code>dict</code>, they are cast to a Python <code>str</code> from a <code>tuple</code>.</p> ATTRIBUTE DESCRIPTION <code>index_set</code> <p>the current active set of multi-indices in the MISC approximation</p> <p> TYPE: <code>IndexSet</code> </p> <code>candidate_set</code> <p>all neighboring multi-indices that are candidates for inclusion in <code>index_set</code></p> <p> TYPE: <code>IndexSet</code> </p> <code>x_vars</code> <p>list of variables that define the input domain</p> <p> TYPE: <code>list[BaseRV]</code> </p> <code>ydim</code> <p>the number of outputs returned by the model</p> <p> TYPE: <code>int</code> </p> <code>_model</code> <p>stores a ref to the model or function that is to be approximated, callable as <code>ret = model(x)</code></p> <p> TYPE: <code>callable[np.ndarray] -&gt; dict</code> </p> <code>_model_args</code> <p>additional arguments to supply to the model</p> <p> TYPE: <code>tuple</code> </p> <code>_model_kwargs</code> <p>additional keyword arguments to supply to the model</p> <p> TYPE: <code>dict</code> </p> <code>truth_alpha</code> <p>the model fidelity indices to treat as the \"ground truth\" model</p> <p> TYPE: <code>tuple[int, ...]</code> </p> <code>max_refine</code> <p>the maximum level of refinement for each fidelity index in \\((\\alpha, \\beta)\\)</p> <p> TYPE: <code>list[int, ...]</code> </p> <code>surrogates</code> <p>keeps track of the <code>BaseInterpolator</code> associated with each set of \\((\\alpha, \\beta)\\)</p> <p> TYPE: <code>MiscTree</code> </p> <code>costs</code> <p>keeps track of total cost associated with adding a single \\((\\alpha, \\beta)\\) to the MISC approximation</p> <p> TYPE: <code>MiscTree</code> </p> <code>misc_coeff</code> <p>the combination technique coefficients for the MISC approximation</p> <p> TYPE: <code>MiscTree</code> </p> <p>Construct the MISC surrogate and initialize with any multi-indices passed in.</p> <p>Model specification</p> <p>The model is a callable function of the form <code>ret = model(x, *args, **kwargs)</code>. The return value is a dictionary of the form <code>ret = {'y': y, 'files': files, 'cost': cost}</code>. In the return dictionary, you specify the raw model output <code>y</code> as an <code>np.ndarray</code> at a minimum. Optionally, you can specify paths to output files and the average model cost (in units of seconds of cpu time), and anything else you want.</p> <p>Warning</p> <p>If the model has multiple fidelities, then the function signature must be <code>model(x, alpha, *args, **kwargs)</code> ; the first argument after <code>x</code> will always be the fidelity indices <code>alpha</code>. The rest of <code>model_args</code> will be passed in after (you do not need to include <code>alpha</code> in <code>model_args</code>, it is done automatically).</p> PARAMETER DESCRIPTION <code>x_vars</code> <p><code>[X1, X2, ...]</code> list of variables specifying bounds/pdfs for each input</p> <p> TYPE: <code>list[BaseRV] | BaseRV</code> </p> <code>model</code> <p>the function to approximate, callable as <code>ret = model(x, *args, **kwargs)</code></p> <p> TYPE: <code>callable</code> </p> <code>multi_index</code> <p><code>[((alpha1), (beta1)), ... ]</code> list of concatenated multi-indices \\((\\alpha, \\beta)\\)</p> <p> TYPE: <code>IndexSet</code> DEFAULT: <code>None</code> </p> <code>truth_alpha</code> <p>specifies the highest model fidelity indices necessary for a \"ground truth\" comparison</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>()</code> </p> <code>max_alpha</code> <p>the maximum model refinement indices to allow, defaults to <code>(2,...)</code> if applicable</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>()</code> </p> <code>max_beta</code> <p>the maximum surrogate refinement indices, defaults to <code>(2,...)</code> of length <code>x_dim</code></p> <p> TYPE: <code>tuple</code> DEFAULT: <code>()</code> </p> <code>log_file</code> <p>specifies a log file (optional)</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>executor</code> <p>parallel executor used to add candidate indices in parallel (optional)</p> <p> TYPE: <code>Executor</code> DEFAULT: <code>None</code> </p> <code>model_args</code> <p>optional args to pass when calling the model</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>()</code> </p> <code>model_kwargs</code> <p>optional kwargs to pass when calling the model</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def __init__(self, x_vars: list[BaseRV] | BaseRV, model: callable,\n             multi_index: IndexSet = None,\n             truth_alpha: tuple = (), max_alpha: tuple = (), max_beta: tuple = (),\n             log_file: str | Path = None, executor: Executor = None,\n             model_args: tuple = (), model_kwargs: dict = None):\n    \"\"\"Construct the MISC surrogate and initialize with any multi-indices passed in.\n\n    !!! Info \"Model specification\"\n        The model is a callable function of the form `ret = model(x, *args, **kwargs)`. The return value is a\n        dictionary of the form `ret = {'y': y, 'files': files, 'cost': cost}`. In the return dictionary, you\n        specify the raw model output `y` as an `np.ndarray` at a _minimum_. Optionally, you can specify paths to\n        output files and the average model cost (in units of seconds of cpu time), and anything else you want.\n\n    !!! Warning\n        If the model has multiple fidelities, then the function signature must be `model(x, alpha, *args, **kwargs)`\n        ; the first argument after `x` will always be the fidelity indices `alpha`. The rest of `model_args` will\n        be passed in after (you do not need to include `alpha` in `model_args`, it is done automatically).\n\n    :param x_vars: `[X1, X2, ...]` list of variables specifying bounds/pdfs for each input\n    :param model: the function to approximate, callable as `ret = model(x, *args, **kwargs)`\n    :param multi_index: `[((alpha1), (beta1)), ... ]` list of concatenated multi-indices $(\\\\alpha, \\\\beta)$\n    :param truth_alpha: specifies the highest model fidelity indices necessary for a \"ground truth\" comparison\n    :param max_alpha: the maximum model refinement indices to allow, defaults to `(2,...)` if applicable\n    :param max_beta: the maximum surrogate refinement indices, defaults to `(2,...)` of length `x_dim`\n    :param log_file: specifies a log file (optional)\n    :param executor: parallel executor used to add candidate indices in parallel (optional)\n    :param model_args: optional args to pass when calling the model\n    :param model_kwargs: optional kwargs to pass when calling the model\n    \"\"\"\n    self.logger = get_logger(self.__class__.__name__, log_file=log_file)\n    self.log_file = log_file\n    self.executor = executor\n    self.training_flag = None  # Keep track of which MISC coeffs are active\n    # (True=active set, False=active+candidate sets, None=Neither/unknown)\n\n    multi_index = list() if multi_index is None else multi_index\n    assert self.is_downward_closed(multi_index), 'Must be a downward closed set.'\n    self.ydim = None\n    self.index_set = []         # The active index set for the MISC approximation\n    self.candidate_set = []     # Candidate indices for refinement\n    self._model = model\n    self._model_args = model_args\n    self._model_kwargs = model_kwargs if model_kwargs is not None else {}\n    self.truth_alpha = truth_alpha\n    self.x_vars = x_vars if isinstance(x_vars, list) else [x_vars]\n    max_alpha = truth_alpha if max_alpha == () else max_alpha\n    max_beta = (2,)*len(self.x_vars) if max_beta == () else max_beta\n    self.max_refine = list(max_alpha + max_beta)    # Max refinement indices\n\n    # Initialize important tree-like structures\n    self.surrogates = dict()        # Maps alphas -&gt; betas -&gt; surrogates\n    self.costs = dict()             # Maps alphas -&gt; betas -&gt; wall clock run times\n    self.misc_coeff = dict()        # Maps alphas -&gt; betas -&gt; MISC coefficients\n\n    # Construct vectors of [0,1]^dim(alpha+beta)\n    Nij = len(self.max_refine)\n    self.ij = np.zeros((2 ** Nij, Nij), dtype=np.uint8)\n    for i, ele in enumerate(itertools.product([0, 1], repeat=Nij)):\n        self.ij[i, :] = ele\n\n    # Initialize any indices that were passed in\n    multi_index = list() if multi_index is None else multi_index\n    for alpha, beta in multi_index:\n        self.activate_index(alpha, beta)\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.activate_index","title":"<code>activate_index(alpha, beta)</code>","text":"<p>Add a multi-index to the active set and all neighbors to the candidate set.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>A multi-index specifying model fidelity</p> <p> TYPE: <code>tuple</code> </p> <code>beta</code> <p>A multi-index specifying surrogate fidelity</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def activate_index(self, alpha: tuple, beta: tuple):\n    \"\"\"Add a multi-index to the active set and all neighbors to the candidate set.\n\n    :param alpha: A multi-index specifying model fidelity\n    :param beta: A multi-index specifying surrogate fidelity\n    \"\"\"\n    # User is responsible for making sure index set is downward-closed\n    alpha, beta = tuple([int(i) for i in alpha]), tuple([int(i) for i in beta])  # Make sure these are python ints\n    self.add_surrogate(alpha, beta)\n    ele = (alpha, beta)\n    if ele in self.index_set:\n        self.logger.warning(f'Multi-index {ele} is already in the active index set. Ignoring...')\n        return\n\n    # Add all possible new candidates (distance of one unit vector away)\n    ind = list(alpha + beta)\n    new_candidates = []\n    for i in range(len(ind)):\n        ind_new = ind.copy()\n        ind_new[i] += 1\n\n        # Don't add if we surpass a refinement limit\n        if np.any(np.array(ind_new) &gt; np.array(self.max_refine)):\n            continue\n\n        # Add the new index if it maintains downward-closedness\n        new_cand = (tuple(ind_new[:len(alpha)]), tuple(ind_new[len(alpha):]))\n        down_closed = True\n        for j in range(len(ind)):\n            ind_check = ind_new.copy()\n            ind_check[j] -= 1\n            if ind_check[j] &gt;= 0:\n                tup_check = (tuple(ind_check[:len(alpha)]), tuple(ind_check[len(alpha):]))\n                if tup_check not in self.index_set and tup_check != ele:\n                    down_closed = False\n                    break\n        if down_closed:\n            new_candidates.append(new_cand)\n\n    # Build an interpolator for each new candidate\n    if self.executor is None:   # Sequential\n        for a, b in new_candidates:\n            self.add_surrogate(a, b)\n    else:                       # Parallel\n        temp_exc = self.executor\n        self.executor = None\n        for a, b in new_candidates:\n            if str(a) not in self.surrogates:\n                self.surrogates[str(a)] = dict()\n                self.costs[str(a)] = dict()\n                self.misc_coeff[str(a)] = dict()\n        self.parallel_add_candidates(new_candidates, temp_exc)\n        self.executor = temp_exc\n\n    # Move to the active index set\n    if ele in self.candidate_set:\n        self.candidate_set.remove(ele)\n    self.index_set.append(ele)\n    new_candidates = [cand for cand in new_candidates if cand not in self.candidate_set]\n    self.candidate_set.extend(new_candidates)\n    self.training_flag = None   # Makes sure misc coeffs get recomputed next time\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.add_surrogate","title":"<code>add_surrogate(alpha, beta)</code>","text":"<p>Build a <code>BaseInterpolator</code> object for a given \\((\\alpha, \\beta)\\)</p> PARAMETER DESCRIPTION <code>alpha</code> <p>A multi-index specifying model fidelity</p> <p> TYPE: <code>tuple</code> </p> <code>beta</code> <p>A multi-index specifying surrogate fidelity</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def add_surrogate(self, alpha: tuple, beta: tuple):\n    \"\"\"Build a `BaseInterpolator` object for a given $(\\\\alpha, \\\\beta)$\n\n    :param alpha: A multi-index specifying model fidelity\n    :param beta: A multi-index specifying surrogate fidelity\n    \"\"\"\n    # Create a dictionary for each alpha model to store multiple surrogate fidelities (beta)\n    if str(alpha) not in self.surrogates:\n        self.surrogates[str(alpha)] = dict()\n        self.costs[str(alpha)] = dict()\n        self.misc_coeff[str(alpha)] = dict()\n\n    # Create a new interpolator object for this multi-index (abstract method)\n    if self.surrogates[str(alpha)].get(str(beta), None) is None:\n        self.logger.info(f'Building interpolator for index {(alpha, beta)} ...')\n        x_new_idx, x_new, interp = self.build_interpolator(alpha, beta)\n        self.surrogates[str(alpha)][str(beta)] = interp\n        cost = self.update_interpolator(x_new_idx, x_new, interp)  # Awkward, but needed to separate the model evals\n        self.costs[str(alpha)][str(beta)] = cost\n        if self.ydim is None:\n            self.ydim = interp.ydim()\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.init_coarse","title":"<code>init_coarse()</code>","text":"<p>Initialize the coarsest interpolation and add to the active index set</p> Source code in <code>src/amisc/component.py</code> <pre><code>def init_coarse(self):\n    \"\"\"Initialize the coarsest interpolation and add to the active index set\"\"\"\n    alpha = (0,) * len(self.truth_alpha)\n    beta = (0,) * len(self.max_refine[len(self.truth_alpha):])\n    self.activate_index(alpha, beta)\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.iterate_candidates","title":"<code>iterate_candidates()</code>","text":"<p>Iterate candidate indices one by one into the active index set.</p> <p>:yields alpha, beta: the multi-indices of the current candidate that has been moved to active set</p> Source code in <code>src/amisc/component.py</code> <pre><code>def iterate_candidates(self):\n    \"\"\"Iterate candidate indices one by one into the active index set.\n\n    :yields alpha, beta: the multi-indices of the current candidate that has been moved to active set\n    \"\"\"\n    for alpha, beta in list(self.candidate_set):\n        # Temporarily add a candidate index to active set\n        self.index_set.append((alpha, beta))\n        yield alpha, beta\n        del self.index_set[-1]\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.predict","title":"<code>predict(x, use_model=None, model_dir=None, training=False, index_set=None, ppool=None)</code>","text":"<p>Evaluate the MISC approximation at new points <code>x</code>.</p> <p>Note</p> <p>By default this will predict the MISC surrogate approximation. However, for convenience you can also specify <code>use_model</code> to call the underlying function instead.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>(..., x_dim)</code> the points to be interpolated, must be within input domain for accuracy</p> <p> TYPE: <code>ndarray | float</code> </p> <code>use_model</code> <p>'best'=high-fidelity, 'worst'=low-fidelity, tuple=a specific <code>alpha</code>, None=surrogate (default)</p> <p> TYPE: <code>str | tuple</code> DEFAULT: <code>None</code> </p> <code>model_dir</code> <p>directory to save output files if <code>use_model</code> is specified, ignored otherwise</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>training</code> <p>if <code>True</code>, then only compute with the active index set, otherwise use all candidates as well</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>index_set</code> <p>a list of concatenated \\((\\alpha, \\beta)\\) to override <code>self.index_set</code> if given, else ignore</p> <p> TYPE: <code>IndexSet</code> DEFAULT: <code>None</code> </p> <code>ppool</code> <p>a joblib <code>Parallel</code> pool to loop over multi-indices in parallel</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., y_dim)</code> the surrogate approximation of the function (or the function itself if <code>use_model</code>)</p> Source code in <code>src/amisc/component.py</code> <pre><code>def predict(self, x: np.ndarray | float, use_model: str | tuple = None, model_dir: str | Path = None,\n            training: bool = False, index_set: IndexSet = None, ppool=None) -&gt; np.ndarray:\n    \"\"\"Evaluate the MISC approximation at new points `x`.\n\n    !!! Note\n        By default this will predict the MISC surrogate approximation. However, for convenience you can also specify\n        `use_model` to call the underlying function instead.\n\n    :param x: `(..., x_dim)` the points to be interpolated, must be within input domain for accuracy\n    :param use_model: 'best'=high-fidelity, 'worst'=low-fidelity, tuple=a specific `alpha`, None=surrogate (default)\n    :param model_dir: directory to save output files if `use_model` is specified, ignored otherwise\n    :param training: if `True`, then only compute with the active index set, otherwise use all candidates as well\n    :param index_set: a list of concatenated $(\\\\alpha, \\\\beta)$ to override `self.index_set` if given, else ignore\n    :param ppool: a joblib `Parallel` pool to loop over multi-indices in parallel\n    :returns y: `(..., y_dim)` the surrogate approximation of the function (or the function itself if `use_model`)\n    \"\"\"\n    x = np.atleast_1d(x)\n    if use_model is not None:\n        return self._bypass_surrogate(x, use_model, model_dir)\n\n    index_set, misc_coeff = self._combination(index_set, training)  # Choose the correct index set and misc_coeff\n\n    def run_batch(alpha, beta, y):\n        comb_coeff = misc_coeff[str(alpha)][str(beta)]\n        if np.abs(comb_coeff) &gt; 0:\n            func = self.surrogates[str(alpha)][str(beta)]\n            y += int(comb_coeff) * func(x)\n\n    if ppool is not None:\n        with tempfile.NamedTemporaryFile(suffix='.dat', mode='w+b', delete=False) as y_fd:\n            pass\n        y_ret = np.memmap(y_fd.name, dtype=x.dtype, mode='r+', shape=x.shape[:-1] + (self.ydim,))\n        ppool(delayed(run_batch)(alpha, beta, y_ret) for alpha, beta in index_set)\n        y = np.empty(y_ret.shape, dtype=x.dtype)\n        y[:] = y_ret[:]\n        del y_ret\n        os.unlink(y_fd.name)\n    else:\n        y = np.zeros(x.shape[:-1] + (self.ydim,), dtype=x.dtype)\n        for alpha, beta in index_set:\n            run_batch(alpha, beta, y)\n\n    return y\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.grad","title":"<code>grad(x, training=False, index_set=None)</code>","text":"<p>Evaluate the derivative/Jacobian of the MISC approximation at new points <code>x</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>(..., x_dim)</code> the evaluation points, must be within input domain for accuracy</p> <p> TYPE: <code>ndarray | float | list</code> </p> <code>training</code> <p>if <code>True</code>, then only compute with the active index set, otherwise use all candidates as well</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>index_set</code> <p>a list of concatenated \\((\\alpha, \\beta)\\) to override <code>self.index_set</code> if given, else ignore</p> <p> TYPE: <code>IndexSet</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., y_dim, x_dim)</code> the Jacobian of the surrogate approximation</p> Source code in <code>src/amisc/component.py</code> <pre><code>def grad(self, x: np.ndarray | float | list, training: bool = False, index_set: IndexSet = None) -&gt; np.ndarray:\n    \"\"\"Evaluate the derivative/Jacobian of the MISC approximation at new points `x`.\n\n    :param x: `(..., x_dim)` the evaluation points, must be within input domain for accuracy\n    :param training: if `True`, then only compute with the active index set, otherwise use all candidates as well\n    :param index_set: a list of concatenated $(\\\\alpha, \\\\beta)$ to override `self.index_set` if given, else ignore\n    :returns: `(..., y_dim, x_dim)` the Jacobian of the surrogate approximation\n    \"\"\"\n    x = np.atleast_1d(x)\n    index_set, misc_coeff = self._combination(index_set, training)  # Choose the correct index set and misc_coeff\n\n    jac = np.zeros(x.shape[:-1] + (self.ydim, len(self.x_vars)), dtype=x.dtype)\n    for alpha, beta in index_set:\n        comb_coeff = misc_coeff[str(alpha)][str(beta)]\n        if np.abs(comb_coeff) &gt; 0:\n            interp = self.surrogates[str(alpha)][str(beta)]\n            jac += int(comb_coeff) * interp.grad(x)\n\n    return jac\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.hessian","title":"<code>hessian(x, training=False, index_set=None)</code>","text":"<p>Evaluate the Hessian of the MISC approximation at new points <code>x</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>(..., x_dim)</code> the evaluation points, must be within input domain for accuracy</p> <p> TYPE: <code>ndarray | float | list</code> </p> <code>training</code> <p>if <code>True</code>, then only compute with the active index set, otherwise use all candidates as well</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>index_set</code> <p>a list of concatenated \\((\\alpha, \\beta)\\) to override <code>self.index_set</code> if given, else ignore</p> <p> TYPE: <code>IndexSet</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., y_dim, x_dim, x_dim)</code> the Hessian of the surrogate approximation</p> Source code in <code>src/amisc/component.py</code> <pre><code>def hessian(self, x: np.ndarray | float | list, training: bool = False, index_set: IndexSet = None) -&gt; np.ndarray:\n    \"\"\"Evaluate the Hessian of the MISC approximation at new points `x`.\n\n    :param x: `(..., x_dim)` the evaluation points, must be within input domain for accuracy\n    :param training: if `True`, then only compute with the active index set, otherwise use all candidates as well\n    :param index_set: a list of concatenated $(\\\\alpha, \\\\beta)$ to override `self.index_set` if given, else ignore\n    :returns: `(..., y_dim, x_dim, x_dim)` the Hessian of the surrogate approximation\n    \"\"\"\n    x = np.atleast_1d(x)\n    index_set, misc_coeff = self._combination(index_set, training)  # Choose the correct index set and misc_coeff\n\n    hess = np.zeros(x.shape[:-1] + (self.ydim, len(self.x_vars), len(self.x_vars)), x.dtype)\n    for alpha, beta in index_set:\n        comb_coeff = misc_coeff[str(alpha)][str(beta)]\n        if np.abs(comb_coeff) &gt; 0:\n            interp = self.surrogates[str(alpha)][str(beta)]\n            hess += int(comb_coeff) * interp.hessian(x)\n\n    return hess\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.update_misc_coeffs","title":"<code>update_misc_coeffs(index_set=None)</code>","text":"<p>Update the combination technique coeffs for MISC using the given index set.</p> PARAMETER DESCRIPTION <code>index_set</code> <p>the index set to consider when computing the MISC coefficients, defaults to the active set</p> <p> TYPE: <code>IndexSet</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MiscTree</code> <p>the MISC coefficients for the given index set (\\(\\alpha\\) -&gt; \\(\\beta\\) -&gt; coeff)</p> Source code in <code>src/amisc/component.py</code> <pre><code>def update_misc_coeffs(self, index_set: IndexSet = None) -&gt; MiscTree:\n    \"\"\"Update the combination technique coeffs for MISC using the given index set.\n\n    :param index_set: the index set to consider when computing the MISC coefficients, defaults to the active set\n    :returns: the MISC coefficients for the given index set ($\\\\alpha$ -&gt; $\\\\beta$ -&gt; coeff)\n    \"\"\"\n    if index_set is None:\n        index_set = self.index_set\n\n    # Construct a (N_indices, dim(alpha+beta)) refactor of the index_set for arrayed computations\n    index_mat = np.zeros((len(index_set), len(self.max_refine)), dtype=np.uint8)\n    for i, (alpha, beta) in enumerate(index_set):\n        index_mat[i, :] = alpha + beta\n    index_mat = np.expand_dims(index_mat, axis=0)                               # (1, Ns, Nij)\n\n    misc_coeff = dict()\n    for alpha, beta in index_set:\n        # Add permutations of [0, 1] to (alpha, beta)\n        alpha_beta = np.array(alpha+beta, dtype=np.uint8)[np.newaxis, :]        # (1, Nij)\n        new_indices = np.expand_dims(alpha_beta + self.ij, axis=1)              # (2**Nij, 1, Nij)\n\n        # Find which indices are in the index_set (using np broadcasting comparison)\n        diff = new_indices - index_mat                                  # (2**Nij, Ns, Nij)\n        idx = np.count_nonzero(diff, axis=-1) == 0                      # (2**Nij, Ns)\n        idx = np.any(idx, axis=-1)                                      # (2**Nij,)\n        ij_use = self.ij[idx, :]                                        # (*, Nij)\n        l1_norm = np.sum(np.abs(ij_use), axis=-1)                       # (*,)\n        coeff = np.sum((-1.) ** l1_norm)                                # float\n\n        # Save misc coeff to a dict() tree structure\n        if misc_coeff.get(str(alpha)) is None:\n            misc_coeff[str(alpha)] = dict()\n        misc_coeff[str(alpha)][str(beta)] = coeff\n        self.misc_coeff[str(alpha)][str(beta)] = coeff\n\n    return misc_coeff\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.get_sub_surrogate","title":"<code>get_sub_surrogate(alpha, beta)</code>","text":"<p>Get the specific sub-surrogate corresponding to the \\((\\alpha, \\beta)\\) fidelity.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>A multi-index specifying model fidelity</p> <p> TYPE: <code>tuple</code> </p> <code>beta</code> <p>A multi-index specifying surrogate fidelity</p> <p> TYPE: <code>tuple</code> </p> RETURNS DESCRIPTION <code>BaseInterpolator</code> <p>the corresponding <code>BaseInterpolator</code> object</p> Source code in <code>src/amisc/component.py</code> <pre><code>def get_sub_surrogate(self, alpha: tuple, beta: tuple) -&gt; BaseInterpolator:\n    \"\"\"Get the specific sub-surrogate corresponding to the $(\\\\alpha, \\\\beta)$ fidelity.\n\n    :param alpha: A multi-index specifying model fidelity\n    :param beta: A multi-index specifying surrogate fidelity\n    :returns: the corresponding `BaseInterpolator` object\n    \"\"\"\n    return self.surrogates[str(alpha)][str(beta)]\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.get_cost","title":"<code>get_cost(alpha, beta)</code>","text":"<p>Return the total cost (wall time s) required to add \\((\\alpha, \\beta)\\) to the MISC approximation.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>A multi-index specifying model fidelity</p> <p> TYPE: <code>tuple</code> </p> <code>beta</code> <p>A multi-index specifying surrogate fidelity</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def get_cost(self, alpha: tuple, beta: tuple) -&gt; float:\n    \"\"\"Return the total cost (wall time s) required to add $(\\\\alpha, \\\\beta)$ to the MISC approximation.\n\n    :param alpha: A multi-index specifying model fidelity\n    :param beta: A multi-index specifying surrogate fidelity\n    \"\"\"\n    try:\n        return self.costs[str(alpha)][str(beta)]\n    except Exception:\n        return 0.0\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.update_input_bds","title":"<code>update_input_bds(idx, bds)</code>","text":"<p>Update the bounds of the input variable at the given index.</p> PARAMETER DESCRIPTION <code>idx</code> <p>the index of the input variable to update</p> <p> TYPE: <code>int</code> </p> <code>bds</code> <p>the new bounds</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def update_input_bds(self, idx: int, bds: tuple):\n    \"\"\"Update the bounds of the input variable at the given index.\n\n    :param idx: the index of the input variable to update\n    :param bds: the new bounds\n    \"\"\"\n    self.x_vars[int(idx)].update_bounds(*bds)\n\n    # Update the bounds in all associated surrogates\n    for alpha in self.surrogates:\n        for beta in self.surrogates[alpha]:\n            self.surrogates[alpha][beta].update_input_bds(idx, bds)\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.save_enabled","title":"<code>save_enabled()</code>","text":"<p>Return whether this model wants to save outputs to file.</p> <p>Note</p> <p>You can specify that a model wants to save outputs to file by providing an <code>'output_dir'</code> kwarg.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def save_enabled(self):\n    \"\"\"Return whether this model wants to save outputs to file.\n\n    !!! Note\n        You can specify that a model wants to save outputs to file by providing an `'output_dir'` kwarg.\n    \"\"\"\n    return self._model_kwargs.get('output_dir') is not None\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.is_one_level_refinement","title":"<code>is_one_level_refinement(beta_old, beta_new)</code>  <code>staticmethod</code>","text":"<p>Check if a new <code>beta</code> multi-index is a one-level refinement from a previous <code>beta</code>.</p> <p>Example</p> <p>Refining from <code>(0, 1, 2)</code> to the new multi-index <code>(1, 1, 2)</code> is a one-level refinement. But refining to either <code>(2, 1, 2)</code> or <code>(1, 2, 2)</code> are not, since more than one refinement occurs at the same time.</p> PARAMETER DESCRIPTION <code>beta_old</code> <p>the starting multi-index</p> <p> TYPE: <code>tuple</code> </p> <code>beta_new</code> <p>the new refined multi-index</p> <p> TYPE: <code>tuple</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>whether <code>beta_new</code> is a one-level refinement from <code>beta_old</code></p> Source code in <code>src/amisc/component.py</code> <pre><code>@staticmethod\ndef is_one_level_refinement(beta_old: tuple, beta_new: tuple) -&gt; bool:\n    \"\"\"Check if a new `beta` multi-index is a one-level refinement from a previous `beta`.\n\n    !!! Example\n        Refining from `(0, 1, 2)` to the new multi-index `(1, 1, 2)` is a one-level refinement. But refining to\n        either `(2, 1, 2)` or `(1, 2, 2)` are not, since more than one refinement occurs at the same time.\n\n    :param beta_old: the starting multi-index\n    :param beta_new: the new refined multi-index\n    :returns: whether `beta_new` is a one-level refinement from `beta_old`\n    \"\"\"\n    level_diff = np.array(beta_new, dtype=int) - np.array(beta_old, dtype=int)\n    ind = np.nonzero(level_diff)[0]\n    return ind.shape[0] == 1 and level_diff[ind] == 1\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.is_downward_closed","title":"<code>is_downward_closed(indices)</code>  <code>staticmethod</code>","text":"<p>Return if a list of \\((\\alpha, \\beta)\\) multi-indices is downward-closed.</p> <p>MISC approximations require a downward-closed set in order to use the combination-technique formula for the coefficients (as implemented here).</p> <p>Example</p> <p>The list <code>[( (0,), (0,) ), ( (1,), (0,) ), ( (1,), (1,) )]</code> is downward-closed. You can visualize this as building a stack of cubes: in order to place a cube, all adjacent cubes must be present (does the logo make sense now?).</p> PARAMETER DESCRIPTION <code>indices</code> <p>list() of (<code>alpha</code>, <code>beta</code>) multi-indices</p> <p> TYPE: <code>IndexSet</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>whether the set of indices is downward-closed</p> Source code in <code>src/amisc/component.py</code> <pre><code>@staticmethod\ndef is_downward_closed(indices: IndexSet) -&gt; bool:\n    \"\"\"Return if a list of $(\\\\alpha, \\\\beta)$ multi-indices is downward-closed.\n\n    MISC approximations require a downward-closed set in order to use the combination-technique formula for the\n    coefficients (as implemented here).\n\n    !!! Example\n        The list `[( (0,), (0,) ), ( (1,), (0,) ), ( (1,), (1,) )]` is downward-closed. You can visualize this as\n        building a stack of cubes: in order to place a cube, all adjacent cubes must be present (does the logo\n        make sense now?).\n\n    :param indices: list() of (`alpha`, `beta`) multi-indices\n    :returns: whether the set of indices is downward-closed\n    \"\"\"\n    # Iterate over every multi-index\n    for alpha, beta in indices:\n        # Every smaller multi-index must also be included in the indices list\n        sub_sets = [np.arange(tuple(alpha+beta)[i]+1) for i in range(len(alpha) + len(beta))]\n        for ele in itertools.product(*sub_sets):\n            tup = (tuple(ele[:len(alpha)]), tuple(ele[len(alpha):]))\n            if tup not in indices:\n                return False\n    return True\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.build_interpolator","title":"<code>build_interpolator(alpha, beta)</code>  <code>abstractmethod</code>","text":"<p>Return a <code>BaseInterpolator</code> object and new refinement points for a given \\((\\alpha, \\beta)\\) multi-index.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>A multi-index specifying model fidelity</p> <p> TYPE: <code>tuple</code> </p> <code>beta</code> <p>A multi-index specifying surrogate fidelity</p> <p> TYPE: <code>tuple</code> </p> RETURNS DESCRIPTION <code>InterpResults</code> <p><code>idx</code>, <code>x</code>, <code>interp</code> - list of new grid indices, the new grid points <code>(N_new, x_dim)</code>, and the <code>BaseInterpolator</code> object. Similar to <code>BaseInterpolator.refine()</code>.</p> Source code in <code>src/amisc/component.py</code> <pre><code>@abstractmethod\ndef build_interpolator(self, alpha: tuple, beta: tuple) -&gt; InterpResults:\n    \"\"\"Return a `BaseInterpolator` object and new refinement points for a given $(\\\\alpha, \\\\beta)$ multi-index.\n\n    :param alpha: A multi-index specifying model fidelity\n    :param beta: A multi-index specifying surrogate fidelity\n    :returns: `idx`, `x`, `interp` - list of new grid indices, the new grid points `(N_new, x_dim)`, and the\n              `BaseInterpolator` object. Similar to `BaseInterpolator.refine()`.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.update_interpolator","title":"<code>update_interpolator(x_new_idx, x_new, interp)</code>  <code>abstractmethod</code>","text":"<p>Secondary method to actually compute and save model evaluations within the interpolator.</p> <p>Note</p> <p>This distinction with <code>build_interpolator</code> was necessary to separately construct the interpolator and be able to evaluate the model at the new interpolation points. You can see that <code>parallel_add_candidates</code> uses this distinction to compute the model in parallel on MPI workers, for example.</p> PARAMETER DESCRIPTION <code>x_new_idx</code> <p>list of new grid point indices</p> <p> TYPE: <code>list[int | tuple | str]</code> </p> <code>x_new</code> <p><code>(N_new, x_dim)</code>, the new grid point locations</p> <p> TYPE: <code>ndarray</code> </p> <code>interp</code> <p>the <code>BaseInterpolator</code> object to compute model evaluations with</p> <p> TYPE: <code>BaseInterpolator</code> </p> RETURNS DESCRIPTION <code>float</code> <p>the cost (in wall time seconds) required to add this <code>BaseInterpolator</code> object</p> Source code in <code>src/amisc/component.py</code> <pre><code>@abstractmethod\ndef update_interpolator(self, x_new_idx: list[int | tuple | str],\n                        x_new: np.ndarray, interp: BaseInterpolator) -&gt; float:\n    \"\"\"Secondary method to actually compute and save model evaluations within the interpolator.\n\n    !!! Note\n        This distinction with `build_interpolator` was necessary to separately construct the interpolator and be\n        able to evaluate the model at the new interpolation points. You can see that `parallel_add_candidates`\n        uses this distinction to compute the model in parallel on MPI workers, for example.\n\n    :param x_new_idx: list of new grid point indices\n    :param x_new: `(N_new, x_dim)`, the new grid point locations\n    :param interp: the `BaseInterpolator` object to compute model evaluations with\n    :returns cost: the cost (in wall time seconds) required to add this `BaseInterpolator` object\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSurrogate.parallel_add_candidates","title":"<code>parallel_add_candidates(candidates, executor)</code>  <code>abstractmethod</code>","text":"<p>Defines a function to handle adding candidate indices in parallel.</p> <p>Note</p> <p>While <code>build_interpolator</code> can make changes to 'self', these changes will not be saved in the master task if running in parallel over MPI workers, for example. This method is a workaround so that all required mutable changes to 'self' are made in the master task, before distributing tasks to parallel workers using this method. You can pass if you don't plan to add candidates in parallel.</p> PARAMETER DESCRIPTION <code>candidates</code> <p>list of [(alpha, beta),...] multi-indices</p> <p> TYPE: <code>IndexSet</code> </p> <code>executor</code> <p>the executor used to iterate candidates in parallel</p> <p> TYPE: <code>Executor</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>@abstractmethod\ndef parallel_add_candidates(self, candidates: IndexSet, executor: Executor):\n    \"\"\"Defines a function to handle adding candidate indices in parallel.\n\n    !!! Note\n        While `build_interpolator` can make changes to 'self', these changes will not be saved in the master task\n        if running in parallel over MPI workers, for example. This method is a workaround so that all required\n        mutable changes to 'self' are made in the master task, before distributing tasks to parallel workers\n        using this method. You can pass if you don't plan to add candidates in parallel.\n\n    :param candidates: list of [(alpha, beta),...] multi-indices\n    :param executor: the executor used to iterate candidates in parallel\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/component/#amisc.component.SparseGridSurrogate","title":"<code>SparseGridSurrogate(*args, **kwargs)</code>","text":"<p>               Bases: <code>ComponentSurrogate</code></p> <p>Concrete MISC surrogate class that maintains a sparse grid composed of smaller tensor-product grids.</p> <p>Note</p> <p>MISC itself can be thought of as an extension to the well-known sparse grid technique, so this class readily integrates with the MISC implementation in <code>ComponentSurrogate</code>. Sparse grids limit the curse of dimensionality up to about <code>dim = 10-15</code> for the input space (which would otherwise be infeasible with a normal full tensor-product grid of the same size).</p> <p>About points in a sparse grid</p> <p>A sparse grid approximates a full tensor-product grid \\((N_1, N_2, ..., N_d)\\), where \\(N_i\\) is the number of grid points along dimension \\(i\\), for a \\(d\\)-dimensional space. Each point is uniquely identified in the sparse grid by a list of indices \\((j_1, j_2, ..., j_d)\\), where \\(j_i = 0 ... N_i\\). We refer to this unique identifier as a \"grid coordinate\". In the <code>HashSG</code> data structure, we use a <code>str(tuple(coord))</code> representation to uniquely identify the coordinate in a hash DS like Python's <code>dict</code>.</p> ATTRIBUTE DESCRIPTION <code>HashSG</code> <p>a type alias for the hash storage of the sparse grid data (a tree-like DS using dicts)</p> <p> TYPE: <code>dict[str: dict[str: np.ndarray | str]]</code> </p> <code>curr_max_beta</code> <p>the current maximum \\(\\beta\\) refinement indices in the sparse grid (for each \\(\\alpha\\))</p> <p> TYPE: <code>dict[str: list[int]]</code> </p> <code>x_grids</code> <p>maps \\(\\alpha\\) indices to a list of 1d grids corresponding to <code>curr_max_beta</code></p> <p> TYPE: <code>dict[str: np.ndarray]</code> </p> <code>xi_map</code> <p>the sparse grid interpolation points</p> <p> TYPE: <code>HashSG</code> </p> <code>yi_map</code> <p>the function values at all sparse grid points</p> <p> TYPE: <code>HashSG</code> </p> <code>yi_nan_map</code> <p>imputed function values to use when <code>yi_map</code> contains <code>nan</code> data (sometimes the model fails...)</p> <p> TYPE: <code>HashSG</code> </p> <code>yi_files</code> <p>optional filenames corresponding to the sparse grid <code>yi_map</code> data</p> <p> TYPE: <code>HashSG</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    # Initialize tree-like hash structures for maintaining a sparse grid of smaller tensor-product grids\n    self.curr_max_beta = dict()     # Maps alphas -&gt; current max refinement indices\n    self.x_grids = dict()           # Maps alphas -&gt; list of ndarrays specifying 1d grids corresponding to max_beta\n    self.xi_map = dict()            # Maps alphas -&gt; grid point coords -&gt; interpolation points\n    self.yi_map = dict()            # Maps alphas -&gt; grid point coords -&gt; interpolation function values\n    self.yi_nan_map = dict()        # Maps alphas -&gt; grid point coords -&gt; interpolated yi values when yi=nan\n    self.yi_files = dict()          # Maps alphas -&gt; grid point coords -&gt; model output files (optional)\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/component/#amisc.component.SparseGridSurrogate.predict","title":"<code>predict(x, use_model=None, model_dir=None, training=False, index_set=None, ppool=None)</code>","text":"<p>Need to override <code>super()</code> to allow passing in interpolation grids <code>xi</code> and <code>yi</code>.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def predict(self, x, use_model=None, model_dir=None, training=False, index_set=None, ppool=None):\n    \"\"\"Need to override `super()` to allow passing in interpolation grids `xi` and `yi`.\"\"\"\n    x = np.atleast_1d(x)\n    if use_model is not None:\n        return self._bypass_surrogate(x, use_model, model_dir)\n\n    index_set, misc_coeff = self._combination(index_set, training)\n\n    def run_batch(alpha, beta, y):\n        comb_coeff = misc_coeff[str(alpha)][str(beta)]\n        if np.abs(comb_coeff) &gt; 0:\n            # Gather the xi/yi interpolation points/qoi_ind for this sub tensor-product grid\n            interp = self.surrogates[str(alpha)][str(beta)]\n            xi, yi = self.get_tensor_grid(alpha, beta)\n\n            # Add this sub tensor-product grid to the MISC approximation\n            y += int(comb_coeff) * interp(x, xi=xi, yi=yi)\n\n    if ppool is not None:\n        with tempfile.NamedTemporaryFile(suffix='.dat', mode='w+b', delete=False) as y_fd:\n            pass\n        y_ret = np.memmap(y_fd.name, dtype=x.dtype, mode='r+', shape=x.shape[:-1] + (self.ydim,))\n        ppool(delayed(run_batch)(alpha, beta, y_ret) for alpha, beta in index_set)\n        y = np.empty(y_ret.shape, dtype=x.dtype)\n        y[:] = y_ret[:]\n        del y_ret\n        os.unlink(y_fd.name)\n    else:\n        y = np.zeros(x.shape[:-1] + (self.ydim,), dtype=x.dtype)\n        for alpha, beta in index_set:\n            run_batch(alpha, beta, y)\n\n    return y\n</code></pre>"},{"location":"reference/component/#amisc.component.SparseGridSurrogate.grad","title":"<code>grad(x, training=False, index_set=None)</code>","text":"<p>Need to override <code>super()</code> to allow passing in interpolation grids <code>xi</code> and <code>yi</code>.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def grad(self, x, training=False, index_set=None):\n    \"\"\"Need to override `super()` to allow passing in interpolation grids `xi` and `yi`.\"\"\"\n    x = np.atleast_1d(x)\n    index_set, misc_coeff = self._combination(index_set, training)  # Choose the correct index set and misc_coeff\n\n    jac = np.zeros(x.shape[:-1] + (self.ydim, len(self.x_vars)), dtype=x.dtype)\n    for alpha, beta in index_set:\n        comb_coeff = misc_coeff[str(alpha)][str(beta)]\n        if np.abs(comb_coeff) &gt; 0:\n            # Gather the xi/yi interpolation points/qoi_ind for this sub tensor-product grid\n            interp = self.surrogates[str(alpha)][str(beta)]\n            xi, yi = self.get_tensor_grid(alpha, beta)\n\n            jac += int(comb_coeff) * interp.grad(x, xi=xi, yi=yi)\n\n    return jac\n</code></pre>"},{"location":"reference/component/#amisc.component.SparseGridSurrogate.hessian","title":"<code>hessian(x, training=False, index_set=None)</code>","text":"<p>Need to override <code>super()</code> to allow passing in interpolation grids <code>xi</code> and <code>yi</code>.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def hessian(self, x, training=False, index_set=None):\n    \"\"\"Need to override `super()` to allow passing in interpolation grids `xi` and `yi`.\"\"\"\n    x = np.atleast_1d(x)\n    index_set, misc_coeff = self._combination(index_set, training)  # Choose the correct index set and misc_coeff\n\n    hess = np.zeros(x.shape[:-1] + (self.ydim, len(self.x_vars), len(self.x_vars)), dtype=x.dtype)\n    for alpha, beta in index_set:\n        comb_coeff = misc_coeff[str(alpha)][str(beta)]\n        if np.abs(comb_coeff) &gt; 0:\n            # Gather the xi/yi interpolation points/qoi_ind for this sub tensor-product grid\n            interp = self.surrogates[str(alpha)][str(beta)]\n            xi, yi = self.get_tensor_grid(alpha, beta)\n\n            hess += int(comb_coeff) * interp.hessian(x, xi=xi, yi=yi)\n\n    return hess\n</code></pre>"},{"location":"reference/component/#amisc.component.SparseGridSurrogate.get_tensor_grid","title":"<code>get_tensor_grid(alpha, beta, update_nan=True)</code>","text":"<p>Construct the <code>xi/yi</code> sub tensor-product grids for a given \\((\\alpha, \\beta)\\) multi-index.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>model fidelity multi-index</p> <p> TYPE: <code>tuple</code> </p> <code>beta</code> <p>surrogate fidelity multi-index</p> <p> TYPE: <code>tuple</code> </p> <code>update_nan</code> <p>try to fill <code>nan</code> with imputed values, otherwise just return the <code>nans</code> in place</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>tuple[ndarray, ndarray]</code> <p><code>xi, yi</code>, of size <code>(prod(grid_sizes), x_dim)</code> and <code>(prod(grid_sizes), y_dim)</code> respectively, the interpolation grid points and corresponding function values for this tensor-product grid</p> Source code in <code>src/amisc/component.py</code> <pre><code>def get_tensor_grid(self, alpha: tuple, beta: tuple, update_nan: bool = True) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Construct the `xi/yi` sub tensor-product grids for a given $(\\\\alpha, \\\\beta)$ multi-index.\n\n    :param alpha: model fidelity multi-index\n    :param beta: surrogate fidelity multi-index\n    :param update_nan: try to fill `nan` with imputed values, otherwise just return the `nans` in place\n    :returns: `xi, yi`, of size `(prod(grid_sizes), x_dim)` and `(prod(grid_sizes), y_dim)` respectively, the\n              interpolation grid points and corresponding function values for this tensor-product grid\n    \"\"\"\n    interp = self.surrogates[str(alpha)][str(beta)]\n    grid_sizes = interp.get_grid_sizes(beta)\n    coords = [list(range(grid_sizes[n])) for n in range(interp.xdim())]\n    xi = np.zeros((np.prod(grid_sizes), interp.xdim()), dtype=np.float32)\n    yi = np.zeros((np.prod(grid_sizes), self.ydim), dtype=np.float32)\n    for i, coord in enumerate(itertools.product(*coords)):\n        xi[i, :] = self.xi_map[str(alpha)][str(coord)]\n        yi_curr = self.yi_map[str(alpha)][str(coord)]\n        if update_nan and np.any(np.isnan(yi_curr)):\n            # Try to replace NaN values if they are stored\n            yi_curr = self.yi_nan_map[str(alpha)].get(str(coord), yi_curr)\n        yi[i, :] = yi_curr\n\n    return xi, yi\n</code></pre>"},{"location":"reference/component/#amisc.component.SparseGridSurrogate.get_training_data","title":"<code>get_training_data()</code>","text":"<p>Grab all <code>x,y</code> training data stored in the sparse grid for each model fidelity level \\(\\alpha\\).</p> RETURNS DESCRIPTION <code>tuple[dict[str:ndarray], dict[str:ndarray]]</code> <p><code>xi</code>, <code>yi</code>, each a <code>dict</code> mapping <code>alpha</code> indices to <code>np.ndarrays</code></p> Source code in <code>src/amisc/component.py</code> <pre><code>def get_training_data(self) -&gt; tuple[dict[str: np.ndarray], dict[str: np.ndarray]]:\n    \"\"\"Grab all `x,y` training data stored in the sparse grid for each model fidelity level $\\\\alpha$.\n\n    :returns: `xi`, `yi`, each a `dict` mapping `alpha` indices to `np.ndarrays`\n    \"\"\"\n    xi, yi = dict(), dict()\n    for alpha, x_map in self.xi_map.items():\n        x = np.zeros((len(x_map), len(self.x_vars)))\n        y = np.zeros((len(x_map), self.ydim))\n        for i, (coord, x_coord) in enumerate(x_map.items()):\n            x[i, :] = x_coord\n            y[i, :] = self.yi_nan_map[alpha].get(coord, self.yi_map[alpha][coord])\n\n        xi[alpha] = x\n        yi[alpha] = y\n\n    return xi, yi\n</code></pre>"},{"location":"reference/component/#amisc.component.SparseGridSurrogate.update_yi","title":"<code>update_yi(alpha, beta, yi_dict)</code>","text":"<p>Helper method to update <code>yi</code> values, accounting for possible <code>nans</code> by regression imputation.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>the model fidelity indices</p> <p> TYPE: <code>tuple</code> </p> <code>beta</code> <p>the surrogate fidelity indices</p> <p> TYPE: <code>tuple</code> </p> <code>yi_dict</code> <p>a <code>dict</code> mapping <code>str(coord)</code> grid coordinates to function values</p> <p> TYPE: <code>dict[str:ndarray]</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def update_yi(self, alpha: tuple, beta: tuple, yi_dict: dict[str: np.ndarray]):\n    \"\"\"Helper method to update `yi` values, accounting for possible `nans` by regression imputation.\n\n    :param alpha: the model fidelity indices\n    :param beta: the surrogate fidelity indices\n    :param yi_dict: a `dict` mapping `str(coord)` grid coordinates to function values\n    \"\"\"\n    self.yi_map[str(alpha)].update(yi_dict)\n    imputer, xdim = None, len(self.x_vars)\n    for grid_coord, yi in yi_dict.items():\n        if np.any(np.isnan(yi)):\n            if imputer is None:\n                # Grab all 'good' interpolation points and train a simple linear regression fit\n                xi_mat, yi_mat = np.zeros((0, xdim)), np.zeros((0, self.ydim))\n                for coord, xi in self.xi_map[str(alpha)].items():\n                    if coord not in self.yi_nan_map[str(alpha)] and coord in self.yi_map[str(alpha)]:\n                        yi_add = self.yi_map[str(alpha)][str(coord)]\n                        xi_mat = np.concatenate((xi_mat, xi.reshape((1, xdim))), axis=0)\n                        yi_mat = np.concatenate((yi_mat, yi_add.reshape((1, self.ydim))), axis=0)\n                nan_idx = np.any(np.isnan(yi_mat), axis=-1)\n                xi_mat = xi_mat[~nan_idx, :]\n                yi_mat = yi_mat[~nan_idx, :]\n                imputer = Pipeline([('scaler', MaxAbsScaler()), ('model', Ridge(alpha=1))])\n                imputer.fit(xi_mat, yi_mat)\n            x_interp = self.xi_map[str(alpha)][str(grid_coord)].reshape((1, xdim))\n            y_interp = np.atleast_1d(np.squeeze(imputer.predict(x_interp)))\n            nan_idx = np.isnan(yi)\n            y_interp[~nan_idx] = yi[~nan_idx]   # Only keep imputed values where yi is nan\n            self.yi_nan_map[str(alpha)][str(grid_coord)] = y_interp\n\n    # Go back and try to re-interpolate old nan values as more points get added to the grid\n    if imputer is not None:\n        for grid_coord in list(self.yi_nan_map[str(alpha)].keys()):\n            if grid_coord not in yi_dict:\n                x_interp = self.xi_map[str(alpha)][str(grid_coord)].reshape((1, xdim))\n                y_interp = imputer.predict(x_interp)\n                self.yi_nan_map[str(alpha)][str(grid_coord)] = np.atleast_1d(np.squeeze(y_interp))\n</code></pre>"},{"location":"reference/component/#amisc.component.SparseGridSurrogate.get_sub_surrogate","title":"<code>get_sub_surrogate(alpha, beta, include_grid=False)</code>","text":"<p>Get the specific sub-surrogate corresponding to the \\((\\alpha, \\beta)\\) fidelity.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>A multi-index specifying model fidelity</p> <p> TYPE: <code>tuple</code> </p> <code>beta</code> <p>A multi-index specifying surrogate fidelity</p> <p> TYPE: <code>tuple</code> </p> <code>include_grid</code> <p>whether to add the <code>xi/yi</code> interpolation points to the returned <code>BaseInterpolator</code> object</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>BaseInterpolator</code> <p>the <code>BaseInterpolator</code> object corresponding to \\((\\alpha, \\beta)\\)</p> Source code in <code>src/amisc/component.py</code> <pre><code>def get_sub_surrogate(self, alpha: tuple, beta: tuple, include_grid: bool = False) -&gt; BaseInterpolator:\n    \"\"\"Get the specific sub-surrogate corresponding to the $(\\\\alpha, \\\\beta)$ fidelity.\n\n    :param alpha: A multi-index specifying model fidelity\n    :param beta: A multi-index specifying surrogate fidelity\n    :param include_grid: whether to add the `xi/yi` interpolation points to the returned `BaseInterpolator` object\n    :returns: the `BaseInterpolator` object corresponding to $(\\\\alpha, \\\\beta)$\n    \"\"\"\n    interp = super().get_sub_surrogate(alpha, beta)\n    if include_grid:\n        interp.xi, interp.yi = self.get_tensor_grid(alpha, beta)\n    return interp\n</code></pre>"},{"location":"reference/component/#amisc.component.SparseGridSurrogate.build_interpolator","title":"<code>build_interpolator(alpha, beta)</code>","text":"<p>Abstract method implementation for constructing the tensor-product grid interpolator.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def build_interpolator(self, alpha, beta):\n    \"\"\"Abstract method implementation for constructing the tensor-product grid interpolator.\"\"\"\n    # Create a new tensor-product grid interpolator for the base index (0, 0, ...)\n    if np.sum(beta) == 0:\n        kwargs = copy.deepcopy(self._model_kwargs)\n        if len(alpha) &gt; 0:\n            kwargs['alpha'] = alpha\n        interp = LagrangeInterpolator(beta, self.x_vars, model=self._model, model_args=self._model_args,\n                                      model_kwargs=kwargs, init_grids=True, reduced=True)\n        x_pt = np.array([float(interp.x_grids[n][beta[n]]) for n in range(interp.xdim())], dtype=np.float32)\n        self.curr_max_beta[str(alpha)] = list(beta)\n        self.x_grids[str(alpha)] = copy.deepcopy(interp.x_grids)\n        self.xi_map[str(alpha)] = {str(beta): x_pt}\n        self.yi_map[str(alpha)] = dict()\n        self.yi_nan_map[str(alpha)] = dict()\n        if self.save_enabled():\n            self.yi_files[str(alpha)] = dict()\n\n        return [beta], x_pt.reshape((1, len(self.x_vars))), interp\n    # Otherwise, all other indices are a refinement of previous grids\n\n    # Look for first multi-index neighbor that is one level of refinement away\n    refine_tup = None\n    for beta_old_str in list(self.surrogates[str(alpha)].keys()):\n        beta_old = ast.literal_eval(beta_old_str)\n        if self.is_one_level_refinement(beta_old, beta):\n            idx_refine = int(np.nonzero(np.array(beta, dtype=int) - np.array(beta_old, dtype=int))[0][0])\n            refine_level = beta[idx_refine]\n            if refine_level &gt; self.curr_max_beta[str(alpha)][idx_refine]:\n                # Generate next refinement grid and save (refine_tup = tuple(x_new_idx, x_new, interp))\n                refine_tup = self.surrogates[str(alpha)][beta_old_str].refine(beta, auto=False)\n                self.curr_max_beta[str(alpha)][idx_refine] = refine_level\n                self.x_grids[str(alpha)][idx_refine] = copy.deepcopy(refine_tup[2].x_grids[idx_refine])\n            else:\n                # Access the refinement grid from memory (it is already computed)\n                num_pts = self.surrogates[str(alpha)][beta_old_str].get_grid_sizes(beta)[idx_refine]\n                x_refine = self.x_grids[str(alpha)][idx_refine][:num_pts]\n                refine_tup = self.surrogates[str(alpha)][beta_old_str].refine(beta, x_refine=x_refine,\n                                                                              auto=False)\n            break  # Only need to grab one neighbor\n\n    # Gather new interpolation grid points\n    x_new_idx, x_new, interp = refine_tup\n    xn_coord = []   # Save multi-index coordinates of points to compute model at for refinement\n    xn_pts = np.zeros((0, interp.xdim()), dtype=np.float32)     # Save physical x location of new points\n    for i, multi_idx in enumerate(x_new_idx):\n        if str(multi_idx) not in self.yi_map[str(alpha)]:\n            # We have not computed this grid coordinate yet\n            xn_coord.append(multi_idx)\n            xn_pts = np.concatenate((xn_pts, x_new[i, np.newaxis, :]), axis=0)  # (N_new, xdim)\n            self.xi_map[str(alpha)][str(multi_idx)] = x_new[i, :]\n\n    return xn_coord, xn_pts, interp\n</code></pre>"},{"location":"reference/component/#amisc.component.SparseGridSurrogate.update_interpolator","title":"<code>update_interpolator(x_new_idx, x_new, interp)</code>","text":"<p>Awkward solution, I know, but actually compute and save the model evaluations here.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def update_interpolator(self, x_new_idx, x_new, interp):\n    \"\"\"Awkward solution, I know, but actually compute and save the model evaluations here.\"\"\"\n    # Compute and store model output at new refinement points in a hash structure\n    yi_ret = interp.set_yi(x_new=(x_new_idx, x_new))\n\n    if self.ydim is None:\n        for coord_str, yi in yi_ret['y'].items():\n            self.ydim = yi.shape[0]\n            break\n\n    alpha = interp._model_kwargs.get('alpha', ())\n    self.update_yi(alpha, interp.beta, yi_ret['y'])\n    if self.save_enabled():\n        self.yi_files[str(alpha)].update(yi_ret['files'])\n    cost = interp.model_cost * len(x_new_idx)\n\n    return cost\n</code></pre>"},{"location":"reference/component/#amisc.component.SparseGridSurrogate.parallel_add_candidates","title":"<code>parallel_add_candidates(candidates, executor)</code>","text":"<p>Work-around to make sure mutable instance variable changes are made before/after splitting tasks using this method over parallel (potentially MPI) workers. You can pass if you are not interested in such parallel ideas.</p> <p>Warning</p> <p>MPI workers cannot save changes to <code>self</code> so this method should only distribute static tasks to the workers.</p> PARAMETER DESCRIPTION <code>candidates</code> <p>list of [(alpha, beta),...] multi-indices</p> <p> TYPE: <code>IndexSet</code> </p> <code>executor</code> <p>the executor used to iterate candidates in parallel</p> <p> TYPE: <code>Executor</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def parallel_add_candidates(self, candidates: IndexSet, executor: Executor):\n    \"\"\"Work-around to make sure mutable instance variable changes are made before/after\n    splitting tasks using this method over parallel (potentially MPI) workers. You can pass if you are not\n    interested in such parallel ideas.\n\n    !!! Warning\n        MPI workers cannot save changes to `self` so this method should only distribute static tasks to the workers.\n\n    :param candidates: list of [(alpha, beta),...] multi-indices\n    :param executor: the executor used to iterate candidates in parallel\n    \"\"\"\n    # Do sequential tasks first (i.e. make mutable changes to self), build up parallel task args\n    task_args = []\n    for alpha, beta in candidates:\n        x_new_idx, x_new, interp = self.build_interpolator(alpha, beta)\n        task_args.append((alpha, beta, x_new_idx, x_new, interp))\n\n    def parallel_task(alpha, beta, x_new_idx, x_new, interp):\n        # Must return anything you want changed in self or interp (mutable changes aren't saved over MPI workers)\n        logger = get_logger(self.__class__.__name__, log_file=self.log_file, stdout=False)\n        logger.info(f'Building interpolator for index {(alpha, beta)} ...')\n        yi_ret = interp.set_yi(x_new=(x_new_idx, x_new))\n        model_cost = interp.model_cost if interp.model_cost is not None else 1\n        return yi_ret, model_cost\n\n    # Wait for all parallel workers to return\n    fs = [executor.submit(parallel_task, *args) for args in task_args]\n    wait(fs, timeout=None, return_when=ALL_COMPLETED)\n\n    # Update self and interp with the results from all workers (and check for errors)\n    for i, future in enumerate(fs):\n        try:\n            a = task_args[i][0]\n            b = task_args[i][1]\n            x_new_idx = task_args[i][2]\n            interp = task_args[i][4]\n            yi_ret, model_cost = future.result()\n            interp.model_cost = model_cost\n            self.surrogates[str(a)][str(b)] = interp\n            self.update_yi(a, b, yi_ret['y'])\n            if self.save_enabled():\n                self.yi_files[str(a)].update(yi_ret['files'])\n            self.costs[str(a)][str(b)] = interp.model_cost * len(x_new_idx)\n\n            if self.ydim is None:\n                for coord_str, yi in self.yi_map[str(a)].items():\n                    self.ydim = yi.shape[0]\n                    break\n        except:\n            self.logger.error(f'An exception occurred in a thread handling build_interpolator{candidates[i]}')\n            raise\n</code></pre>"},{"location":"reference/component/#amisc.component.AnalyticalSurrogate","title":"<code>AnalyticalSurrogate(x_vars, model, *args, **kwargs)</code>","text":"<p>               Bases: <code>ComponentSurrogate</code></p> <p>Concrete \"surrogate\" class that just uses the analytical model (i.e. bypasses surrogate evaluation).</p> <p>Initializes a stand-in <code>ComponentSurrogate</code> with all unnecessary fields set to empty.</p> <p>Warning</p> <p>This overwrites anything passed in for <code>truth_alpha</code>, <code>max_alpha</code>, <code>max_beta</code>, or <code>multi_index</code> since these are not used for an analytical model.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def __init__(self, x_vars, model, *args, **kwargs):\n    \"\"\"Initializes a stand-in `ComponentSurrogate` with all unnecessary fields set to empty.\n\n    !!! Warning\n        This overwrites anything passed in for `truth_alpha`, `max_alpha`, `max_beta`, or `multi_index` since\n        these are not used for an analytical model.\n    \"\"\"\n    kwargs['truth_alpha'] = ()\n    kwargs['max_alpha'] = ()\n    kwargs['max_beta'] = ()\n    kwargs['multi_index'] = []\n    super().__init__(x_vars, model, *args, **kwargs)\n</code></pre>"},{"location":"reference/component/#amisc.component.AnalyticalSurrogate.predict","title":"<code>predict(x, **kwargs)</code>","text":"<p>Evaluate the analytical model at points <code>x</code>, ignore extra <code>**kwargs</code> passed in.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>(..., x_dim)</code> the points to be evaluated</p> <p> TYPE: <code>ndarray | float</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., y_dim)</code> the exact model output at the input points</p> Source code in <code>src/amisc/component.py</code> <pre><code>def predict(self, x: np.ndarray | float, **kwargs) -&gt; np.ndarray:\n    \"\"\"Evaluate the analytical model at points `x`, ignore extra `**kwargs` passed in.\n\n    :param x: `(..., x_dim)` the points to be evaluated\n    :returns y: `(..., y_dim)` the exact model output at the input points\n    \"\"\"\n    ret = self._model(x, *self._model_args, **self._model_kwargs)\n\n    if not isinstance(ret, dict):\n        self.logger.warning(f\"Function {self._model} did not return a dict of the form {{'y': y}}. Please make sure\"\n                            f\" you do so to avoid conflicts. Returning the value directly instead...\")\n\n    return ret['y'] if isinstance(ret, dict) else ret\n</code></pre>"},{"location":"reference/component/#amisc.component.AnalyticalSurrogate.grad","title":"<code>grad(x, training=False, index_set=None)</code>","text":"<p>Use auto-diff to compute derivative of an analytical model. Model must be implemented with <code>numpy</code>.</p> <p>Not implemented yet</p> <p>Hypothetically, auto-diff libraries like <code>jax</code> could be used to write a generic gradient function here for analytical models implemented directly in Python/numpy. But there are a lot of quirks that should be worked out first.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def grad(self, x, training=False, index_set=None):\n    \"\"\"Use auto-diff to compute derivative of an analytical model. Model must be implemented with `numpy`.\n\n    !!! Warning \"Not implemented yet\"\n        Hypothetically, auto-diff libraries like `jax` could be used to write a generic gradient function here for\n        analytical models implemented directly in Python/numpy. But there are a lot of quirks that should be worked\n        out first.\n    \"\"\"\n    raise NotImplementedError('Need to implement a generic auto-diff function here, like using jax for example.')\n</code></pre>"},{"location":"reference/component/#amisc.component.AnalyticalSurrogate.hessian","title":"<code>hessian(x, training=False, index_set=None)</code>","text":"<p>Use auto-diff to compute derivative of an analytical model. Model must be implemented with <code>numpy</code>.</p> <p>Not implemented yet</p> <p>Hypothetically, auto-diff libraries like <code>jax</code> could be used to write a generic Hessian function here for analytical models implemented directly in Python/numpy. But there are a lot of quirks that should be worked out first.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def hessian(self, x, training=False, index_set=None):\n    \"\"\"Use auto-diff to compute derivative of an analytical model. Model must be implemented with `numpy`.\n\n    !!! Warning \"Not implemented yet\"\n        Hypothetically, auto-diff libraries like `jax` could be used to write a generic Hessian function here for\n        analytical models implemented directly in Python/numpy. But there are a lot of quirks that should be worked\n        out first.\n    \"\"\"\n    raise NotImplementedError('Need to implement a generic auto-diff function here, like using jax for example.')\n</code></pre>"},{"location":"reference/component/#amisc.component.AnalyticalSurrogate.activate_index","title":"<code>activate_index(*args)</code>","text":"<p>Do nothing</p> Source code in <code>src/amisc/component.py</code> <pre><code>def activate_index(self, *args):\n    \"\"\"Do nothing\"\"\"\n    pass\n</code></pre>"},{"location":"reference/component/#amisc.component.AnalyticalSurrogate.add_surrogate","title":"<code>add_surrogate(*args)</code>","text":"<p>Do nothing</p> Source code in <code>src/amisc/component.py</code> <pre><code>def add_surrogate(self, *args):\n    \"\"\"Do nothing\"\"\"\n    pass\n</code></pre>"},{"location":"reference/component/#amisc.component.AnalyticalSurrogate.init_coarse","title":"<code>init_coarse()</code>","text":"<p>Do nothing</p> Source code in <code>src/amisc/component.py</code> <pre><code>def init_coarse(self):\n    \"\"\"Do nothing\"\"\"\n    pass\n</code></pre>"},{"location":"reference/component/#amisc.component.AnalyticalSurrogate.update_misc_coeffs","title":"<code>update_misc_coeffs(**kwargs)</code>","text":"<p>Do nothing</p> Source code in <code>src/amisc/component.py</code> <pre><code>def update_misc_coeffs(self, **kwargs):\n    \"\"\"Do nothing\"\"\"\n    pass\n</code></pre>"},{"location":"reference/component/#amisc.component.AnalyticalSurrogate.get_sub_surrogate","title":"<code>get_sub_surrogate(*args)</code>","text":"<p>Nothing to return for analytical model</p> Source code in <code>src/amisc/component.py</code> <pre><code>def get_sub_surrogate(self, *args):\n    \"\"\"Nothing to return for analytical model\"\"\"\n    return None\n</code></pre>"},{"location":"reference/component/#amisc.component.AnalyticalSurrogate.get_cost","title":"<code>get_cost(*args)</code>","text":"<p>Return no cost for analytical model</p> Source code in <code>src/amisc/component.py</code> <pre><code>def get_cost(self, *args):\n    \"\"\"Return no cost for analytical model\"\"\"\n    return 0\n</code></pre>"},{"location":"reference/component/#amisc.component.AnalyticalSurrogate.build_interpolator","title":"<code>build_interpolator(*args)</code>","text":"<p>Abstract method implementation, return none for an analytical model</p> Source code in <code>src/amisc/component.py</code> <pre><code>def build_interpolator(self, *args):\n    \"\"\"Abstract method implementation, return none for an analytical model\"\"\"\n    return None\n</code></pre>"},{"location":"reference/component/#amisc.component.AnalyticalSurrogate.update_interpolator","title":"<code>update_interpolator(*args)</code>","text":"<p>Abstract method implementation, return <code>cost=0</code> for an analytical model</p> Source code in <code>src/amisc/component.py</code> <pre><code>def update_interpolator(self, *args):\n    \"\"\"Abstract method implementation, return `cost=0` for an analytical model\"\"\"\n    return 0\n</code></pre>"},{"location":"reference/component/#amisc.component.AnalyticalSurrogate.parallel_add_candidates","title":"<code>parallel_add_candidates(*args)</code>","text":"<p>Abstract method implementation, do nothing</p> Source code in <code>src/amisc/component.py</code> <pre><code>def parallel_add_candidates(self, *args):\n    \"\"\"Abstract method implementation, do nothing\"\"\"\n    pass\n</code></pre>"},{"location":"reference/interpolator/","title":"interpolator","text":""},{"location":"reference/interpolator/#amisc.interpolator","title":"<code>amisc.interpolator</code>","text":"<p>Provides interpolator classes. Interpolators manage training data and specify how to refine/gather more data.</p> <p>Includes:</p> <ul> <li><code>BaseInterpolator</code>: Abstract class providing basic structure of an interpolator</li> <li><code>LagrangeInterpolator</code>: Concrete implementation for tensor-product barycentric Lagrange interpolation</li> </ul>"},{"location":"reference/interpolator/#amisc.interpolator.BaseInterpolator","title":"<code>BaseInterpolator(beta, x_vars, xi=None, yi=None, model=None, model_args=(), model_kwargs=None)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base interpolator abstract class.</p> <p>Setting the training data</p> <p>You can leave the training data <code>xi</code>, <code>yi</code> empty; they can be iteratively refined later on.</p> <p>Model specification</p> <p>The model is a callable function of the form <code>ret = model(x, *args, **kwargs)</code>. The return value is a dictionary of the form <code>ret = {'y': y, 'files': files, 'cost': cost}</code>. In the return dictionary, you specify the raw model output <code>y</code> as an <code>np.ndarray</code> at a minimum. Optionally, you can specify paths to output files and the average model cost (in units of seconds of cpu time), and anything else you want.</p> ATTRIBUTE DESCRIPTION <code>beta</code> <p>specifies the refinement level of this interpolator as a set of natural number indices</p> <p> TYPE: <code>tuple[int, ...]</code> </p> <code>x_vars</code> <p>list of variables that fully determines the input domain of interest for interpolation</p> <p> TYPE: <code>list[BaseRV]</code> </p> <code>xi</code> <p><code>(Nx, x_dim)</code>, interpolation points (or knots, training samples, etc.) stored as an array</p> <p> TYPE: <code>np.ndarray</code> </p> <code>yi</code> <p><code>(Nx, y_dim)</code>, function values at the interpolation points, i.e. the training data</p> <p> TYPE: <code>np.ndarray</code> </p> <code>_model</code> <p>stores a ref to the model or function that is to be interpolated, callable as <code>ret = model(x)</code></p> <p> TYPE: <code>callable[np.ndarray] -&gt; dict</code> </p> <code>_model_args</code> <p>additional arguments to supply to the model</p> <p> TYPE: <code>tuple</code> </p> <code>_model_kwargs</code> <p>additional keyword arguments to supply to the model</p> <p> TYPE: <code>dict</code> </p> <code>model_cost</code> <p>the average total cpu time (in seconds) for a single model evaluation call of one set of inputs</p> <p> TYPE: <code>float</code> </p> <code>output_files</code> <p>tracks model output files corresponding to <code>yi</code> training data (for more complex models)</p> <p> TYPE: <code>list[str | Path]</code> </p> <code>logger</code> <p>a logging utility reference</p> <p> TYPE: <code>logging.Logger</code> </p> <p>Construct the interpolator.</p> PARAMETER DESCRIPTION <code>beta</code> <p>refinement level indices</p> <p> TYPE: <code>tuple</code> </p> <code>x_vars</code> <p>list of variables to specify input domain of interpolation</p> <p> TYPE: <code>BaseRV | list[BaseRV]</code> </p> <code>xi</code> <p><code>(Nx, xdim)</code>, interpolation points (optional)</p> <p> DEFAULT: <code>None</code> </p> <code>yi</code> <p><code>(Nx, ydim)</code>, the function values at the interpolation points (optional)</p> <p> DEFAULT: <code>None</code> </p> <code>model</code> <p>callable as {'y': y} = model(x), with <code>x = (..., x_dim)</code>, <code>y = (..., y_dim)</code></p> <p> DEFAULT: <code>None</code> </p> <code>model_args</code> <p>optional args for the model</p> <p> DEFAULT: <code>()</code> </p> <code>model_kwargs</code> <p>optional kwargs for the model</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def __init__(self, beta: tuple, x_vars: BaseRV | list[BaseRV], xi=None, yi=None,\n             model=None, model_args=(), model_kwargs=None):\n    \"\"\"Construct the interpolator.\n\n    :param beta: refinement level indices\n    :param x_vars: list of variables to specify input domain of interpolation\n    :param xi: `(Nx, xdim)`, interpolation points (optional)\n    :param yi: `(Nx, ydim)`, the function values at the interpolation points (optional)\n    :param model: callable as {'y': y} = model(x), with `x = (..., x_dim)`, `y = (..., y_dim)`\n    :param model_args: optional args for the model\n    :param model_kwargs: optional kwargs for the model\n    \"\"\"\n    x_vars = [x_vars] if not isinstance(x_vars, list) else x_vars\n    self.logger = get_logger(self.__class__.__name__)\n    self._model = model\n    self._model_args = model_args\n    self._model_kwargs = model_kwargs if model_kwargs is not None else {}\n    self.output_files = []                              # Save output files with same indexing as xi, yi\n    self.xi = xi                                        # Interpolation points\n    self.yi = yi                                        # Function values at interpolation points\n    self.beta = beta                                    # Refinement level indices\n    self.x_vars = x_vars                                # BaseRV() objects for each input\n    self.model_cost = None                              # Total cpu time to evaluate model once (s)\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.BaseInterpolator.__call__","title":"<code>__call__(x)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the interpolation at points <code>x</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>(..., x_dim)</code>, the points to be interpolated, must be within the input domain for accuracy</p> <p> TYPE: <code>ndarray | float</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., y_dim)</code>, the interpolated function values</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>@abstractmethod\ndef __call__(self, x: np.ndarray | float) -&gt; np.ndarray:\n    \"\"\"Evaluate the interpolation at points `x`.\n\n    :param x: `(..., x_dim)`, the points to be interpolated, must be within the input domain for accuracy\n    :returns y: `(..., y_dim)`, the interpolated function values\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.BaseInterpolator._fmt_input","title":"<code>_fmt_input(x)</code>","text":"<p>Helper function to make sure input <code>x</code> is an ndarray of shape <code>(..., xdim)</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p>if 1d-like as (n,), then converted to 2d as (1, n) if n==xdim or (n, 1) if xdim==1</p> <p> TYPE: <code>float | list | ndarray</code> </p> RETURNS DESCRIPTION <code>tuple[bool, ndarray]</code> <p><code>x</code> as at least a 2d array <code>(..., xdim)</code>, and whether <code>x</code> was originally 1d-like</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def _fmt_input(self, x: float | list | np.ndarray) -&gt; tuple[bool, np.ndarray]:\n    \"\"\"Helper function to make sure input `x` is an ndarray of shape `(..., xdim)`.\n\n    :param x: if 1d-like as (n,), then converted to 2d as (1, n) if n==xdim or (n, 1) if xdim==1\n    :returns: `x` as at least a 2d array `(..., xdim)`, and whether `x` was originally 1d-like\n    \"\"\"\n    x = np.atleast_1d(x)\n    shape_1d = len(x.shape) == 1\n    if shape_1d:\n        if x.shape[0] != self.xdim() and self.xdim() &gt; 1:\n            raise ValueError(f'Input x shape {x.shape} is incompatible with xdim of {self.xdim()}')\n        x = np.expand_dims(x, axis=0 if x.shape[0] == self.xdim() else 1)\n\n    return shape_1d, x\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.BaseInterpolator.grad","title":"<code>grad(x, xi=None, yi=None)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the gradient/Jacobian at points <code>x</code> using the interpolator.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>(..., xdim)</code>, the evaluation points, must be within domain of <code>self.xi</code> for accuracy</p> <p> TYPE: <code>ndarray | float | list</code> </p> <code>xi</code> <p><code>(Ni, xdim)</code> optional, interpolation grid points to use (e.g. if <code>self.reduced=True</code>)</p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> <code>yi</code> <p><code>(Ni, ydim)</code> optional, function values at xi to use (e.g. if <code>self.reduced=True</code>)</p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., ydim, xdim)</code>, the Jacobian at points <code>x</code></p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>@abstractmethod\ndef grad(self, x: np.ndarray | float | list, xi: np.ndarray = None, yi: np.ndarray = None) -&gt; np.ndarray:\n    \"\"\"Evaluate the gradient/Jacobian at points `x` using the interpolator.\n\n    :param x: `(..., xdim)`, the evaluation points, must be within domain of `self.xi` for accuracy\n    :param xi: `(Ni, xdim)` optional, interpolation grid points to use (e.g. if `self.reduced=True`)\n    :param yi: `(Ni, ydim)` optional, function values at xi to use (e.g. if `self.reduced=True`)\n    :returns jac: `(..., ydim, xdim)`, the Jacobian at points `x`\n    \"\"\"\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.BaseInterpolator.hessian","title":"<code>hessian(x, xi=None, yi=None)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the Hessian at points <code>x</code> using the interpolator.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>(..., xdim)</code>, the evaluation points, must be within domain of <code>self.xi</code> for accuracy</p> <p> TYPE: <code>ndarray | float | list</code> </p> <code>xi</code> <p><code>(Ni, xdim)</code> optional, interpolation grid points to use (e.g. if <code>self.reduced=True</code>)</p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> <code>yi</code> <p><code>(Ni, ydim)</code> optional, function values at xi to use (e.g. if <code>self.reduced=True</code>)</p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., ydim, xdim, xdim)</code>, the Hessian at points <code>x</code></p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>@abstractmethod\ndef hessian(self, x: np.ndarray | float | list, xi: np.ndarray = None, yi: np.ndarray = None) -&gt; np.ndarray:\n    \"\"\"Evaluate the Hessian at points `x` using the interpolator.\n\n    :param x: `(..., xdim)`, the evaluation points, must be within domain of `self.xi` for accuracy\n    :param xi: `(Ni, xdim)` optional, interpolation grid points to use (e.g. if `self.reduced=True`)\n    :param yi: `(Ni, ydim)` optional, function values at xi to use (e.g. if `self.reduced=True`)\n    :returns hess: `(..., ydim, xdim, xdim)`, the Hessian at points `x`\n    \"\"\"\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.BaseInterpolator.refine","title":"<code>refine(beta, auto=True)</code>  <code>abstractmethod</code>","text":"<p>Return a new interpolator with one dimension refined by one level, as specified by <code>beta</code>.</p> <p>When you want to compute the model manually</p> <p>You can set <code>auto=False</code>, in which case the newly refined interpolation points <code>x</code> will be returned to you along with their indices, in the form <code>idx, x, interp = refine(beta, auto=False)</code>. You might also want to do this if you did not provide a model when constructing the Interpolator (so <code>auto=True</code> won't work).</p> PARAMETER DESCRIPTION <code>beta</code> <p>the new refinement level indices, should only refine one dimension by one level</p> <p> TYPE: <code>tuple</code> </p> <code>auto</code> <p>whether to automatically compute and store model at refinement points (default is True)</p> <p> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <p><code>idx</code> - indices into <code>xi</code>, <code>x</code> - the new interpolation points, and <code>interp</code> - a refined BaseInterpolator object, just returns <code>interp</code> if <code>auto=True</code></p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>@abstractmethod\ndef refine(self, beta: tuple, auto=True):\n    \"\"\"Return a new interpolator with one dimension refined by one level, as specified by `beta`.\n\n    !!! Info \"When you want to compute the model manually\"\n        You can set `auto=False`, in which case the newly refined interpolation points `x` will be returned to you\n        along with their indices, in the form `idx, x, interp = refine(beta, auto=False)`. You might also want to\n        do this if you did not provide a model when constructing the Interpolator (so `auto=True` won't work).\n\n    :param beta: the new refinement level indices, should only refine one dimension by one level\n    :param auto: whether to automatically compute and store model at refinement points (default is True)\n    :returns: `idx` - indices into `xi`, `x` - the new interpolation points, and `interp` - a refined\n               BaseInterpolator object, just returns `interp` if `auto=True`\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.BaseInterpolator.save_enabled","title":"<code>save_enabled()</code>","text":"<p>Return whether the underlying model wants to save outputs to file.</p> <p>Note</p> <p>You can specify that a model wants to save outputs to file by providing an <code>'output_dir'</code> kwarg.</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def save_enabled(self):\n    \"\"\"Return whether the underlying model wants to save outputs to file.\n\n    !!! Note\n        You can specify that a model wants to save outputs to file by providing an `'output_dir'` kwarg.\n    \"\"\"\n    return self._model_kwargs.get('output_dir') is not None\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.BaseInterpolator.set_yi","title":"<code>set_yi(yi=None, model=None, x_new=())</code>","text":"<p>Set the training data; if <code>yi=None</code>, then compute from the model.</p> <p>Warning</p> <p>You would use <code>x_new</code> if you wanted to compute the model at these specific locations and store the result. This will ignore anything passed in for <code>yi</code>, and it assumes a model is already specified (or passed in).</p> <p>Info</p> <p>You can pass in integer indices for <code>x_new</code> or tuple indices. Integers will index into <code>self.xi</code>. Tuples provide extra flexibility for more complicated indexing, e.g. they might specify indices along different coordinate directions in an N-dimensional grid. If you pass in a list of tuple indices for <code>x_new</code>, the resulting model outputs will be returned back to you in the form <code>dict[str: np.ndarray]</code>. The keys are string casts of the tuple indices, and the values are the corresponding model outputs.</p> PARAMETER DESCRIPTION <code>yi</code> <p><code>(Nx, y_dim)</code>, training data to set, must match dimension of <code>self.xi</code></p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> <code>model</code> <p>callable function, optionally overrides <code>self._model</code></p> <p> TYPE: <code>callable</code> DEFAULT: <code>None</code> </p> <code>x_new</code> <p>tuple of <code>(idx, x)</code>, where <code>x</code> is an <code>(N_new, x_dim)</code> array of new interpolation points to include and <code>idx</code> specifies the indices of these points into <code>self.xi</code></p> <p> TYPE: <code>tuple[list[int | tuple], ndarray]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>dict[str:ndarray] | None</code> <p>dict[str: np.ndarray] if <code>idx</code> contains tuple elements, otherwise <code>None</code></p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def set_yi(self, yi: np.ndarray = None, model: callable = None,\n           x_new: tuple[list[int | tuple], np.ndarray] = ()) -&gt; dict[str: np.ndarray] | None:\n    \"\"\"Set the training data; if `yi=None`, then compute from the model.\n\n    !!! Warning\n        You would use `x_new` if you wanted to compute the model at these specific locations and store the result.\n        This will ignore anything passed in for `yi`, and it assumes a model is already specified (or passed in).\n\n    !!! Info\n        You can pass in integer indices for `x_new` or tuple indices. Integers will index into `self.xi`. Tuples\n        provide extra flexibility for more complicated indexing, e.g. they might specify indices along different\n        coordinate directions in an N-dimensional grid. If you pass in a list of tuple indices for `x_new`, the\n        resulting model outputs will be returned back to you in the form `dict[str: np.ndarray]`. The keys are\n        string casts of the tuple indices, and the values are the corresponding model outputs.\n\n    :param yi: `(Nx, y_dim)`, training data to set, must match dimension of `self.xi`\n    :param model: callable function, optionally overrides `self._model`\n    :param x_new: tuple of `(idx, x)`, where `x` is an `(N_new, x_dim)` array of new interpolation points to\n                  include and `idx` specifies the indices of these points into `self.xi`\n    :returns: dict[str: np.ndarray] if `idx` contains tuple elements, otherwise `None`\n    \"\"\"\n    if model is not None:\n        self._model = model\n    if self._model is None:\n        error_msg = 'Model not specified for computing QoIs at interpolation grid points.'\n        self.logger.error(error_msg)\n        raise Exception(error_msg)\n\n    # Overrides anything passed in for yi (you would only be using this if yi was set previously)\n    if x_new:\n        new_idx = x_new[0]\n        new_x = x_new[1]\n        return_y = isinstance(new_idx[0], tuple)  # Return y rather than storing it if tuple indices are passed in\n        ret = dict(y=dict(), files=dict())\n        model_ret = self._model(new_x, *self._model_args, **self._model_kwargs)\n        if not isinstance(model_ret, dict):\n            self.logger.warning(\n                f\"Function {self._model} did not return a dict of the form {{'y': y}}. Please make sure\"\n                f\" you do so to avoid conflicts. Returning the value directly instead...\")\n            model_ret = dict(y=model_ret)\n        y_new, files_new, cpu_time = model_ret['y'], model_ret.get('files', None), model_ret.get('cost', 1)\n\n        if self.save_enabled():\n            for j in range(y_new.shape[0]):\n                if return_y:\n                    ret['y'][str(new_idx[j])] = y_new[j, :].astype(np.float32)\n                    ret['files'][str(new_idx[j])] = files_new[j]\n                else:\n                    self.yi[new_idx[j], :] = y_new[j, :].astype(np.float32)\n                    self.output_files[new_idx[j]] = files_new[j]\n        else:\n            for j in range(y_new.shape[0]):\n                if return_y:\n                    ret['y'][str(new_idx[j])] = y_new[j, :].astype(np.float32)\n                else:\n                    self.yi[new_idx[j], :] = y_new[j, :].astype(np.float32)\n\n        if self.model_cost is None:\n            self.model_cost = max(1, cpu_time)\n\n        return ret\n\n    # Set yi directly\n    if yi is not None:\n        self.yi = yi.astype(np.float32)\n        return\n\n    # Compute yi\n    model_ret = self._model(self.xi, *self._model_args, **self._model_kwargs)\n    if not isinstance(model_ret, dict):\n        self.logger.warning(f\"Function {self._model} did not return a dict of the form {{'y': y}}. Please make sure\"\n                            f\" you do so to avoid conflicts. Returning the value directly instead...\")\n        model_ret = dict(y=model_ret)\n\n    self.yi, self.output_files, cpu_time = model_ret['y'], model_ret.get('files', list()), model_ret.get('cost', 1)\n\n    if self.model_cost is None:\n        self.model_cost = max(1, cpu_time)\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.BaseInterpolator.update_input_bds","title":"<code>update_input_bds(idx, bds)</code>","text":"<p>Update the input bounds at the given index.</p> PARAMETER DESCRIPTION <code>idx</code> <p>the index of the input variable to update</p> <p> TYPE: <code>int</code> </p> <code>bds</code> <p>the new bounds for the variable</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def update_input_bds(self, idx: int, bds: tuple):\n    \"\"\"Update the input bounds at the given index.\n\n    :param idx: the index of the input variable to update\n    :param bds: the new bounds for the variable\n    \"\"\"\n    self.x_vars[idx].update_bounds(*bds)\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.BaseInterpolator.xdim","title":"<code>xdim()</code>","text":"<p>Get the dimension of the input domain.</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def xdim(self):\n    \"\"\"Get the dimension of the input domain.\"\"\"\n    return len(self.x_vars)\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.BaseInterpolator.ydim","title":"<code>ydim()</code>","text":"<p>Get the dimension of the outputs.</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def ydim(self):\n    \"\"\"Get the dimension of the outputs.\"\"\"\n    return self.yi.shape[-1] if self.yi is not None else None\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.LagrangeInterpolator","title":"<code>LagrangeInterpolator(beta, x_vars, init_grids=True, reduced=False, **kwargs)</code>","text":"<p>               Bases: <code>BaseInterpolator</code></p> <p>Tensor-product (multivariate) grid interpolator, based on barycentric Lagrange polynomials.</p> <p>Info</p> <p>The refinement level indices <code>beta</code> are used in this class to specify anisotropic refinement along each coordinate direction of the input domain, so <code>x_dim = len(beta)</code>.</p> ATTRIBUTE DESCRIPTION <code>x_grids</code> <p>univariate Leja sequence points in each 1d dimension</p> <p> TYPE: <code>list[np.ndarray]</code> </p> <code>weights</code> <p>the barycentric weights corresponding to <code>x_grids</code></p> <p> TYPE: <code>list[np.ndarray]</code> </p> <code>reduced</code> <p>whether to store <code>xi</code> and <code>yi</code> training data, can set to <code>False</code> to save memory, e.g. if an external sparse grid data structure manages this data instead</p> <p> TYPE: <code>bool</code> </p> <p>Initialize a Lagrange tensor-product grid interpolator.</p> PARAMETER DESCRIPTION <code>beta</code> <p>refinement level indices for each input dimension</p> <p> TYPE: <code>tuple</code> </p> <code>x_vars</code> <p>list of variables specifying bounds/pdfs for each input x</p> <p> TYPE: <code>BaseRV | list[BaseRV]</code> </p> <code>init_grids</code> <p>whether to compute 1d Leja sequences on initialization</p> <p> DEFAULT: <code>True</code> </p> <code>reduced</code> <p>whether to store xi/yi matrices, e.g. set true if storing in external sparse grid structure</p> <p> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>other optional arguments (see <code>BaseInterpolator</code>)</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def __init__(self, beta: tuple, x_vars: BaseRV | list[BaseRV], init_grids=True, reduced=False, **kwargs):\n    \"\"\"Initialize a Lagrange tensor-product grid interpolator.\n\n    :param beta: refinement level indices for each input dimension\n    :param x_vars: list of variables specifying bounds/pdfs for each input x\n    :param init_grids: whether to compute 1d Leja sequences on initialization\n    :param reduced: whether to store xi/yi matrices, e.g. set true if storing in external sparse grid structure\n    :param **kwargs: other optional arguments (see `BaseInterpolator`)\n    \"\"\"\n    self.weights = []   # Barycentric weights for each dimension\n    self.x_grids = []   # Univariate nested leja sequences in each dimension\n    self.reduced = reduced\n    super().__init__(beta, x_vars, **kwargs)\n\n    if init_grids:\n        # Construct 1d univariate Leja sequences in each dimension\n        grid_sizes = self.get_grid_sizes(self.beta)\n        self.x_grids = [self.leja_1d(grid_sizes[n], self.x_vars[n].bounds(),\n                                     wt_fcn=self.x_vars[n].pdf).astype(np.float32) for n in range(self.xdim())]\n\n        for n in range(self.xdim()):\n            Nx = grid_sizes[n]\n            bds = self.x_vars[n].bounds()\n            grid = self.x_grids[n]\n            C = (bds[1] - bds[0]) / 4.0  # Interval capacity (see Berrut and Trefethen 2004)\n            xj = grid.reshape((Nx, 1))\n            xi = grid.reshape((1, Nx))\n            dist = (xj - xi) / C\n            np.fill_diagonal(dist, 1)  # Ignore product when i==j\n            self.weights.append((1.0 / np.prod(dist, axis=1)).astype(np.float32))  # (Nx,)\n\n        # Cartesian product of univariate grids\n        if not self.reduced:\n            self.xi = np.empty((np.prod(grid_sizes), self.xdim()), dtype=np.float32)\n            for i, ele in enumerate(itertools.product(*self.x_grids)):\n                self.xi[i, :] = ele\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.LagrangeInterpolator.__call__","title":"<code>__call__(x, xi=None, yi=None)</code>","text":"<p>Evaluate the barycentric interpolation at points <code>x</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>(..., xdim)</code>, the points to be interpolated, must be within domain of <code>self.xi</code> for accuracy</p> <p> TYPE: <code>ndarray | float</code> </p> <code>xi</code> <p><code>(Ni, xdim)</code> optional, interpolation grid points to use (e.g. if <code>self.reduced=True</code>)</p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> <code>yi</code> <p><code>(Ni, ydim)</code> optional, function values at xi to use (e.g. if <code>self.reduced=True</code>)</p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., ydim)</code>, the interpolated function values</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def __call__(self, x: np.ndarray | float, xi: np.ndarray = None, yi: np.ndarray = None) -&gt; np.ndarray:\n    \"\"\"Evaluate the barycentric interpolation at points `x`.\n\n    :param x: `(..., xdim)`, the points to be interpolated, must be within domain of `self.xi` for accuracy\n    :param xi: `(Ni, xdim)` optional, interpolation grid points to use (e.g. if `self.reduced=True`)\n    :param yi: `(Ni, ydim)` optional, function values at xi to use (e.g. if `self.reduced=True`)\n    :returns y: `(..., ydim)`, the interpolated function values\n    \"\"\"\n    shape_1d, x = self._fmt_input(x)\n    if yi is None:\n        yi = self.yi.copy()\n    if xi is None:\n        xi = self.xi.copy()\n    xdim = xi.shape[-1]\n    ydim = yi.shape[-1]\n    dims = list(range(xdim))\n\n    nan_idx = np.any(np.isnan(yi), axis=-1)\n    if np.any(nan_idx):\n        # Use a simple linear regression fit to impute missing values (may have resulted from bad model outputs)\n        imputer = Pipeline([('scaler', MaxAbsScaler()), ('model', Ridge(alpha=1))])\n        imputer.fit(xi[~nan_idx, :], yi[~nan_idx, :])\n        yi[nan_idx, :] = imputer.predict(xi[nan_idx, :])\n\n    # Create ragged edge matrix of interpolation pts and weights\n    grid_sizes = self.get_grid_sizes(self.beta)     # For example:\n    x_j = np.empty((xdim, max(grid_sizes)))         # A= [#####--\n    w_j = np.empty((xdim, max(grid_sizes)))               #######\n    x_j[:] = np.nan                                       ###----]\n    w_j[:] = np.nan\n    for n in range(xdim):\n        x_j[n, :grid_sizes[n]] = self.x_grids[n]\n        w_j[n, :grid_sizes[n]] = self.weights[n]\n    diff = x[..., np.newaxis] - x_j\n    div_zero_idx = np.isclose(diff, 0, rtol=1e-4, atol=1e-8)\n    diff[div_zero_idx] = 1\n    quotient = w_j / diff                   # (..., xdim, Nx)\n    qsum = np.nansum(quotient, axis=-1)     # (..., xdim)\n\n    # Loop over multi-indices and compute tensor-product lagrange polynomials\n    y = np.zeros(x.shape[:-1] + (ydim,), dtype=x.dtype)    # (..., ydim)\n    indices = [np.arange(grid_sizes[n]) for n in range(xdim)]\n    for i, j in enumerate(itertools.product(*indices)):\n        L_j = quotient[..., dims, j] / qsum  # (..., xdim)\n        other_pts = np.copy(div_zero_idx)\n        other_pts[div_zero_idx[..., dims, j]] = False\n\n        # Set L_j(x==x_j)=1 for the current j and set L_j(x==x_j)=0 for x_j = x_i, i != j\n        L_j[div_zero_idx[..., dims, j]] = 1\n        L_j[np.any(other_pts, axis=-1)] = 0\n\n        # Add multivariate basis polynomial contribution to interpolation output\n        L_j = np.prod(L_j, axis=-1, keepdims=True)      # (..., 1)\n        y += L_j * yi[i, :]\n\n    return np.atleast_1d(np.squeeze(y)) if shape_1d else y\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.LagrangeInterpolator.get_grid_sizes","title":"<code>get_grid_sizes(beta, k=2)</code>  <code>staticmethod</code>","text":"<p>Compute number of grid points in each input dimension.</p> PARAMETER DESCRIPTION <code>beta</code> <p>refinement level indices</p> <p> TYPE: <code>tuple</code> </p> <code>k</code> <p>level-to-grid-size multiplier (probably just always <code>k=2</code>)</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> RETURNS DESCRIPTION <code>list[int]</code> <p>list of grid sizes in each dimension</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>@staticmethod\ndef get_grid_sizes(beta: tuple, k: int = 2) -&gt; list[int]:\n    \"\"\"Compute number of grid points in each input dimension.\n\n    :param beta: refinement level indices\n    :param k: level-to-grid-size multiplier (probably just always `k=2`)\n    :returns: list of grid sizes in each dimension\n    \"\"\"\n    return [k*beta[i] + 1 for i in range(len(beta))]\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.LagrangeInterpolator.grad","title":"<code>grad(x, xi=None, yi=None)</code>","text":"<p>Evaluate the gradient/Jacobian at points <code>x</code> using the interpolator.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>(..., xdim)</code>, the evaluation points, must be within domain of <code>self.xi</code> for accuracy</p> <p> TYPE: <code>ndarray | float | list</code> </p> <code>xi</code> <p><code>(Ni, xdim)</code> optional, interpolation grid points to use (e.g. if <code>self.reduced=True</code>)</p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> <code>yi</code> <p><code>(Ni, ydim)</code> optional, function values at xi to use (e.g. if <code>self.reduced=True</code>)</p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., ydim, xdim)</code>, the Jacobian at points <code>x</code></p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def grad(self, x: np.ndarray | float | list, xi: np.ndarray = None, yi: np.ndarray = None) -&gt; np.ndarray:\n    \"\"\"Evaluate the gradient/Jacobian at points `x` using the interpolator.\n\n    :param x: `(..., xdim)`, the evaluation points, must be within domain of `self.xi` for accuracy\n    :param xi: `(Ni, xdim)` optional, interpolation grid points to use (e.g. if `self.reduced=True`)\n    :param yi: `(Ni, ydim)` optional, function values at xi to use (e.g. if `self.reduced=True`)\n    :returns jac: `(..., ydim, xdim)`, the Jacobian at points `x`\n    \"\"\"\n    shape_1d, x = self._fmt_input(x)\n    if yi is None:\n        yi = self.yi.copy()\n    if xi is None:\n        xi = self.xi.copy()\n    xdim = xi.shape[-1]\n    ydim = yi.shape[-1]\n    nan_idx = np.any(np.isnan(yi), axis=-1)\n    if np.any(nan_idx):\n        # Use a simple linear regression fit to impute missing values (may have resulted from bad model outputs)\n        imputer = Pipeline([('scaler', MaxAbsScaler()), ('model', Ridge(alpha=1))])\n        imputer.fit(xi[~nan_idx, :], yi[~nan_idx, :])\n        yi[nan_idx, :] = imputer.predict(xi[nan_idx, :])\n\n    # Create ragged edge matrix of interpolation pts and weights\n    grid_sizes = self.get_grid_sizes(self.beta)     # For example:\n    x_j = np.empty((xdim, max(grid_sizes)))         # A= [#####--\n    w_j = np.empty((xdim, max(grid_sizes)))               #######\n    x_j[:] = np.nan                                       ###----]\n    w_j[:] = np.nan\n    for n in range(xdim):\n        x_j[n, :grid_sizes[n]] = self.x_grids[n]\n        w_j[n, :grid_sizes[n]] = self.weights[n]\n\n    # Compute values ahead of time that will be needed for the gradient\n    diff = x[..., np.newaxis] - x_j\n    div_zero_idx = np.isclose(diff, 0, rtol=1e-4, atol=1e-8)\n    diff[div_zero_idx] = 1\n    quotient = w_j / diff                           # (..., xdim, Nx)\n    qsum = np.nansum(quotient, axis=-1)             # (..., xdim)\n    sqsum = np.nansum(w_j / diff ** 2, axis=-1)     # (..., xdim)\n\n    # Loop over multi-indices and compute derivative of tensor-product lagrange polynomials\n    jac = np.zeros(x.shape[:-1] + (ydim, xdim), dtype=x.dtype)  # (..., ydim, xdim)\n    indices = [np.arange(grid_sizes[n]) for n in range(self.xdim())]\n    for k in range(xdim):\n        dims = [idx for idx in np.arange(xdim) if idx != k]\n        for i, j in enumerate(itertools.product(*indices)):\n            j_dims = [j[idx] for idx in dims]\n            L_j = quotient[..., dims, j_dims] / qsum[..., dims]  # (..., xdim-1)\n            other_pts = np.copy(div_zero_idx)\n            other_pts[div_zero_idx[..., list(np.arange(xdim)), j]] = False\n\n            # Set L_j(x==x_j)=1 for the current j and set L_j(x==x_j)=0 for x_j = x_i, i != j\n            L_j[div_zero_idx[..., dims, j_dims]] = 1\n            L_j[np.any(other_pts[..., dims, :], axis=-1)] = 0\n\n            # Partial derivative of L_j with respect to x_k\n            dLJ_dx = ((w_j[k, j[k]] / (qsum[..., k] * diff[..., k, j[k]])) *\n                      (sqsum[..., k] / qsum[..., k] - 1 / diff[..., k, j[k]]))\n\n            # Set derivatives when x is at the interpolation points (i.e. x==x_j)\n            p_idx = [idx for idx in np.arange(grid_sizes[k]) if idx != j[k]]\n            w_j_large = np.broadcast_to(w_j[k, :], x.shape[:-1] + w_j.shape[-1:]).copy()\n            curr_j_idx = div_zero_idx[..., k, j[k]]\n            other_j_idx = np.any(other_pts[..., k, :], axis=-1)\n            dLJ_dx[curr_j_idx] = -np.nansum((w_j[k, p_idx] / w_j[k, j[k]]) /\n                                            (x[curr_j_idx, k, np.newaxis] - x_j[k, p_idx]), axis=-1)\n            dLJ_dx[other_j_idx] = ((w_j[k, j[k]] / w_j_large[other_pts[..., k, :]]) /\n                                   (x[other_j_idx, k] - x_j[k, j[k]]))\n\n            dLJ_dx = np.expand_dims(dLJ_dx, axis=-1) * np.prod(L_j, axis=-1, keepdims=True)  # (..., 1)\n\n            # Add contribution to the Jacobian\n            jac[..., k] += dLJ_dx * yi[i, :]\n\n    return np.atleast_1d(np.squeeze(jac)) if shape_1d else jac\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.LagrangeInterpolator.hessian","title":"<code>hessian(x, xi=None, yi=None)</code>","text":"<p>Evaluate the Hessian at points <code>x</code> using the interpolator.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>(..., xdim)</code>, the evaluation points, must be within domain of <code>self.xi</code> for accuracy</p> <p> TYPE: <code>ndarray | float | list</code> </p> <code>xi</code> <p><code>(Ni, xdim)</code> optional, interpolation grid points to use (e.g. if <code>self.reduced=True</code>)</p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> <code>yi</code> <p><code>(Ni, ydim)</code> optional, function values at xi to use (e.g. if <code>self.reduced=True</code>)</p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., ydim, xdim, xdim)</code>, the vector Hessian at points <code>x</code></p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def hessian(self, x: np.ndarray | float | list, xi: np.ndarray = None, yi: np.ndarray = None) -&gt; np.ndarray:\n    \"\"\"Evaluate the Hessian at points `x` using the interpolator.\n\n    :param x: `(..., xdim)`, the evaluation points, must be within domain of `self.xi` for accuracy\n    :param xi: `(Ni, xdim)` optional, interpolation grid points to use (e.g. if `self.reduced=True`)\n    :param yi: `(Ni, ydim)` optional, function values at xi to use (e.g. if `self.reduced=True`)\n    :returns hess: `(..., ydim, xdim, xdim)`, the vector Hessian at points `x`\n    \"\"\"\n    shape_1d, x = self._fmt_input(x)\n    if yi is None:\n        yi = self.yi.copy()\n    if xi is None:\n        xi = self.xi.copy()\n    xdim = xi.shape[-1]\n    ydim = yi.shape[-1]\n    nan_idx = np.any(np.isnan(yi), axis=-1)\n    if np.any(nan_idx):\n        # Use a simple linear regression fit to impute missing values (may have resulted from bad model outputs)\n        imputer = Pipeline([('scaler', MaxAbsScaler()), ('model', Ridge(alpha=1))])\n        imputer.fit(xi[~nan_idx, :], yi[~nan_idx, :])\n        yi[nan_idx, :] = imputer.predict(xi[nan_idx, :])\n\n    # Create ragged edge matrix of interpolation pts and weights\n    grid_sizes = self.get_grid_sizes(self.beta)     # For example:\n    x_j = np.empty((xdim, max(grid_sizes)))         # A= [#####--\n    w_j = np.empty((xdim, max(grid_sizes)))               #######\n    x_j[:] = np.nan                                       ###----]\n    w_j[:] = np.nan\n    for n in range(xdim):\n        x_j[n, :grid_sizes[n]] = self.x_grids[n]\n        w_j[n, :grid_sizes[n]] = self.weights[n]\n\n    # Compute values ahead of time that will be needed for the gradient\n    diff = x[..., np.newaxis] - x_j\n    div_zero_idx = np.isclose(diff, 0, rtol=1e-4, atol=1e-8)\n    diff[div_zero_idx] = 1\n    quotient = w_j / diff                               # (..., xdim, Nx)\n    qsum = np.nansum(quotient, axis=-1)                 # (..., xdim)\n    qsum_p = -np.nansum(w_j / diff ** 2, axis=-1)       # (..., xdim)\n    qsum_pp = 2 * np.nansum(w_j / diff ** 3, axis=-1)   # (..., xdim)\n\n    # Loop over multi-indices and compute 2nd derivative of tensor-product lagrange polynomials\n    hess = np.zeros(x.shape[:-1] + (ydim, xdim, xdim), dtype=x.dtype)  # (..., ydim, xdim, xdim)\n    indices = [np.arange(grid_sizes[n]) for n in range(self.xdim())]\n    for m in range(xdim):\n        for n in range(m, xdim):\n            dims = [idx for idx in np.arange(xdim) if idx not in [m, n]]\n            for i, j in enumerate(itertools.product(*indices)):\n                j_dims = [j[idx] for idx in dims]\n                L_j = quotient[..., dims, j_dims] / qsum[..., dims]  # (..., xdim-2)\n                other_pts = np.copy(div_zero_idx)\n                other_pts[div_zero_idx[..., list(np.arange(xdim)), j]] = False\n\n                # Set L_j(x==x_j)=1 for the current j and set L_j(x==x_j)=0 for x_j = x_i, i != j\n                L_j[div_zero_idx[..., dims, j_dims]] = 1\n                L_j[np.any(other_pts[..., dims, :], axis=-1)] = 0\n\n                # Cross-terms in Hessian\n                if m != n:\n                    # Partial derivative of L_j with respect to x_m and x_n\n                    d2LJ_dx2 = np.ones(x.shape[:-1])\n                    for k in [m, n]:\n                        dLJ_dx = ((w_j[k, j[k]] / (qsum[..., k] * diff[..., k, j[k]])) *\n                                  (-qsum_p[..., k] / qsum[..., k] - 1 / diff[..., k, j[k]]))\n\n                        # Set derivatives when x is at the interpolation points (i.e. x==x_j)\n                        p_idx = [idx for idx in np.arange(grid_sizes[k]) if idx != j[k]]\n                        w_j_large = np.broadcast_to(w_j[k, :], x.shape[:-1] + w_j.shape[-1:]).copy()\n                        curr_j_idx = div_zero_idx[..., k, j[k]]\n                        other_j_idx = np.any(other_pts[..., k, :], axis=-1)\n                        dLJ_dx[curr_j_idx] = -np.nansum((w_j[k, p_idx] / w_j[k, j[k]]) /\n                                                        (x[curr_j_idx, k, np.newaxis] - x_j[k, p_idx]), axis=-1)\n                        dLJ_dx[other_j_idx] = ((w_j[k, j[k]] / w_j_large[other_pts[..., k, :]]) /\n                                               (x[other_j_idx, k] - x_j[k, j[k]]))\n\n                        d2LJ_dx2 *= dLJ_dx\n\n                    d2LJ_dx2 = np.expand_dims(d2LJ_dx2, axis=-1) * np.prod(L_j, axis=-1, keepdims=True)  # (..., 1)\n                    hess[..., m, n] += d2LJ_dx2 * yi[i, :]\n                    hess[..., n, m] += d2LJ_dx2 * yi[i, :]\n\n                # Diagonal terms in Hessian:\n                else:\n                    front_term = w_j[m, j[m]] / (qsum[..., m] * diff[..., m, j[m]])\n                    first_term = (-qsum_pp[..., m] / qsum[..., m]) + 2*(qsum_p[..., m] / qsum[..., m]) ** 2\n                    second_term = (2*(qsum_p[..., m] / (qsum[..., m] * diff[..., m, j[m]]))\n                                   + 2 / diff[..., m, j[m]] ** 2)\n                    d2LJ_dx2 = front_term * (first_term + second_term)\n\n                    # Set derivatives when x is at the interpolation points (i.e. x==x_j)\n                    curr_j_idx = div_zero_idx[..., m, j[m]]\n                    other_j_idx = np.any(other_pts[..., m, :], axis=-1)\n                    if np.any(curr_j_idx) or np.any(other_j_idx):\n                        p_idx = [idx for idx in np.arange(grid_sizes[m]) if idx != j[m]]\n                        w_j_large = np.broadcast_to(w_j[m, :], x.shape[:-1] + w_j.shape[-1:]).copy()\n                        x_j_large = np.broadcast_to(x_j[m, :], x.shape[:-1] + x_j.shape[-1:]).copy()\n\n                        # if these points are at the current j interpolation point\n                        d2LJ_dx2[curr_j_idx] = (2 * np.nansum((w_j[m, p_idx] / w_j[m, j[m]]) /\n                                                             (x[curr_j_idx, m, np.newaxis] - x_j[m, p_idx]), axis=-1) ** 2 +  # noqa: E501\n                                                2 * np.nansum((w_j[m, p_idx] / w_j[m, j[m]]) /\n                                                              (x[curr_j_idx, m, np.newaxis] - x_j[m, p_idx])**2, axis=-1))  # noqa: E501\n\n                        # if these points are at any other interpolation point\n                        other_pts_inv = other_pts.copy()\n                        other_pts_inv[other_j_idx, m, :grid_sizes[m]] = np.invert(other_pts[other_j_idx, m, :grid_sizes[m]])  # noqa: E501\n                        curr_x_j = x_j_large[other_pts[..., m, :]].reshape((-1, 1))\n                        other_x_j = x_j_large[other_pts_inv[..., m, :]].reshape((-1, len(p_idx)))\n                        curr_w_j = w_j_large[other_pts[..., m, :]].reshape((-1, 1))\n                        other_w_j = w_j_large[other_pts_inv[..., m, :]].reshape((-1, len(p_idx)))\n                        curr_div = w_j[m, j[m]] / np.squeeze(curr_w_j, axis=-1)\n                        curr_diff = np.squeeze(curr_x_j, axis=-1) - x_j[m, j[m]]\n                        d2LJ_dx2[other_j_idx] = ((-2*curr_div / curr_diff) * (np.nansum(\n                            (other_w_j / curr_w_j) / (curr_x_j - other_x_j), axis=-1) + 1 / curr_diff))\n\n                    d2LJ_dx2 = np.expand_dims(d2LJ_dx2, axis=-1) * np.prod(L_j, axis=-1, keepdims=True)  # (..., 1)\n                    hess[..., m, n] += d2LJ_dx2 * yi[i, :]\n\n    return np.atleast_1d(np.squeeze(hess)) if shape_1d else hess\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.LagrangeInterpolator.leja_1d","title":"<code>leja_1d(N, z_bds, z_pts=None, wt_fcn=None)</code>  <code>staticmethod</code>","text":"<p>Find the next <code>N</code> points in the Leja sequence of <code>z_pts</code>.</p> PARAMETER DESCRIPTION <code>N</code> <p>number of new points to add to the sequence</p> <p> TYPE: <code>int</code> </p> <code>z_bds</code> <p>bounds on the 1d domain</p> <p> TYPE: <code>tuple</code> </p> <code>z_pts</code> <p>current univariate Leja sequence <code>(Nz,)</code>, start at middle of <code>z_bds</code> if <code>None</code></p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> <code>wt_fcn</code> <p>weighting function, uses a constant weight if <code>None</code>, callable as <code>wt_fcn(z)</code></p> <p> TYPE: <code>callable</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>the Leja sequence <code>z_pts</code> augmented by <code>N</code> new points</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>@staticmethod\ndef leja_1d(N: int, z_bds: tuple, z_pts: np.ndarray = None, wt_fcn: callable = None) -&gt; np.ndarray:\n    \"\"\"Find the next `N` points in the Leja sequence of `z_pts`.\n\n    :param N: number of new points to add to the sequence\n    :param z_bds: bounds on the 1d domain\n    :param z_pts: current univariate Leja sequence `(Nz,)`, start at middle of `z_bds` if `None`\n    :param wt_fcn: weighting function, uses a constant weight if `None`, callable as `wt_fcn(z)`\n    :returns: the Leja sequence `z_pts` augmented by `N` new points\n    \"\"\"\n    # if wt_fcn is None:\n    wt_fcn = lambda z: 1  # UPDATE: ignore RV weighting, unbounded pdfs like Gaussian cause problems\n    if z_pts is None:\n        z_pts = (z_bds[1] + z_bds[0]) / 2\n        N = N - 1\n    z_pts = np.atleast_1d(z_pts).astype(np.float32)\n\n    # Construct Leja sequence by maximizing the Leja objective sequentially\n    for i in range(N):\n        obj_fun = lambda z: -wt_fcn(np.array(z).astype(np.float32)) * np.prod(np.abs(z - z_pts))\n        res = direct(obj_fun, [z_bds])  # Use global DIRECT optimization over 1d domain\n        z_star = res.x\n        z_pts = np.concatenate((z_pts, z_star))\n\n    return z_pts\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.LagrangeInterpolator.refine","title":"<code>refine(beta, auto=True, x_refine=None)</code>","text":"<p>Return a new interpolator with one dimension refined by one level, specified by <code>beta</code>.</p> <p>Note</p> <p>If <code>self.reduced=True</code> or <code>auto=False</code>, then this function will return tuple indices <code>idx</code> corresponding to the new interpolation points <code>x</code>. The tuple indices specify one index along each input dimension.</p> PARAMETER DESCRIPTION <code>beta</code> <p>the new refinement level indices</p> <p> </p> <code>auto</code> <p>whether to automatically compute model at refinement points</p> <p> DEFAULT: <code>True</code> </p> <code>x_refine</code> <p><code>(Nx,)</code> use this array as the refined 1d grid if provided, otherwise compute via <code>leja_1d</code></p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>interp</code> - a <code>LagrangeInterpolator</code> with a refined grid (default), otherwise if <code>auto=False</code>, returns <code>idx, x, interp</code>, where <code>idx</code> and <code>x</code> correspond to new interpolation points.</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def refine(self, beta, auto=True, x_refine: np.ndarray = None):\n    \"\"\"Return a new interpolator with one dimension refined by one level, specified by `beta`.\n\n    !!! Note\n        If `self.reduced=True` or `auto=False`, then this function will return tuple indices `idx` corresponding\n        to the new interpolation points `x`. The tuple indices specify one index along each input dimension.\n\n    :param beta: the new refinement level indices\n    :param auto: whether to automatically compute model at refinement points\n    :param x_refine: `(Nx,)` use this array as the refined 1d grid if provided, otherwise compute via `leja_1d`\n    :returns: `interp` - a `LagrangeInterpolator` with a refined grid (default), otherwise if `auto=False`,\n              returns `idx, x, interp`, where `idx` and `x` correspond to new interpolation points.\n    \"\"\"\n    try:\n        # Initialize a new interpolator with the new refinement levels\n        interp = LagrangeInterpolator(beta, self.x_vars, model=self._model, model_args=self._model_args,\n                                      model_kwargs=self._model_kwargs, init_grids=False, reduced=self.reduced)\n\n        # Find the dimension and number of new points to add\n        old_grid_sizes = self.get_grid_sizes(self.beta)\n        new_grid_sizes = interp.get_grid_sizes(beta)\n        dim_refine = 0\n        num_new_pts = 0\n        for idx, grid_size in enumerate(new_grid_sizes):\n            if grid_size != old_grid_sizes[idx]:\n                dim_refine = idx\n                num_new_pts = grid_size - old_grid_sizes[idx]\n                break\n\n        # Add points to leja grid in this dimension\n        interp.x_grids = copy.deepcopy(self.x_grids)\n        xi = copy.deepcopy(x_refine) if x_refine is not None else self.leja_1d(num_new_pts,\n                                                                               interp.x_vars[dim_refine].bounds(),\n                                                                               z_pts=interp.x_grids[dim_refine],\n                                                                               wt_fcn=interp.x_vars[dim_refine].pdf)\n        interp.x_grids[dim_refine] = xi.astype(np.float32)\n\n        # Update barycentric weights in this dimension\n        interp.weights = copy.deepcopy(self.weights)\n        Nx_old = old_grid_sizes[dim_refine]\n        Nx_new = new_grid_sizes[dim_refine]\n        old_wts = copy.deepcopy(self.weights[dim_refine])\n        new_wts = np.zeros(Nx_new, dtype=np.float32)\n        new_wts[:Nx_old] = old_wts\n        bds = interp.x_vars[dim_refine].bounds()\n        C = (bds[1] - bds[0]) / 4.0  # Interval capacity\n        xi = interp.x_grids[dim_refine]\n        for j in range(Nx_old, Nx_new):\n            new_wts[:j] *= (C / (xi[:j] - xi[j]))\n            new_wts[j] = np.prod(C / (xi[j] - xi[:j]))\n        interp.weights[dim_refine] = new_wts\n\n        # Copy yi over at existing interpolation points\n        x_new = np.zeros((0, interp.xdim()), dtype=np.float32)\n        x_new_idx = []\n        tol = 1e-12     # Tolerance for floating point comparison\n        j = 0           # Use this idx for iterating over existing yi\n        if not self.reduced:\n            interp.xi = np.zeros((np.prod(new_grid_sizes), self.xdim()), dtype=np.float32)\n            interp.yi = np.zeros((np.prod(new_grid_sizes), self.ydim()), dtype=np.float32)\n            if self.save_enabled():\n                interp.output_files = [None] * np.prod(new_grid_sizes)\n\n        old_indices = [list(range(old_grid_sizes[n])) for n in range(self.xdim())]\n        old_indices = list(itertools.product(*old_indices))\n        new_indices = [list(range(new_grid_sizes[n])) for n in range(self.xdim())]\n        new_indices = list(itertools.product(*new_indices))\n        for i in range(len(new_indices)):\n            # Get the new grid coordinate/index and physical x location/point\n            new_x_idx = new_indices[i]\n            new_x_pt = np.array([float(interp.x_grids[n][new_x_idx[n]]) for n in range(self.xdim())],\n                                dtype=np.float32)\n\n            if not self.reduced:\n                # Store the old xi/yi and return new x points\n                interp.xi[i, :] = new_x_pt\n                if j &lt; len(old_indices) and np.all(np.abs(np.array(old_indices[j]) -\n                                                          np.array(new_indices[i])) &lt; tol):\n                    # If we already have this interpolation point\n                    interp.yi[i, :] = self.yi[j, :]\n                    if self.save_enabled():\n                        interp.output_files[i] = self.output_files[j]\n                    j += 1\n                else:\n                    # Otherwise, save new interpolation point and its index\n                    x_new = np.concatenate((x_new, new_x_pt.reshape((1, self.xdim()))))\n                    x_new_idx.append(i)\n            else:\n                # Just find the new x indices and return those for the reduced case\n                if j &lt; len(old_indices) and np.all(np.abs(np.array(old_indices[j]) -\n                                                          np.array(new_indices[i])) &lt; tol):\n                    j += 1\n                else:\n                    x_new = np.concatenate((x_new, new_x_pt.reshape((1, self.xdim()))))\n                    x_new_idx.append(new_x_idx)     # Add a tuple() multi-index if not saving xi/yi\n\n        # Evaluate the model at new interpolation points\n        interp.model_cost = self.model_cost\n        if self._model is None:\n            self.logger.warning('No model available to evaluate new interpolation points, returning the points '\n                                'to you instead...')\n            return x_new_idx, x_new, interp\n        elif not auto or self.reduced:\n            return x_new_idx, x_new, interp\n        else:\n            interp.set_yi(x_new=(x_new_idx, x_new))\n            return interp\n    except Exception as e:\n        import traceback\n        tb_str = str(traceback.format_exception(e))\n        self.logger.error(tb_str)\n        raise Exception(f'Original exception in refine(): {tb_str}')\n</code></pre>"},{"location":"reference/rv/","title":"rv","text":""},{"location":"reference/rv/#amisc.rv","title":"<code>amisc.rv</code>","text":"<p>Provides small classes for random variables.</p> <p>Includes:</p> <ul> <li><code>BaseRV</code>: Abstract wrapper class of a random variable</li> <li><code>UniformRV</code>: a uniformly-distributed random variable</li> <li><code>NormalRV</code>: a normally-distributed random variable</li> <li><code>ScalarRV</code>: a stand-in class for a variable with no uncertainty or pdf</li> <li><code>LogUniformRV</code>: base 10 log-uniform</li> <li><code>LogNormalRV</code>: base 10 log-normal</li> </ul>"},{"location":"reference/rv/#amisc.rv.BaseRV","title":"<code>BaseRV(id='', *, tex='', description='Random variable', units='-', param_type='calibration', nominal=1, domain=(0, 1))</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Small wrapper class similar to <code>scipy.stats</code> random variables (RVs).</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>an identifier for the variable</p> <p> TYPE: <code>str</code> </p> <code>bds</code> <p>the explicit domain bounds of the variable (limits of where you expect to use it)</p> <p> TYPE: <code>tuple[float, float]</code> </p> <code>nominal</code> <p>a typical value for this variable (within <code>bds</code>)</p> <p> TYPE: <code>float</code> </p> <code>tex</code> <p>latex format for the random variable, i.e. r\"\\(x_i\\)\"</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>a lengthier description of the variable</p> <p> TYPE: <code>str</code> </p> <code>units</code> <p>assumed units for the variable (if applicable)</p> <p> TYPE: <code>str</code> </p> <code>param_type</code> <p>an additional descriptor for how this rv is used, e.g. calibration, operating, design, etc.</p> <p> TYPE: <code>str</code> </p> <p>Child classes must implement <code>sample</code> and <code>pdf</code> methods.</p> Source code in <code>src/amisc/rv.py</code> <pre><code>def __init__(self, id='', *, tex='', description='Random variable', units='-',\n             param_type='calibration', nominal=1, domain=(0, 1)):\n    \"\"\"Child classes must implement `sample` and `pdf` methods.\"\"\"\n    self.bds = tuple(domain)\n    self.nominal = nominal\n    self.id = id if id != '' else 'X_' + ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))\n    self.is_custom_id = id != ''  # Whether a custom id was assigned to this variable\n    self.tex = tex\n    self.description = description\n    self.units = units\n    self.param_type = param_type\n</code></pre>"},{"location":"reference/rv/#amisc.rv.BaseRV.bounds","title":"<code>bounds()</code>","text":"<p>Return a tuple of the defined domain of this RV.</p> Source code in <code>src/amisc/rv.py</code> <pre><code>def bounds(self):\n    \"\"\"Return a tuple of the defined domain of this RV.\"\"\"\n    return self.bds\n</code></pre>"},{"location":"reference/rv/#amisc.rv.BaseRV.pdf","title":"<code>pdf(x)</code>  <code>abstractmethod</code>","text":"<p>Compute the PDF of the RV at the given <code>x</code> locations.</p> PARAMETER DESCRIPTION <code>x</code> <p>locations to compute the PDF at</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>the PDF evaluations at <code>x</code></p> Source code in <code>src/amisc/rv.py</code> <pre><code>@abstractmethod\ndef pdf(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute the PDF of the RV at the given `x` locations.\n\n    :param x: locations to compute the PDF at\n    :returns f: the PDF evaluations at `x`\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/rv/#amisc.rv.BaseRV.sample","title":"<code>sample(shape, nominal=None)</code>  <code>abstractmethod</code>","text":"<p>Draw samples from the PDF.</p> PARAMETER DESCRIPTION <code>shape</code> <p>the shape of the returned samples</p> <p> TYPE: <code>tuple | int</code> </p> <code>nominal</code> <p>a nominal value to use if applicable (i.e. a center for relative Uniform or Normal)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>samples from the PDF of this random variable</p> Source code in <code>src/amisc/rv.py</code> <pre><code>@abstractmethod\ndef sample(self, shape: tuple | int, nominal: float = None) -&gt; np.ndarray:\n    \"\"\"Draw samples from the PDF.\n\n    :param shape: the shape of the returned samples\n    :param nominal: a nominal value to use if applicable (i.e. a center for relative Uniform or Normal)\n    :returns: samples from the PDF of this random variable\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/rv/#amisc.rv.BaseRV.sample_domain","title":"<code>sample_domain(shape)</code>","text":"<p>Return an array of the given <code>shape</code> for random samples over the domain of this RV.</p> PARAMETER DESCRIPTION <code>shape</code> <p>the shape of samples to return</p> <p> TYPE: <code>tuple | int</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>random samples over the domain of the random variable</p> Source code in <code>src/amisc/rv.py</code> <pre><code>def sample_domain(self, shape: tuple | int) -&gt; np.ndarray:\n    \"\"\"Return an array of the given `shape` for random samples over the domain of this RV.\n\n    :param shape: the shape of samples to return\n    :returns samples: random samples over the domain of the random variable\n    \"\"\"\n    if isinstance(shape, int):\n        shape = (shape, )\n    return np.random.rand(*shape) * (self.bds[1] - self.bds[0]) + self.bds[0]\n</code></pre>"},{"location":"reference/rv/#amisc.rv.BaseRV.to_tex","title":"<code>to_tex(units=False, symbol=True)</code>","text":"<p>Return a raw string that is well-formatted for plotting (with tex).</p> PARAMETER DESCRIPTION <code>units</code> <p>whether to include the units in the string</p> <p> DEFAULT: <code>False</code> </p> <code>symbol</code> <p>just latex symbol if true, otherwise the full description</p> <p> DEFAULT: <code>True</code> </p> Source code in <code>src/amisc/rv.py</code> <pre><code>def to_tex(self, units=False, symbol=True):\n    \"\"\"Return a raw string that is well-formatted for plotting (with tex).\n\n    :param units: whether to include the units in the string\n    :param symbol: just latex symbol if true, otherwise the full description\n    \"\"\"\n    s = self.tex if symbol else self.description\n    if s == '':\n        s = str(self)\n    return r'{} [{}]'.format(s, self.units) if units else r'{}'.format(s)\n</code></pre>"},{"location":"reference/rv/#amisc.rv.BaseRV.update_bounds","title":"<code>update_bounds(lb, ub)</code>","text":"<p>Update the defined domain of this RV to <code>(lb, ub)</code>.</p> Source code in <code>src/amisc/rv.py</code> <pre><code>def update_bounds(self, lb, ub):\n    \"\"\"Update the defined domain of this RV to `(lb, ub)`.\"\"\"\n    self.bds = (lb, ub)\n</code></pre>"},{"location":"reference/rv/#amisc.rv.LogNormalRV","title":"<code>LogNormalRV(mu, std, id='', **kwargs)</code>","text":"<p>               Bases: <code>BaseRV</code></p> <p>A base 10 log-normal distributed random variable.</p> ATTRIBUTE DESCRIPTION <code>mu</code> <p>the center of the log-normal distribution</p> <p> TYPE: <code>float</code> </p> <code>std</code> <p>the standard deviation of the log-normal distribution</p> <p> TYPE: <code>float</code> </p> <p>Construct the RV with the mean and std of the underlying distribution, i.e. \\(\\log_{10}(x) \\sim N(\\mu, \\sigma)\\).</p> PARAMETER DESCRIPTION <code>mu</code> <p>the center of the log-normal distribution</p> <p> TYPE: <code>float</code> </p> <code>std</code> <p>the standard deviation of the log-normal distribution</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/amisc/rv.py</code> <pre><code>def __init__(self, mu: float, std: float, id='', **kwargs):\n    \"\"\"Construct the RV with the mean and std of the underlying distribution,\n    i.e. $\\\\log_{10}(x) \\\\sim N(\\\\mu, \\\\sigma)$.\n\n    :param mu: the center of the log-normal distribution\n    :param std: the standard deviation of the log-normal distribution\n    \"\"\"\n    domain = kwargs.get('domain', None)\n    if domain is None:\n        domain = (10 ** (mu - 3*std), 10 ** (mu + 3*std))   # Use a default domain of +- 3std\n    kwargs['domain'] = domain\n    super().__init__(id, **kwargs)\n    self.std = std\n    self.mu = mu\n</code></pre>"},{"location":"reference/rv/#amisc.rv.LogNormalRV.recenter","title":"<code>recenter(mu, std=None)</code>","text":"<p>Move the center of the distribution to <code>mu</code> with standard deviation <code>std</code> (optional)</p> PARAMETER DESCRIPTION <code>mu</code> <p>the new center of the distribution</p> <p> TYPE: <code>float</code> </p> <code>std</code> <p>(optional) new standard deviation</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/rv.py</code> <pre><code>def recenter(self, mu: float, std: float = None):\n    \"\"\"Move the center of the distribution to `mu` with standard deviation `std` (optional)\n\n    :param mu: the new center of the distribution\n    :param std: (optional) new standard deviation\n    \"\"\"\n    self.mu = mu\n    if std is not None:\n        self.std = std\n</code></pre>"},{"location":"reference/rv/#amisc.rv.LogUniformRV","title":"<code>LogUniformRV(log10_a, log10_b, id='', **kwargs)</code>","text":"<p>               Bases: <code>BaseRV</code></p> <p>A base 10 log-uniform distributed random variable, only supports absolute bounds.</p> <p>Construct the log-uniform random variable.</p> PARAMETER DESCRIPTION <code>log10_a</code> <p>the lower bound in log10 space</p> <p> TYPE: <code>float</code> </p> <code>log10_b</code> <p>the upper bound in log10 space</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/amisc/rv.py</code> <pre><code>def __init__(self, log10_a: float, log10_b: float, id='', **kwargs):\n    \"\"\"Construct the log-uniform random variable.\n\n    :param log10_a: the lower bound in log10 space\n    :param log10_b: the upper bound in log10 space\n    \"\"\"\n    super().__init__(id, **kwargs)\n    self.bds = (10**log10_a, 10**log10_b)\n</code></pre>"},{"location":"reference/rv/#amisc.rv.NormalRV","title":"<code>NormalRV(mu, std, id='', **kwargs)</code>","text":"<p>               Bases: <code>BaseRV</code></p> <p>A normally-distributed random variable.</p> ATTRIBUTE DESCRIPTION <code>mu</code> <p>float, the mean of the normal distribution</p> <p> TYPE: <code>float</code> </p> <code>std</code> <p>float, the standard deviation of the normal distribution</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/amisc/rv.py</code> <pre><code>def __init__(self, mu, std, id='', **kwargs):\n    domain = kwargs.get('domain', None)\n    if domain is None:\n        domain = (mu - 2.5*std, mu + 2.5*std)   # Use a default domain of +- 2.5std\n    kwargs['domain'] = domain\n    super().__init__(id, **kwargs)\n    self.mu = mu\n    self.std = std\n\n    # Set default nominal value as the provided mean\n    if kwargs.get('nominal', None) is None:\n        self.nominal = mu\n</code></pre>"},{"location":"reference/rv/#amisc.rv.NormalRV.recenter","title":"<code>recenter(mu, std=None)</code>","text":"<p>Move the center of the distribution to <code>mu</code> with standard deviation <code>std</code> (optional)</p> PARAMETER DESCRIPTION <code>mu</code> <p>the new center of the distribution</p> <p> TYPE: <code>float</code> </p> <code>std</code> <p>(optional) new standard deviation</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/rv.py</code> <pre><code>def recenter(self, mu: float, std: float =None):\n    \"\"\"Move the center of the distribution to `mu` with standard deviation `std` (optional)\n\n    :param mu: the new center of the distribution\n    :param std: (optional) new standard deviation\n    \"\"\"\n    self.mu = mu\n    if std is not None:\n        self.std = std\n</code></pre>"},{"location":"reference/rv/#amisc.rv.ScalarRV","title":"<code>ScalarRV(id='', *, tex='', description='Random variable', units='-', param_type='calibration', nominal=1, domain=(0, 1))</code>","text":"<p>               Bases: <code>BaseRV</code></p> <p>A stand-in variable with no uncertainty/pdf, just scalars.</p> <p>Child classes must implement <code>sample</code> and <code>pdf</code> methods.</p> Source code in <code>src/amisc/rv.py</code> <pre><code>def __init__(self, id='', *, tex='', description='Random variable', units='-',\n             param_type='calibration', nominal=1, domain=(0, 1)):\n    \"\"\"Child classes must implement `sample` and `pdf` methods.\"\"\"\n    self.bds = tuple(domain)\n    self.nominal = nominal\n    self.id = id if id != '' else 'X_' + ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))\n    self.is_custom_id = id != ''  # Whether a custom id was assigned to this variable\n    self.tex = tex\n    self.description = description\n    self.units = units\n    self.param_type = param_type\n</code></pre>"},{"location":"reference/rv/#amisc.rv.UniformRV","title":"<code>UniformRV(arg1, arg2, id='', **kwargs)</code>","text":"<p>               Bases: <code>BaseRV</code></p> <p>A uniformly-distributed random variable.</p> <p>Can be uniformly distributed in one of three ways: between global bounds, relative within a percent, or relative within a set absolute tolerance.</p> ATTRIBUTE DESCRIPTION <code>type</code> <p>specifies the type of uniform distribution, either 'bds', 'pct', or 'tol' as described above</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>the absolute tolerance or percent uncertainty if type is 'tol' or 'pct'</p> <p> TYPE: <code>float</code> </p> <p>Construct a uniformly-distributed random variable.</p> PARAMETER DESCRIPTION <code>arg1</code> <p>lower bound if specifying U(lb, ub), otherwise a tol or pct if specifying U(+/- tol/pct)</p> <p> TYPE: <code>float</code> </p> <code>arg2</code> <p>upper bound if specifying U(lb, ub), otherwise a str of either 'tol' or 'pct'</p> <p> TYPE: <code>float | str</code> </p> Source code in <code>src/amisc/rv.py</code> <pre><code>def __init__(self, arg1: float, arg2: float | str, id='', **kwargs):\n    \"\"\"Construct a uniformly-distributed random variable.\n\n    :param arg1: lower bound if specifying U(lb, ub), otherwise a tol or pct if specifying U(+/- tol/pct)\n    :param arg2: upper bound if specifying U(lb, ub), otherwise a str of either 'tol' or 'pct'\n    \"\"\"\n    domain = kwargs.get('domain', None)\n    if isinstance(arg2, str):\n        self.value = arg1\n        self.type = arg2\n    else:\n        self.value = None\n        self.type = 'bds'\n    if self.type == 'bds':\n        domain = (arg1, arg2) if domain is None else tuple(domain)     # This means domain overrides (arg1, arg2)\n    else:\n        domain = (0, 1) if domain is None else tuple(domain)\n    kwargs['domain'] = domain\n    super().__init__(id, **kwargs)\n\n    # Set default nominal value as middle of the domain if not specified\n    if kwargs.get('nominal', None) is None:\n        self.nominal = (self.bds[1] + self.bds[0]) / 2\n</code></pre>"},{"location":"reference/rv/#amisc.rv.UniformRV.get_uniform_bounds","title":"<code>get_uniform_bounds(nominal=None)</code>","text":"<p>Return the correct set of bounds based on type of uniform distribution.</p> PARAMETER DESCRIPTION <code>nominal</code> <p>the center value for relative uniform distributions</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[float, float]</code> <p>the uniform bounds</p> Source code in <code>src/amisc/rv.py</code> <pre><code>def get_uniform_bounds(self, nominal: float = None) -&gt; tuple[float, float]:\n    \"\"\"Return the correct set of bounds based on type of uniform distribution.\n\n    :param nominal: the center value for relative uniform distributions\n    :returns: the uniform bounds\n    \"\"\"\n    match self.type:\n        case 'bds':\n            return self.bds\n        case 'pct':\n            if nominal is None:\n                return self.bds\n            return nominal * (1 - self.value), nominal * (1 + self.value)\n        case 'tol':\n            if nominal is None:\n                return self.bds\n            return nominal - self.value, nominal + self.value\n        case other:\n            raise NotImplementedError(f'self.type = {other} not known. Choose from [\"pct, \"tol\", \"bds\"]')\n</code></pre>"},{"location":"reference/rv/#amisc.rv.UniformRV.pdf","title":"<code>pdf(x, nominal=None)</code>","text":"<p>Compute the pdf for a uniform distribution.</p> PARAMETER DESCRIPTION <code>x</code> <p>locations to compute the pdf at</p> <p> TYPE: <code>ndarray</code> </p> <code>nominal</code> <p>center location for relative uniform rvs</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>the evaluated PDF at <code>x</code></p> Source code in <code>src/amisc/rv.py</code> <pre><code>def pdf(self, x: np.ndarray, nominal: float = None) -&gt; np.ndarray:\n    \"\"\"Compute the pdf for a uniform distribution.\n\n    :param x: locations to compute the pdf at\n    :param nominal: center location for relative uniform rvs\n    :returns: the evaluated PDF at `x`\n    \"\"\"\n    x = np.atleast_1d(x)\n    bds = self.get_uniform_bounds(nominal)\n    den = bds[1] - bds[0]\n    den = 1 if np.isclose(den, 0) else den\n    y = np.broadcast_to(1 / den, x.shape).copy()\n    y[np.where(x &gt; bds[1])] = 0\n    y[np.where(x &lt; bds[0])] = 0\n    return y\n</code></pre>"},{"location":"reference/system/","title":"system","text":""},{"location":"reference/system/#amisc.system","title":"<code>amisc.system</code>","text":"<p>The <code>SystemSurrogate</code> is a framework for multidisciplinary models. It manages multiple single discipline component models and the connections between them. It provides a top-level interface for constructing and evaluating surrogates.</p>"},{"location":"reference/system/#amisc.system--features","title":"Features","text":"<ul> <li>Manages multidisciplinary models in a graph data structure, supports feedforward and feedback connections</li> <li>Feedback connections are solved with a fixed-point iteration (FPI) nonlinear solver</li> <li>FPI uses Anderson acceleration and surrogate evaluations for speed-up</li> <li>Top-level interface for training and using surrogates of each component model</li> <li>Adaptive experimental design for choosing training data efficiently</li> <li>Convenient testing, plotting, and performance metrics provided to assess quality of surrogates</li> <li>Detailed logging and traceback information</li> <li>Supports parallel execution with OpenMP and MPI protocols</li> <li>Abstract and flexible interfacing with component models</li> </ul> <p>Model specification</p> <p>Models are callable Python wrapper functions of the form <code>ret = model(x, *args, **kwargs)</code>, where <code>x</code> is an <code>np.ndarray</code> of model inputs (and <code>*args, **kwargs</code> allow passing any other required configurations for your model). The return value is a Python dictionary of the form <code>ret = {'y': y, 'files': files, 'cost': cost, etc.}</code>. In the return dictionary, you specify the raw model output <code>y</code> as an <code>np.ndarray</code> at a minimum. Optionally, you can specify paths to output files and the average model cost (in seconds of cpu time), and anything else you want. Your <code>model()</code> function can do anything it wants in order to go from <code>x</code> \u2192 <code>y</code>. Python has the flexibility to call virtually any external codes, or to implement the function natively with <code>numpy</code>.</p> <p>Component specification</p> <p>A component adds some extra configuration around a callable <code>model</code>. These configurations are defined in a Python dictionary, which we give the custom type <code>ComponentSpec</code>. At a bare minimum, you must specify a callable <code>model</code> and its connections to other models within the multidisciplinary system. The limiting case is a single component model, for which the configuration is simply <code>component = ComponentSpec(model)</code>.</p>"},{"location":"reference/system/#amisc.system.ComponentSpec","title":"<code>ComponentSpec(model, name='', exo_in=None, coupling_in=None, coupling_out=None, truth_alpha=(), max_alpha=(), max_beta=(), surrogate='lagrange', model_args=(), model_kwargs=None, save_output=False)</code>","text":"<p>               Bases: <code>UserDict</code></p> <p>Provides a simple extension class of a Python dictionary, used to configure a component model.</p> <p>Specifying a list of random variables</p> <p>The three fields: <code>exo_in</code>, <code>coupling_in</code>, and <code>coupling_out</code> fully determine how a component fits within a multidisciplinary system. For each, you must specify a list of variables in the same order as the model uses them. The model will use all exogenous inputs first, and then all coupling inputs. You can use a variable's global integer index into the system <code>exo_vars</code> or <code>coupling_vars</code>, or you can use the <code>str</code> id of the variable or the variable itself. This is summarized in the <code>amisc.IndicesRV</code> custom type.</p> <p>Example</p> <p>Let's say you have a model: <pre><code>def my_model(x, *args, **kwargs):\n    print(x.shape)  # (3,), so a total of 3 inputs\n    G = 6.674e-11\n    m1 = x[0]           # System-level input\n    m2 = x[1]           # System-level input\n    r = x[2]            # Coupling input\n    F = G*m1*m2 / r**2\n    return {'y': F}\n</code></pre> Let's say this model is part of a larger system where <code>m1</code> and <code>m2</code> are specified by the system, and <code>r</code> comes from a different model that predicts the distance between two objects. You would set the configuration as: <pre><code>component = ComponentSpec(my_model, exo_in=['m1', 'm2'], coupling_in=['r'], coupling_out=['F'])\n</code></pre></p> <p>Construct the configuration for this component model.</p> <p>Warning</p> <p>Always specify the model at a global scope, i.e. don't use <code>lambda</code> or nested functions. When saving to file, only a symbolic reference to the function signature will be saved, which must be globally defined when loading back from that save file.</p> PARAMETER DESCRIPTION <code>model</code> <p>the component model, must be defined in a global scope (i.e. in a module or top-level of a script)</p> <p> TYPE: <code>callable</code> </p> <code>name</code> <p>the name used to identify this component model</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>exo_in</code> <p>specifies the global, system-level (i.e. exogenous/external) inputs to this model</p> <p> TYPE: <code>IndicesRV</code> DEFAULT: <code>None</code> </p> <code>coupling_in</code> <p>specifies the coupling inputs received from other models</p> <p> TYPE: <code>IndicesRV | dict[str:IndicesRV]</code> DEFAULT: <code>None</code> </p> <code>coupling_out</code> <p>specifies all outputs of this model (which may couple later to downstream models)</p> <p> TYPE: <code>IndicesRV</code> DEFAULT: <code>None</code> </p> <code>truth_alpha</code> <p>the model fidelity indices to treat as a \"ground truth\" reference</p> <p> TYPE: <code>tuple | int</code> DEFAULT: <code>()</code> </p> <code>max_alpha</code> <p>the maximum model fidelity indices to allow for refinement purposes</p> <p> TYPE: <code>tuple | int</code> DEFAULT: <code>()</code> </p> <code>max_beta</code> <p>the maximum surrogate fidelity indices to allow for refinement purposes</p> <p> TYPE: <code>tuple | int</code> DEFAULT: <code>()</code> </p> <code>surrogate</code> <p>one of ('lagrange, 'analytical'), or the <code>ComponentSurrogate</code> class to use directly</p> <p> TYPE: <code>str | ComponentSurrogate</code> DEFAULT: <code>'lagrange'</code> </p> <code>model_args</code> <p>optional arguments to pass to the component model</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>()</code> </p> <code>model_kwargs</code> <p>optional keyword arguments to pass to the component model</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>save_output</code> <p>whether this model will be saving outputs to file</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>def __init__(self, model: callable, name: str = '', exo_in: IndicesRV = None,\n             coupling_in: IndicesRV | dict[str: IndicesRV] = None, coupling_out: IndicesRV = None,\n             truth_alpha: tuple | int = (), max_alpha: tuple | int = (), max_beta: tuple | int = (),\n             surrogate: str | ComponentSurrogate = 'lagrange', model_args: tuple = (), model_kwargs: dict = None,\n             save_output: bool = False):\n    \"\"\"Construct the configuration for this component model.\n\n    !!! Warning\n        Always specify the model at a _global_ scope, i.e. don't use `lambda` or nested functions. When saving to\n        file, only a symbolic reference to the function signature will be saved, which must be globally defined\n        when loading back from that save file.\n\n    :param model: the component model, must be defined in a global scope (i.e. in a module or top-level of a script)\n    :param name: the name used to identify this component model\n    :param exo_in: specifies the global, system-level (i.e. exogenous/external) inputs to this model\n    :param coupling_in: specifies the coupling inputs received from other models\n    :param coupling_out: specifies all outputs of this model (which may couple later to downstream models)\n    :param truth_alpha: the model fidelity indices to treat as a \"ground truth\" reference\n    :param max_alpha: the maximum model fidelity indices to allow for refinement purposes\n    :param max_beta: the maximum surrogate fidelity indices to allow for refinement purposes\n    :param surrogate: one of ('lagrange, 'analytical'), or the `ComponentSurrogate` class to use directly\n    :param model_args: optional arguments to pass to the component model\n    :param model_kwargs: optional keyword arguments to pass to the component model\n    :param save_output: whether this model will be saving outputs to file\n    \"\"\"\n    d = locals()\n    d2 = {key: value for key, value in d.items() if key in ComponentSpec.Options}\n    super().__init__(d2)\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate","title":"<code>SystemSurrogate(components, exo_vars, coupling_vars, est_bds=0, save_dir=None, executor=None, stdout=True, init_surr=True, logger_name=None)</code>","text":"<p>Multidisciplinary (MD) surrogate framework top-level class.</p> <p>Accessing individual components</p> <p>The <code>ComponentSurrogate</code> objects that compose <code>SystemSurrogate</code> are internally stored in the <code>self.graph.nodes</code> data structure. You can access them with <code>get_component(comp_name)</code>.</p> ATTRIBUTE DESCRIPTION <code>exo_vars</code> <p>global list of exogenous/external inputs for the MD system</p> <p> TYPE: <code>list[BaseRV]</code> </p> <code>coupling_vars</code> <p>global list of coupling variables for the MD system (including all system-level outputs)</p> <p> TYPE: <code>list[BaseRV]</code> </p> <code>refine_level</code> <p>the total number of refinement steps that have been made</p> <p> TYPE: <code>int</code> </p> <code>build_metrics</code> <p>contains data that summarizes surrogate training progress</p> <p> TYPE: <code>dict</code> </p> <code>root_dir</code> <p>root directory where all surrogate build products are saved to file</p> <p> TYPE: <code>str</code> </p> <code>log_file</code> <p>log file where all logs are written to by default</p> <p> TYPE: <code>str</code> </p> <code>executor</code> <p>manages parallel execution for the system</p> <p> TYPE: <code>Executor</code> </p> <code>graph</code> <p>the internal graph data structure of the MD system</p> <p> TYPE: <code>nx.DiGraph</code> </p> <p>Construct the MD system surrogate.</p> <p>Warning</p> <p>Component models should always use coupling variables in the order they appear in the system-level <code>coupling_vars</code>.</p> PARAMETER DESCRIPTION <code>components</code> <p>list of components in the MD system (using the ComponentSpec class)</p> <p> TYPE: <code>list[ComponentSpec] | ComponentSpec</code> </p> <code>exo_vars</code> <p>list of system-level exogenous/external inputs</p> <p> TYPE: <code>list[BaseRV] | BaseRV</code> </p> <code>coupling_vars</code> <p>list of all coupling variables (including all system-level outputs)</p> <p> TYPE: <code>list[BaseRV] | BaseRV</code> </p> <code>est_bds</code> <p>number of samples to estimate coupling variable bounds, do nothing if 0</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>save_dir</code> <p>root directory for all build products (.log, .pkl, .json, etc.), won't save if None</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>executor</code> <p>an instance of a <code>concurrent.futures.Executor</code>, use to iterate new candidates in parallel</p> <p> TYPE: <code>Executor</code> DEFAULT: <code>None</code> </p> <code>stdout</code> <p>whether to log to console</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>init_surr</code> <p>whether to initialize the surrogate immediately when constructing</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>logger_name</code> <p>the name of the logger to use, if None then uses class name by default</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>def __init__(self, components: list[ComponentSpec] | ComponentSpec, exo_vars: list[BaseRV] | BaseRV,\n             coupling_vars: list[BaseRV] | BaseRV, est_bds: int = 0, save_dir: str | Path = None,\n             executor: Executor = None, stdout: bool = True, init_surr: bool = True, logger_name: str = None):\n    \"\"\"Construct the MD system surrogate.\n\n    !!! Warning\n        Component models should always use coupling variables in the order they appear in the system-level\n        `coupling_vars`.\n\n    :param components: list of components in the MD system (using the ComponentSpec class)\n    :param exo_vars: list of system-level exogenous/external inputs\n    :param coupling_vars: list of all coupling variables (including all system-level outputs)\n    :param est_bds: number of samples to estimate coupling variable bounds, do nothing if 0\n    :param save_dir: root directory for all build products (.log, .pkl, .json, etc.), won't save if None\n    :param executor: an instance of a `concurrent.futures.Executor`, use to iterate new candidates in parallel\n    :param stdout: whether to log to console\n    :param init_surr: whether to initialize the surrogate immediately when constructing\n    :param logger_name: the name of the logger to use, if None then uses class name by default\n    \"\"\"\n    # Setup root save directory\n    if save_dir is not None:\n        timestamp = datetime.datetime.now(tz=timezone.utc).isoformat().split('.')[0].replace(':', '.')\n        save_dir = Path(save_dir) / ('amisc_' + timestamp)\n        os.mkdir(save_dir)\n    self.root_dir = None\n    self.log_file = None\n    self.logger = None\n    self.executor = executor\n    self.graph = nx.DiGraph()\n    self.set_root_directory(save_dir, stdout=stdout, logger_name=logger_name)\n\n    # Store system info in a directed graph data structure\n    self.exo_vars = copy.deepcopy(exo_vars) if isinstance(exo_vars, list) else [exo_vars]\n    self.x_vars = self.exo_vars     # Create an alias to be consistent with components\n    self.coupling_vars = copy.deepcopy(coupling_vars) if isinstance(coupling_vars, list) else [coupling_vars]\n    self.refine_level = 0\n    self.build_metrics = dict()     # Save refinement error metrics during training\n\n    # Construct graph nodes\n    components = [components] if not isinstance(components, list) else components\n    for k, comp in enumerate(components):\n        if comp['name'] == '':\n            comp['name'] = f'Component {k}'\n    Nk = len(components)\n    nodes = {comp['name']: comp for comp in components}  # work-around since self.graph.nodes is not built yet\n    for k in range(Nk):\n        # Add the component as a str() node, with attributes specifying details of the surrogate\n        comp_dict = components[k]\n        indices, surr = self._build_component(comp_dict, nodes=nodes)\n        self.graph.add_node(comp_dict['name'], surrogate=surr, is_computed=False, **indices)\n\n    # Connect all neighbor nodes\n    for node, node_obj in self.graph.nodes.items():\n        for neighbor in node_obj['local_in']:\n            self.graph.add_edge(neighbor, node)\n\n    self.set_logger(logger_name, stdout=stdout)  # Need to update component loggers\n\n    # Estimate coupling variable bounds\n    if est_bds &gt; 0:\n        self._estimate_coupling_bds(est_bds)\n\n    # Init system with most coarse fidelity indices in each component\n    if init_surr:\n        self.init_system()\n    self._save_progress('sys_init.pkl')\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.fit","title":"<code>fit(qoi_ind=None, num_refine=100, max_iter=20, max_tol=0.001, max_runtime=1, save_interval=0, update_bounds=True, test_set=None, n_jobs=1)</code>","text":"<p>Train the system surrogate adaptively by iterative refinement until an end condition is met.</p> PARAMETER DESCRIPTION <code>qoi_ind</code> <p>list of system QoI variables to focus refinement on, use all QoI if not specified</p> <p> TYPE: <code>IndicesRV</code> DEFAULT: <code>None</code> </p> <code>num_refine</code> <p>number of samples of exogenous inputs to compute error indicators on</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>max_iter</code> <p>the maximum number of refinement steps to take</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>max_tol</code> <p>the max allowable value in relative L2 error to achieve</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>max_runtime</code> <p>the maximum wall clock time (hr) to run refinement for (will go until all models finish)</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>save_interval</code> <p>number of refinement steps between each progress save, none if 0</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>update_bounds</code> <p>whether to continuously update coupling variable bounds during refinement</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>test_set</code> <p><code>dict(xt=(Nt, x_dim), yt=(Nt, y_dim)</code> to show convergence of surrogate to the truth model</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>n_jobs</code> <p>number of cpu workers for computing error indicators (on master MPI task), 1=sequential</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>@_save_on_error\ndef fit(self, qoi_ind: IndicesRV = None, num_refine: int = 100, max_iter: int = 20, max_tol: float = 1e-3,\n        max_runtime: float = 1, save_interval: int = 0, update_bounds: bool = True, test_set: dict = None,\n        n_jobs: int = 1):\n    \"\"\"Train the system surrogate adaptively by iterative refinement until an end condition is met.\n\n    :param qoi_ind: list of system QoI variables to focus refinement on, use all QoI if not specified\n    :param num_refine: number of samples of exogenous inputs to compute error indicators on\n    :param max_iter: the maximum number of refinement steps to take\n    :param max_tol: the max allowable value in relative L2 error to achieve\n    :param max_runtime: the maximum wall clock time (hr) to run refinement for (will go until all models finish)\n    :param save_interval: number of refinement steps between each progress save, none if 0\n    :param update_bounds: whether to continuously update coupling variable bounds during refinement\n    :param test_set: `dict(xt=(Nt, x_dim), yt=(Nt, y_dim)` to show convergence of surrogate to the truth model\n    :param n_jobs: number of cpu workers for computing error indicators (on master MPI task), 1=sequential\n    \"\"\"\n    qoi_ind = self._get_qoi_ind(qoi_ind)\n    Nqoi = len(qoi_ind)\n    max_iter = self.refine_level + max_iter\n    curr_error = np.inf\n    t_start = time.time()\n    test_stats, xt, yt, t_fig, t_ax = None, None, None, None, None\n\n    # Record of (error indicator, component, alpha, beta, num_evals, total added cost (s)) for each iteration\n    train_record = self.build_metrics.get('train_record', [])\n    if test_set is not None:\n        xt, yt = test_set['xt'], test_set['yt']\n    xt, yt = self.build_metrics.get('xt', xt), self.build_metrics.get('yt', yt)  # Overrides test set param\n\n    # Track convergence progress on a test set and on the max error indicator\n    err_fig, err_ax = plt.subplots()\n    if xt is not None and yt is not None:\n        self.build_metrics['xt'] = xt\n        self.build_metrics['yt'] = yt\n        if self.build_metrics.get('test_stats') is not None:\n            test_stats = self.build_metrics.get('test_stats')\n        else:\n            # Get initial perf metrics, (2, Nqoi)\n            test_stats = np.expand_dims(self.get_test_metrics(xt, yt, qoi_ind=qoi_ind), axis=0)\n        t_fig, t_ax = plt.subplots(1, Nqoi) if Nqoi &gt; 1 else plt.subplots()\n\n    # Set up a parallel pool of workers, sequential if n_jobs=1\n    with Parallel(n_jobs=n_jobs, verbose=0) as ppool:\n        while True:\n            # Check all end conditions\n            if self.refine_level &gt;= max_iter:\n                self._print_title_str(f'Termination criteria reached: Max iteration {self.refine_level}/{max_iter}')\n                break\n            if curr_error == -np.inf:\n                self._print_title_str('Termination criteria reached: No candidates left to refine')\n                break\n            if curr_error &lt; max_tol:\n                self._print_title_str(f'Termination criteria reached: L2 error {curr_error} &lt; tol {max_tol}')\n                break\n            if ((time.time() - t_start)/3600.0) &gt;= max_runtime:\n                actual = datetime.timedelta(seconds=time.time()-t_start)\n                target = datetime.timedelta(seconds=max_runtime*3600)\n                self._print_title_str(f'Termination criteria reached: runtime {str(actual)} &gt; {str(target)}')\n                break\n\n            # Refine surrogate and save progress\n            refine_res = self.refine(qoi_ind=qoi_ind, num_refine=num_refine, update_bounds=update_bounds,\n                                     ppool=ppool)\n            curr_error = refine_res[0]\n            if save_interval &gt; 0 and self.refine_level % save_interval == 0:\n                self._save_progress(f'sys_iter_{self.refine_level}.pkl')\n\n            # Plot progress of error indicator\n            train_record.append(refine_res)\n            error_record = [res[0] for res in train_record]\n            self.build_metrics['train_record'] = train_record\n            err_ax.clear(); err_ax.grid(); err_ax.plot(error_record, '-k')\n            ax_default(err_ax, 'Iteration', r'Relative $L_2$ error indicator', legend=False)\n            err_ax.set_yscale('log')\n            if self.root_dir is not None:\n                err_fig.savefig(str(Path(self.root_dir) / 'error_indicator.png'), dpi=300, format='png')\n\n            # Plot progress on test set\n            if xt is not None and yt is not None:\n                stats = self.get_test_metrics(xt, yt, qoi_ind=qoi_ind)\n                test_stats = np.concatenate((test_stats, stats[np.newaxis, ...]), axis=0)\n                for i in range(Nqoi):\n                    ax = t_ax if Nqoi == 1 else t_ax[i]\n                    ax.clear(); ax.grid(); ax.set_yscale('log')\n                    ax.plot(test_stats[:, 1, i], '-k')\n                    ax.set_title(self.coupling_vars[qoi_ind[i]].to_tex(units=True))\n                    ax_default(ax, 'Iteration', r'Relative $L_2$ error', legend=False)\n                t_fig.set_size_inches(3.5*Nqoi, 3.5)\n                t_fig.tight_layout()\n                if self.root_dir is not None:\n                    t_fig.savefig(str(Path(self.root_dir) / 'test_set.png'), dpi=300, format='png')\n                self.build_metrics['test_stats'] = test_stats\n\n    self._save_progress('sys_final.pkl')\n    self.logger.info(f'Final system surrogate: \\n {self}')\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.get_allocation","title":"<code>get_allocation(idx=None)</code>","text":"<p>Get a breakdown of cost allocation up to a certain iteration number during training (starting at 1).</p> PARAMETER DESCRIPTION <code>idx</code> <p>the iteration number to get allocation results for (defaults to last refinement step)</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>cost_alloc, offline_alloc, cost_cum</code> - the cost alloc per node/fidelity and cumulative training cost</p> Source code in <code>src/amisc/system.py</code> <pre><code>def get_allocation(self, idx: int = None):\n    \"\"\"Get a breakdown of cost allocation up to a certain iteration number during training (starting at 1).\n\n    :param idx: the iteration number to get allocation results for (defaults to last refinement step)\n    :returns: `cost_alloc, offline_alloc, cost_cum` - the cost alloc per node/fidelity and cumulative training cost\n    \"\"\"\n    if idx is None:\n        idx = self.refine_level\n    if idx &gt; self.refine_level:\n        raise ValueError(f'Specified index: {idx} is greater than the max training level of {self.refine_level}')\n\n    cost_alloc = dict()     # Cost allocation per node and model fidelity\n    cost_cum = [0.0]        # Cumulative cost allocation during training\n\n    # Add initialization costs for each node\n    for node, node_obj in self.graph.nodes.items():\n        surr = node_obj['surrogate']\n        base_alpha = (0,) * len(surr.truth_alpha)\n        base_beta = (0,) * (len(surr.max_refine) - len(surr.truth_alpha))\n        base_cost = surr.get_cost(base_alpha, base_beta)\n        cost_alloc[node] = dict()\n        if base_cost &gt; 0:\n            cost_alloc[node][str(base_alpha)] = np.array([1, float(base_cost)])\n            cost_cum[0] += float(base_cost)\n\n    # Add cumulative training costs\n    for i in range(idx):\n        err_indicator, node, alpha, beta, num_evals, cost = self.build_metrics['train_record'][i]\n        if cost_alloc[node].get(str(alpha), None) is None:\n            cost_alloc[node][str(alpha)] = np.zeros(2)  # (num model evals, total cpu_time cost)\n        cost_alloc[node][str(alpha)] += [round(num_evals), float(cost)]\n        cost_cum.append(float(cost))\n\n    # Get summary of total offline costs spent building search candidates (i.e. training overhead)\n    offline_alloc = dict()\n    for node, node_obj in self.graph.nodes.items():\n        surr = node_obj['surrogate']\n        offline_alloc[node] = dict()\n        for alpha, beta in surr.candidate_set:\n            if offline_alloc[node].get(str(alpha), None) is None:\n                offline_alloc[node][str(alpha)] = np.zeros(2)   # (num model evals, total cpu_time cost)\n            added_cost = surr.get_cost(alpha, beta)\n            base_cost = surr.get_sub_surrogate(alpha, beta).model_cost\n            offline_alloc[node][str(alpha)] += [round(added_cost/base_cost), float(added_cost)]\n\n    return cost_alloc, offline_alloc, np.cumsum(cost_cum)\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.get_component","title":"<code>get_component(comp_name)</code>","text":"<p>Return the <code>ComponentSurrogate</code> object for this component.</p> PARAMETER DESCRIPTION <code>comp_name</code> <p>name of the component to return</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>ComponentSurrogate</code> <p>the <code>ComponentSurrogate</code> object</p> Source code in <code>src/amisc/system.py</code> <pre><code>def get_component(self, comp_name: str) -&gt; ComponentSurrogate:\n    \"\"\"Return the `ComponentSurrogate` object for this component.\n\n    :param comp_name: name of the component to return\n    :returns: the `ComponentSurrogate` object\n    \"\"\"\n    comp = self if comp_name == 'System' else self.graph.nodes[comp_name]['surrogate']\n    return comp\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.get_test_metrics","title":"<code>get_test_metrics(xt, yt, qoi_ind=None, training=True)</code>","text":"<p>Get relative L2 error metric over a test set.</p> PARAMETER DESCRIPTION <code>xt</code> <p><code>(Nt, x_dim)</code> random test set of inputs</p> <p> TYPE: <code>ndarray</code> </p> <code>yt</code> <p><code>(Nt, y_dim)</code> random test set outputs</p> <p> TYPE: <code>ndarray</code> </p> <code>qoi_ind</code> <p>list of indices of QoIs to get metrics for</p> <p> TYPE: <code>IndicesRV</code> DEFAULT: <code>None</code> </p> <code>training</code> <p>whether to evaluate the surrogate in training or evaluation mode</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>stats</code> - <code>(2, Nqoi)</code> array \u2192 <code>[num_candidates, rel_L2_error]</code> for each QoI</p> Source code in <code>src/amisc/system.py</code> <pre><code>def get_test_metrics(self, xt: np.ndarray, yt: np.ndarray, qoi_ind: IndicesRV = None,\n                     training: bool = True) -&gt; np.ndarray:\n    \"\"\"Get relative L2 error metric over a test set.\n\n    :param xt: `(Nt, x_dim)` random test set of inputs\n    :param yt: `(Nt, y_dim)` random test set outputs\n    :param qoi_ind: list of indices of QoIs to get metrics for\n    :param training: whether to evaluate the surrogate in training or evaluation mode\n    :returns: `stats` - `(2, Nqoi)` array &amp;rarr; `[num_candidates, rel_L2_error]` for each QoI\n    \"\"\"\n    qoi_ind = self._get_qoi_ind(qoi_ind)\n    ysurr = self(xt, training=training)\n    ysurr = ysurr[:, qoi_ind]\n    yt = yt[:, qoi_ind]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        rel_l2_err = np.sqrt(np.mean((yt - ysurr) ** 2, axis=0)) / np.sqrt(np.mean(yt ** 2, axis=0))\n        rel_l2_err = np.nan_to_num(rel_l2_err, posinf=np.nan, neginf=np.nan, nan=np.nan)\n    num_cands = 0\n    for node, node_obj in self.graph.nodes.items():\n        num_cands += len(node_obj['surrogate'].index_set) + len(node_obj['surrogate'].candidate_set)\n\n    # Get test stats for each QoI\n    stats = np.zeros((2, yt.shape[-1]))\n    self.logger.debug(f'{\"QoI idx\": &gt;10} {\"Iteration\": &gt;10} {\"len(I_k)\": &gt;10} {\"Relative L2\": &gt;15}')\n    for i in range(yt.shape[-1]):\n        stats[:, i] = np.array([num_cands, rel_l2_err[i]])\n        self.logger.debug(f'{i: 10d} {self.refine_level: 10d} {num_cands: 10d} {rel_l2_err[i]: 15.5f}')\n\n    return stats\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.init_system","title":"<code>init_system()</code>","text":"<p>Add the coarsest multi-index to each component surrogate.</p> Source code in <code>src/amisc/system.py</code> <pre><code>@_save_on_error\ndef init_system(self):\n    \"\"\"Add the coarsest multi-index to each component surrogate.\"\"\"\n    self._print_title_str('Initializing all component surrogates')\n    for node, node_obj in self.graph.nodes.items():\n        node_obj['surrogate'].init_coarse()\n        # for alpha, beta in list(node_obj['surrogate'].candidate_set):\n        #     # Add one refinement in each input dimension to initialize\n        #     node_obj['surrogate'].activate_index(alpha, beta)\n        self.logger.info(f\"Initialized component '{node}'.\")\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.insert_component","title":"<code>insert_component(component, exo_add=None, qoi_add=None)</code>","text":"<p>Insert a new component into the system.</p> PARAMETER DESCRIPTION <code>component</code> <p>specs of new component model</p> <p> TYPE: <code>ComponentSpec</code> </p> <code>exo_add</code> <p>variables to add to system exogenous inputs (will be appended to end of <code>exo_vars</code>)</p> <p> TYPE: <code>BaseRV | list[BaseRV]</code> DEFAULT: <code>None</code> </p> <code>qoi_add</code> <p>system output QoIs to add (will be appended to end of <code>coupling_vars</code>)</p> <p> TYPE: <code>BaseRV | list[BaseRV]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>def insert_component(self, component: ComponentSpec, exo_add: BaseRV | list[BaseRV] = None,\n                     qoi_add: BaseRV | list[BaseRV] = None):\n    \"\"\"Insert a new component into the system.\n\n    :param component: specs of new component model\n    :param exo_add: variables to add to system exogenous inputs (will be appended to end of `exo_vars`)\n    :param qoi_add: system output QoIs to add (will be appended to end of `coupling_vars`)\n    \"\"\"\n    if exo_add is not None:\n        exo_add = [exo_add] if not isinstance(exo_add, list) else exo_add\n        self.exo_vars.extend(exo_add)\n    if qoi_add is not None:\n        qoi_add = [qoi_add] if not isinstance(qoi_add, list) else qoi_add\n        self.coupling_vars.extend(qoi_add)\n\n    indices, surr = self._build_component(component)\n    surr.init_coarse()\n    self.graph.add_node(component['name'], surrogate=surr, is_computed=False, **indices)\n\n    # Add graph edges\n    neighbors = list(indices['local_in'].keys())\n    for neighbor in neighbors:\n        self.graph.add_edge(neighbor, component['name'])\n    self.logger.info(f\"Inserted component '{component['name']}'.\")\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.load_from_file","title":"<code>load_from_file(filename, root_dir=None, executor=None, stdout=True, logger_name=None)</code>  <code>staticmethod</code>","text":"<p>Load a <code>SystemSurrogate</code> object from file.</p> PARAMETER DESCRIPTION <code>filename</code> <p>the .pkl file to load</p> <p> TYPE: <code>str | Path</code> </p> <code>root_dir</code> <p>if provided, an <code>amisc_timestamp</code> directory will be created at <code>root_dir</code>. Ignored if the <code>.pkl</code> file already resides in an <code>amisc</code>-like directory. If none, then the surrogate object is only loaded into memory and is not given a file directory for any save artifacts.</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>executor</code> <p>a <code>concurrent.futures.Executor</code> object to set; clears it if None</p> <p> TYPE: <code>Executor</code> DEFAULT: <code>None</code> </p> <code>stdout</code> <p>whether to log to console</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>logger_name</code> <p>the name of the logger to use, if None then uses class name by default</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>the <code>SystemSurrogate</code> object</p> Source code in <code>src/amisc/system.py</code> <pre><code>@staticmethod\ndef load_from_file(filename: str | Path, root_dir: str | Path = None, executor: Executor = None,\n                   stdout: bool = True, logger_name: str = None):\n    \"\"\"Load a `SystemSurrogate` object from file.\n\n    :param filename: the .pkl file to load\n    :param root_dir: if provided, an `amisc_timestamp` directory will be created at `root_dir`. Ignored if the\n                     `.pkl` file already resides in an `amisc`-like directory. If none, then the surrogate object\n                     is only loaded into memory and is not given a file directory for any save artifacts.\n    :param executor: a `concurrent.futures.Executor` object to set; clears it if None\n    :param stdout: whether to log to console\n    :param logger_name: the name of the logger to use, if None then uses class name by default\n    :returns: the `SystemSurrogate` object\n    \"\"\"\n    with open(Path(filename), 'rb') as dill_file:\n        sys_surr = dill.load(dill_file)\n        sys_surr.set_executor(executor)\n        sys_surr.x_vars = sys_surr.exo_vars     # backwards compatible v0.2.0\n\n    copy_flag = False\n    if root_dir is None:\n        parts = Path(filename).resolve().parts\n        if len(parts) &gt; 2:\n            if parts[-3].startswith('amisc_'):\n                root_dir = Path(filename).parent.parent  # Assumes amisc_root/sys/filename.pkl default structure\n    else:\n        if not Path(root_dir).is_dir():\n            root_dir = '.'\n        timestamp = datetime.datetime.now(tz=timezone.utc).isoformat().split('.')[0].replace(':', '.')\n        root_dir = Path(root_dir) / ('amisc_' + timestamp)\n        os.mkdir(root_dir)\n        copy_flag = True\n\n    sys_surr.set_root_directory(root_dir, stdout=stdout, logger_name=logger_name)\n\n    if copy_flag:\n        shutil.copyfile(Path(filename), root_dir / 'sys' / Path(filename).name)\n\n    return sys_surr\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.plot_allocation","title":"<code>plot_allocation(cmap='Blues', text_bar_width=0.06, arrow_bar_width=0.02)</code>","text":"<p>Plot bar charts showing cost allocation during training.</p> <p>Beta feature</p> <p>This has pretty good default settings, but it might look terrible for your use. Mostly provided here as a template for making cost allocation bar charts. Please feel free to copy and edit in your own code.</p> PARAMETER DESCRIPTION <code>cmap</code> <p>the colormap string identifier for <code>plt</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'Blues'</code> </p> <code>text_bar_width</code> <p>the minimum total cost fraction above which a bar will print centered model fidelity text</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.06</code> </p> <code>arrow_bar_width</code> <p>the minimum total cost fraction above which a bar will try to print text with an arrow; below this amount, the bar is too skinny and won't print any text</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.02</code> </p> RETURNS DESCRIPTION <p><code>fig, ax</code>, Figure and Axes objects</p> Source code in <code>src/amisc/system.py</code> <pre><code>def plot_allocation(self, cmap: str = 'Blues', text_bar_width: float = 0.06, arrow_bar_width: float = 0.02):\n    \"\"\"Plot bar charts showing cost allocation during training.\n\n    !!! Warning \"Beta feature\"\n        This has pretty good default settings, but it might look terrible for your use. Mostly provided here as\n        a template for making cost allocation bar charts. Please feel free to copy and edit in your own code.\n\n    :param cmap: the colormap string identifier for `plt`\n    :param text_bar_width: the minimum total cost fraction above which a bar will print centered model fidelity text\n    :param arrow_bar_width: the minimum total cost fraction above which a bar will try to print text with an arrow;\n                            below this amount, the bar is too skinny and won't print any text\n    :returns: `fig, ax`, Figure and Axes objects\n    \"\"\"\n    # Get total cost (including offline overhead)\n    train_alloc, offline_alloc, cost_cum = self.get_allocation()\n    total_cost = cost_cum[-1]\n    for node, alpha_dict in offline_alloc.items():\n        for alpha, cost in alpha_dict.items():\n            total_cost += cost[1]\n\n    # Remove nodes with cost=0 from alloc dicts (i.e. analytical models)\n    remove_nodes = []\n    for node, alpha_dict in train_alloc.items():\n        if len(alpha_dict) == 0:\n            remove_nodes.append(node)\n    for node in remove_nodes:\n        del train_alloc[node]\n        del offline_alloc[node]\n\n    # Bar chart showing cost allocation breakdown for MF system at end\n    fig, axs = plt.subplots(1, 2, sharey='row')\n    width = 0.7\n    x = np.arange(len(train_alloc))\n    xlabels = list(train_alloc.keys())\n    cmap = plt.get_cmap(cmap)\n    for k in range(2):\n        ax = axs[k]\n        alloc = train_alloc if k == 0 else offline_alloc\n        ax.set_title('Online training' if k == 0 else 'Overhead')\n        for j, (node, alpha_dict) in enumerate(alloc.items()):\n            bottom = 0\n            c_intervals = np.linspace(0, 1, len(alpha_dict))\n            bars = [(alpha, cost, cost[1] / total_cost) for alpha, cost in alpha_dict.items()]\n            bars = sorted(bars, key=lambda ele: ele[2], reverse=True)\n            for i, (alpha, cost, frac) in enumerate(bars):\n                p = ax.bar(x[j], frac, width, color=cmap(c_intervals[i]), linewidth=1,\n                           edgecolor=[0, 0, 0], bottom=bottom)\n                bottom += frac\n                if frac &gt; text_bar_width:\n                    ax.bar_label(p, labels=[f'{alpha}, {round(cost[0])}'], label_type='center')\n                elif frac &gt; arrow_bar_width:\n                    xy = (x[j] + width / 2, bottom - frac / 2)  # Label smaller bars with a text off to the side\n                    ax.annotate(f'{alpha}, {round(cost[0])}', xy, xytext=(xy[0] + 0.2, xy[1]),\n                                arrowprops={'arrowstyle': '-&gt;', 'linewidth': 1})\n                else:\n                    pass  # Don't label really small bars\n        ax_default(ax, '', \"Fraction of total cost\" if k == 0 else '', legend=False)\n        ax.set_xticks(x, xlabels)\n        ax.set_xlim(left=-1, right=x[-1] + 1)\n    fig.set_size_inches(8, 4)\n    fig.tight_layout()\n\n    if self.root_dir is not None:\n        fig.savefig(Path(self.root_dir) / 'mf_allocation.png', dpi=300, format='png')\n\n    return fig, axs\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.plot_slice","title":"<code>plot_slice(slice_idx=None, qoi_idx=None, show_surr=True, show_model=None, model_dir=None, N=50, nominal=None, random_walk=False, from_file=None)</code>","text":"<p>Helper function to plot 1d slices of the surrogate and/or model(s) over the inputs.</p> PARAMETER DESCRIPTION <code>slice_idx</code> <p>list of exogenous input variables or indices to take 1d slices of</p> <p> TYPE: <code>IndicesRV</code> DEFAULT: <code>None</code> </p> <code>qoi_idx</code> <p>list of model output variables or indices to plot 1d slices of</p> <p> TYPE: <code>IndicesRV</code> DEFAULT: <code>None</code> </p> <code>show_surr</code> <p>whether to show the surrogate prediction</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_model</code> <p>also plot model predictions, list() of ['best', 'worst', tuple(alpha), etc.]</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>model_dir</code> <p>base directory to save model outputs (if specified)</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>N</code> <p>the number of points to take in the 1d slice</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> <code>nominal</code> <p><code>dict</code> of <code>str(var)-&gt;nominal</code> to use as constant value for all non-sliced variables</p> <p> TYPE: <code>dict[str:float]</code> DEFAULT: <code>None</code> </p> <code>random_walk</code> <p>whether to slice in a random d-dimensional direction or hold all params const while slicing</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>from_file</code> <p>path to a .pkl file to load a saved slice from disk</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>fig, ax</code> with <code>num_slice</code> by <code>num_qoi</code> subplots</p> Source code in <code>src/amisc/system.py</code> <pre><code>def plot_slice(self, slice_idx: IndicesRV = None, qoi_idx: IndicesRV = None, show_surr: bool = True,\n               show_model: list = None, model_dir: str | Path = None, N: int = 50, nominal: dict[str: float] = None,\n               random_walk: bool = False, from_file: str | Path = None):\n    \"\"\"Helper function to plot 1d slices of the surrogate and/or model(s) over the inputs.\n\n    :param slice_idx: list of exogenous input variables or indices to take 1d slices of\n    :param qoi_idx: list of model output variables or indices to plot 1d slices of\n    :param show_surr: whether to show the surrogate prediction\n    :param show_model: also plot model predictions, list() of ['best', 'worst', tuple(alpha), etc.]\n    :param model_dir: base directory to save model outputs (if specified)\n    :param N: the number of points to take in the 1d slice\n    :param nominal: `dict` of `str(var)-&gt;nominal` to use as constant value for all non-sliced variables\n    :param random_walk: whether to slice in a random d-dimensional direction or hold all params const while slicing\n    :param from_file: path to a .pkl file to load a saved slice from disk\n    :returns: `fig, ax` with `num_slice` by `num_qoi` subplots\n    \"\"\"\n    # Manage loading important quantities from file (if provided)\n    xs, ys_model, ys_surr = None, None, None\n    if from_file is not None:\n        with open(Path(from_file), 'rb') as fd:\n            slice_data = pickle.load(fd)\n            slice_idx = slice_data['slice_idx']     # Must use same input slices as save file\n            show_model = slice_data['show_model']   # Must use same model data as save file\n            qoi_idx = slice_data['qoi_idx'] if qoi_idx is None else qoi_idx\n            xs = slice_data['xs']\n            model_dir = None  # Don't run or save any models if loading from file\n\n    # Set default values (take up to the first 3 slices by default)\n    rand_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))\n    if model_dir is not None:\n        os.mkdir(Path(model_dir) / f'sweep_{rand_id}')\n    if nominal is None:\n        nominal = dict()\n    slice_idx = list(np.arange(0, min(3, len(self.exo_vars)))) if slice_idx is None else slice_idx\n    qoi_idx = list(np.arange(0, min(3, len(self.coupling_vars)))) if qoi_idx is None else qoi_idx\n    if isinstance(slice_idx[0], str | BaseRV):\n        slice_idx = [self.exo_vars.index(var) for var in slice_idx]\n    if isinstance(qoi_idx[0], str | BaseRV):\n        qoi_idx = [self.coupling_vars.index(var) for var in qoi_idx]\n\n    exo_bds = [var.bounds() for var in self.exo_vars]\n    xlabels = [self.exo_vars[idx].to_tex(units=True) for idx in slice_idx]\n    ylabels = [self.coupling_vars[idx].to_tex(units=True) for idx in qoi_idx]\n\n    # Construct slice model inputs (if not provided)\n    if xs is None:\n        xs = np.zeros((N, len(slice_idx), len(self.exo_vars)))\n        for i in range(len(slice_idx)):\n            if random_walk:\n                # Make a random straight-line walk across d-cube\n                r0 = np.squeeze(self.sample_inputs((1,), use_pdf=False), axis=0)\n                r0[slice_idx[i]] = exo_bds[slice_idx[i]][0]             # Start slice at this lower bound\n                rf = np.squeeze(self.sample_inputs((1,), use_pdf=False), axis=0)\n                rf[slice_idx[i]] = exo_bds[slice_idx[i]][1]             # Slice up to this upper bound\n                xs[0, i, :] = r0\n                for k in range(1, N):\n                    xs[k, i, :] = xs[k-1, i, :] + (rf-r0)/(N-1)\n            else:\n                # Otherwise, only slice one variable\n                for j in range(len(self.exo_vars)):\n                    if j == slice_idx[i]:\n                        xs[:, i, j] = np.linspace(exo_bds[slice_idx[i]][0], exo_bds[slice_idx[i]][1], N)\n                    else:\n                        xs[:, i, j] = nominal.get(self.exo_vars[j], self.exo_vars[j].nominal)\n\n    # Walk through each model that is requested by show_model\n    if show_model is not None:\n        if from_file is not None:\n            ys_model = slice_data['ys_model']\n        else:\n            ys_model = list()\n            for model in show_model:\n                output_dir = None\n                if model_dir is not None:\n                    output_dir = (Path(model_dir) / f'sweep_{rand_id}' /\n                                  str(model).replace('{', '').replace('}', '').replace(':', '=').replace(\"'\", ''))\n                    os.mkdir(output_dir)\n                ys_model.append(self(xs, use_model=model, model_dir=output_dir))\n    if show_surr:\n        ys_surr = self(xs) if from_file is None else slice_data['ys_surr']\n\n    # Make len(qoi) by len(inputs) grid of subplots\n    fig, axs = plt.subplots(len(qoi_idx), len(slice_idx), sharex='col', sharey='row')\n    for i in range(len(qoi_idx)):\n        for j in range(len(slice_idx)):\n            if len(qoi_idx) == 1:\n                ax = axs if len(slice_idx) == 1 else axs[j]\n            elif len(slice_idx) == 1:\n                ax = axs if len(qoi_idx) == 1 else axs[i]\n            else:\n                ax = axs[i, j]\n            x = xs[:, j, slice_idx[j]]\n            if show_model is not None:\n                c = np.array([[0, 0, 0, 1], [0.5, 0.5, 0.5, 1]]) if len(show_model) &lt;= 2 else (\n                    plt.get_cmap('jet')(np.linspace(0, 1, len(show_model))))\n                for k in range(len(show_model)):\n                    model_str = (str(show_model[k]).replace('{', '').replace('}', '')\n                                 .replace(':', '=').replace(\"'\", ''))\n                    model_ret = ys_model[k]\n                    y_model = model_ret[:, j, qoi_idx[i]]\n                    label = {'best': 'High-fidelity' if len(show_model) &gt; 1 else 'Model',\n                             'worst': 'Low-fidelity'}.get(model_str, model_str)\n                    ax.plot(x, y_model, ls='-', c=c[k, :], label=label)\n            if show_surr:\n                y_surr = ys_surr[:, j, qoi_idx[i]]\n                ax.plot(x, y_surr, '--r', label='Surrogate')\n            ylabel = ylabels[i] if j == 0 else ''\n            xlabel = xlabels[j] if i == len(qoi_idx) - 1 else ''\n            legend = (i == 0 and j == len(slice_idx) - 1)\n            ax_default(ax, xlabel, ylabel, legend=legend)\n    fig.set_size_inches(3 * len(slice_idx), 3 * len(qoi_idx))\n    fig.tight_layout()\n\n    # Save results (unless we were already loading from a save file)\n    if from_file is None and self.root_dir is not None:\n        fname = f's{\",\".join([str(i) for i in slice_idx])}_q{\",\".join([str(i) for i in qoi_idx])}'\n        fname = f'sweep_rand{rand_id}_' + fname if random_walk else f'sweep_nom{rand_id}_' + fname\n        fdir = Path(self.root_dir) if model_dir is None else Path(model_dir) / f'sweep_{rand_id}'\n        fig.savefig(fdir / f'{fname}.png', dpi=300, format='png')\n        save_dict = {'slice_idx': slice_idx, 'qoi_idx': qoi_idx, 'show_model': show_model, 'show_surr': show_surr,\n                     'nominal': nominal, 'random_walk': random_walk, 'xs': xs, 'ys_model': ys_model,\n                     'ys_surr': ys_surr}\n        with open(fdir / f'{fname}.pkl', 'wb') as fd:\n            pickle.dump(save_dict, fd)\n\n    return fig, axs\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.predict","title":"<code>predict(x, max_fpi_iter=100, anderson_mem=10, fpi_tol=1e-10, use_model=None, model_dir=None, verbose=False, training=False, index_set=None, qoi_ind=None, ppool=None)</code>","text":"<p>Evaluate the system surrogate at exogenous inputs <code>x</code>.</p> <p>Warning</p> <p>You can use this function to predict outputs for your MD system using the full-order models rather than the surrogate, by specifying <code>use_model</code>. This is convenient because <code>SystemSurrogate</code> manages all the coupled information flow between models automatically. However, it is highly recommended to not use the full model if your system contains feedback loops. The FPI nonlinear solver would be infeasible using anything more computationally demanding than the surrogate.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>(..., x_dim)</code> the points to get surrogate predictions for</p> <p> TYPE: <code>ndarray | float</code> </p> <code>max_fpi_iter</code> <p>the limit on convergence for the fixed-point iteration routine</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>anderson_mem</code> <p>hyperparameter for tuning the convergence of FPI with anderson acceleration</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>fpi_tol</code> <p>tolerance limit for convergence of fixed-point iteration</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-10</code> </p> <code>use_model</code> <p>'best'=highest-fidelity, 'worst'=lowest-fidelity, tuple=specific fidelity, None=surrogate, specify a <code>dict</code> of the above to assign different model fidelities for diff components</p> <p> TYPE: <code>str | tuple | dict</code> DEFAULT: <code>None</code> </p> <code>model_dir</code> <p>directory to save model outputs if <code>use_model</code> is specified</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>verbose</code> <p>whether to print out iteration progress during execution</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>training</code> <p>whether to call the system surrogate in training or evaluation mode, ignored if <code>use_model</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>index_set</code> <p><code>dict(node=[indices])</code> to override default index set for a node (only useful for parallel)</p> <p> TYPE: <code>dict[str:IndexSet]</code> DEFAULT: <code>None</code> </p> <code>qoi_ind</code> <p>list of qoi indices to return, defaults to returning all system <code>coupling_vars</code></p> <p> TYPE: <code>IndicesRV</code> DEFAULT: <code>None</code> </p> <code>ppool</code> <p>a joblib <code>Parallel</code> instance to pass to each component to loop over multi-indices in parallel</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., y_dim)</code> the surrogate approximation of the system QoIs</p> Source code in <code>src/amisc/system.py</code> <pre><code>def predict(self, x: np.ndarray | float, max_fpi_iter: int = 100, anderson_mem: int = 10, fpi_tol: float = 1e-10,\n            use_model: str | tuple | dict = None, model_dir: str | Path = None, verbose: bool = False,\n            training: bool = False, index_set: dict[str: IndexSet] = None, qoi_ind: IndicesRV = None,\n            ppool=None) -&gt; np.ndarray:\n    \"\"\"Evaluate the system surrogate at exogenous inputs `x`.\n\n    !!! Warning\n        You can use this function to predict outputs for your MD system using the full-order models rather than the\n        surrogate, by specifying `use_model`. This is convenient because `SystemSurrogate` manages all the\n        coupled information flow between models automatically. However, it is *highly* recommended to not use\n        the full model if your system contains feedback loops. The FPI nonlinear solver would be infeasible using\n        anything more computationally demanding than the surrogate.\n\n    :param x: `(..., x_dim)` the points to get surrogate predictions for\n    :param max_fpi_iter: the limit on convergence for the fixed-point iteration routine\n    :param anderson_mem: hyperparameter for tuning the convergence of FPI with anderson acceleration\n    :param fpi_tol: tolerance limit for convergence of fixed-point iteration\n    :param use_model: 'best'=highest-fidelity, 'worst'=lowest-fidelity, tuple=specific fidelity, None=surrogate,\n                       specify a `dict` of the above to assign different model fidelities for diff components\n    :param model_dir: directory to save model outputs if `use_model` is specified\n    :param verbose: whether to print out iteration progress during execution\n    :param training: whether to call the system surrogate in training or evaluation mode, ignored if `use_model`\n    :param index_set: `dict(node=[indices])` to override default index set for a node (only useful for parallel)\n    :param qoi_ind: list of qoi indices to return, defaults to returning all system `coupling_vars`\n    :param ppool: a joblib `Parallel` instance to pass to each component to loop over multi-indices in parallel\n    :returns y: `(..., y_dim)` the surrogate approximation of the system QoIs\n    \"\"\"\n    # Allocate space for all system outputs (just save all coupling vars)\n    x = np.atleast_1d(x)\n    ydim = len(self.coupling_vars)\n    y = np.zeros(x.shape[:-1] + (ydim,), dtype=x.dtype)\n    valid_idx = ~np.isnan(x[..., 0])  # Keep track of valid samples (set to False if FPI fails)\n    t1 = 0\n    output_dir = None\n    qoi_ind = self._get_qoi_ind(qoi_ind)\n    is_computed = np.full(ydim, False)\n\n    # Interpret which model fidelities to use for each component (if specified)\n    if use_model is not None:\n        if not isinstance(use_model, dict):\n            use_model = {node: use_model for node in self.graph.nodes}  # use same for each component\n    else:\n        use_model = {node: None for node in self.graph.nodes}\n    use_model = {node: use_model.get(node, None) for node in self.graph.nodes}\n\n    # Initialize all components\n    for node, node_obj in self.graph.nodes.items():\n        node_obj['is_computed'] = False\n\n    # Convert system into DAG by grouping strongly-connected-components\n    dag = nx.condensation(self.graph)\n\n    # Compute component models in topological order\n    for supernode in nx.topological_sort(dag):\n        if np.all(is_computed[qoi_ind]):\n            break  # Exit early if all qois of interest are computed\n\n        scc = [n for n in dag.nodes[supernode]['members']]\n\n        # Compute single component feedforward output (no FPI needed)\n        if len(scc) == 1:\n            if verbose:\n                self.logger.info(f\"Running component '{scc[0]}'...\")\n                t1 = time.time()\n\n            # Gather inputs\n            node_obj = self.graph.nodes[scc[0]]\n            exo_inputs = x[..., node_obj['exo_in']]\n            # for comp_name in node_obj['local_in']:\n            #     assert self.graph.nodes[comp_name]['is_computed']\n            coupling_inputs = y[..., node_obj['global_in']]\n            comp_input = np.concatenate((exo_inputs, coupling_inputs), axis=-1)  # (..., xdim)\n\n            # Compute outputs\n            indices = index_set.get(scc[0], None) if index_set is not None else None\n            if model_dir is not None:\n                output_dir = Path(model_dir) / scc[0]\n                os.mkdir(output_dir)\n            comp_output = node_obj['surrogate'](comp_input[valid_idx, :], use_model=use_model.get(scc[0]),\n                                                model_dir=output_dir, training=training, index_set=indices,\n                                                ppool=ppool)\n            for local_i, global_i in enumerate(node_obj['global_out']):\n                y[valid_idx, global_i] = comp_output[..., local_i]\n                is_computed[global_i] = True\n            node_obj['is_computed'] = True\n\n            if verbose:\n                self.logger.info(f\"Component '{scc[0]}' completed. Runtime: {time.time() - t1} s\")\n\n        # Handle FPI for SCCs with more than one component\n        else:\n            # Set the initial guess for all coupling vars (middle of domain)\n            coupling_bds = [rv.bounds() for rv in self.coupling_vars]\n            x_couple = np.array([(bds[0] + bds[1]) / 2 for bds in coupling_bds]).astype(x.dtype)\n            x_couple = np.broadcast_to(x_couple, x.shape[:-1] + x_couple.shape).copy()\n\n            adj_nodes = []\n            fpi_idx = set()\n            for node in scc:\n                for comp_name, local_idx in self.graph.nodes[node]['local_in'].items():\n                    # Track the global idx of all coupling vars that need FPI\n                    if comp_name in scc:\n                        fpi_idx.update([self.graph.nodes[comp_name]['global_out'][idx] for idx in local_idx])\n\n                    # Override coupling vars from components outside the scc (should already be computed)\n                    if comp_name not in scc and comp_name not in adj_nodes:\n                        # assert self.graph.nodes[comp_name]['is_computed']\n                        global_idx = self.graph.nodes[comp_name]['global_out']\n                        x_couple[..., global_idx] = y[..., global_idx]\n                        adj_nodes.append(comp_name)  # Only need to do this once for each adj component\n            x_couple_next = x_couple.copy()\n            fpi_idx = sorted(fpi_idx)\n\n            # Main FPI loop\n            if verbose:\n                self.logger.info(f\"Initializing FPI for SCC {scc} ...\")\n                t1 = time.time()\n            k = 0\n            residual_hist = None\n            x_hist = None\n            while True:\n                for node in scc:\n                    # Gather inputs from exogenous and coupling sources\n                    node_obj = self.graph.nodes[node]\n                    exo_inputs = x[..., node_obj['exo_in']]\n                    coupling_inputs = x_couple[..., node_obj['global_in']]\n                    comp_input = np.concatenate((exo_inputs, coupling_inputs), axis=-1)     # (..., xdim)\n\n                    # Compute component outputs (just don't do this FPI with the real models, please..)\n                    indices = index_set.get(node, None) if index_set is not None else None\n                    comp_output = node_obj['surrogate'](comp_input[valid_idx, :], use_model=use_model.get(node),\n                                                        model_dir=None, training=training, index_set=indices,\n                                                        ppool=ppool)\n                    global_idx = node_obj['global_out']\n                    for local_i, global_i in enumerate(global_idx):\n                        x_couple_next[valid_idx, global_i] = comp_output[..., local_i]\n                        # Can't splice valid_idx with global_idx for some reason, have to loop over global_idx here\n\n                # Compute residual and check end conditions\n                residual = np.expand_dims(x_couple_next[..., fpi_idx] - x_couple[..., fpi_idx], axis=-1)\n                max_error = np.max(np.abs(residual[valid_idx, :, :]))\n                if verbose:\n                    self.logger.info(f'FPI iter: {k}. Max residual: {max_error}. Time: {time.time() - t1} s')\n                if max_error &lt;= fpi_tol:\n                    if verbose:\n                        self.logger.info(f'FPI converged for SCC {scc} in {k} iterations with {max_error} &lt; tol '\n                                         f'{fpi_tol}. Final time: {time.time() - t1} s')\n                    break\n                if k &gt;= max_fpi_iter:\n                    self.logger.warning(f'FPI did not converge in {max_fpi_iter} iterations for SCC {scc}: '\n                                        f'{max_error} &gt; tol {fpi_tol}. Some samples will be returned as NaN.')\n                    converged_idx = np.max(np.abs(residual), axis=(-1, -2)) &lt;= fpi_tol\n                    for idx in fpi_idx:\n                        y[~converged_idx, idx] = np.nan\n                    valid_idx = np.logical_and(valid_idx, converged_idx)\n                    break\n\n                # Keep track of residual and x_couple histories\n                if k == 0:\n                    residual_hist = residual.copy()                                 # (..., xdim, 1)\n                    x_hist = np.expand_dims(x_couple_next[..., fpi_idx], axis=-1)   # (..., xdim, 1)\n                    x_couple[:] = x_couple_next[:]\n                    k += 1\n                    continue  # skip anderson accel on first iteration\n\n                # Iterate with anderson acceleration (only iterate on samples that are not yet converged)\n                converged_idx = np.max(np.abs(residual), axis=(-1, -2)) &lt;= fpi_tol\n                curr_idx = np.logical_and(valid_idx, ~converged_idx)\n                residual_hist = np.concatenate((residual_hist, residual), axis=-1)\n                x_hist = np.concatenate((x_hist, np.expand_dims(x_couple_next[..., fpi_idx], axis=-1)), axis=-1)\n                mk = min(anderson_mem, k)\n                Fk = residual_hist[curr_idx, :, k-mk:]                               # (..., xdim, mk+1)\n                C = np.ones(Fk.shape[:-2] + (1, mk + 1))\n                b = np.zeros(Fk.shape[:-2] + (len(fpi_idx), 1))\n                d = np.ones(Fk.shape[:-2] + (1, 1))\n                alpha = np.expand_dims(self._constrained_lls(Fk, b, C, d), axis=-3)   # (..., 1, mk+1, 1)\n                x_new = np.squeeze(x_hist[curr_idx, :, np.newaxis, -(mk+1):] @ alpha, axis=(-1, -2))\n                for local_i, global_i in enumerate(fpi_idx):\n                    x_couple[curr_idx, global_i] = x_new[..., local_i]\n                k += 1\n\n            # Save outputs of each component in SCC after convergence of FPI\n            for node in scc:\n                global_idx = self.graph.nodes[node]['global_out']\n                for global_i in global_idx:\n                    y[valid_idx, global_i] = x_couple_next[valid_idx, global_i]\n                    is_computed[global_i] = True\n                self.graph.nodes[node]['is_computed'] = True\n\n    # Return all component outputs (..., Nqoi); samples that didn't converge during FPI are left as np.nan\n    return y[..., qoi_ind]\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.refine","title":"<code>refine(qoi_ind=None, num_refine=100, update_bounds=True, ppool=None)</code>","text":"<p>Find and refine the component surrogate with the largest error on system-level QoI.</p> PARAMETER DESCRIPTION <code>qoi_ind</code> <p>indices of system QoI to focus surrogate refinement on, use all QoI if not specified</p> <p> TYPE: <code>IndicesRV</code> DEFAULT: <code>None</code> </p> <code>num_refine</code> <p>number of samples of exogenous inputs to compute error indicators on</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>update_bounds</code> <p>whether to continuously update coupling variable bounds</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ppool</code> <p>a <code>Parallel</code> instance from <code>joblib</code> to compute error indicators in parallel, None=sequential</p> <p> TYPE: <code>Parallel</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>a tuple of <code>(error_indicator, component, node_star, alpha_star, beta_star, N, cost)</code> indicating the chosen candidate index and incurred cost</p> Source code in <code>src/amisc/system.py</code> <pre><code>def refine(self, qoi_ind: IndicesRV = None, num_refine: int = 100, update_bounds: bool = True,\n           ppool: Parallel = None) -&gt; tuple:\n    \"\"\"Find and refine the component surrogate with the largest error on system-level QoI.\n\n    :param qoi_ind: indices of system QoI to focus surrogate refinement on, use all QoI if not specified\n    :param num_refine: number of samples of exogenous inputs to compute error indicators on\n    :param update_bounds: whether to continuously update coupling variable bounds\n    :param ppool: a `Parallel` instance from `joblib` to compute error indicators in parallel, None=sequential\n    :returns refine_res: a tuple of `(error_indicator, component, node_star, alpha_star, beta_star, N, cost)`\n                         indicating the chosen candidate index and incurred cost\n    \"\"\"\n    self._print_title_str(f'Refining system surrogate: iteration {self.refine_level + 1}')\n    set_loky_pickler('dill')    # Dill can serialize 'self' for parallel workers\n    temp_exc = self.executor    # It can't serialize an executor though, so must save this temporarily\n    self.set_executor(None)\n    qoi_ind = self._get_qoi_ind(qoi_ind)\n\n    # Compute entire integrated-surrogate on a random test set for global system QoI error estimation\n    x_exo = self.sample_inputs((num_refine,))\n    y_curr = self(x_exo, training=True)\n    y_min, y_max = None, None\n    if update_bounds:\n        y_min = np.min(y_curr, axis=0, keepdims=True)  # (1, ydim)\n        y_max = np.max(y_curr, axis=0, keepdims=True)  # (1, ydim)\n\n    # Find the candidate surrogate with the largest error indicator\n    error_max, error_indicator = -np.inf, -np.inf\n    node_star, alpha_star, beta_star, l2_star, cost_star = None, None, None, -np.inf, 0\n    for node, node_obj in self.graph.nodes.items():\n        self.logger.info(f\"Estimating error for component '{node}'...\")\n        candidates = node_obj['surrogate'].candidate_set.copy()\n\n        def compute_error(alpha, beta):\n            # Helper function for computing error indicators for a given candidate (alpha, beta)\n            index_set = node_obj['surrogate'].index_set.copy()\n            index_set.append((alpha, beta))\n            y_cand = self(x_exo, training=True, index_set={node: index_set})\n            ymin = np.min(y_cand, axis=0, keepdims=True)\n            ymax = np.max(y_cand, axis=0, keepdims=True)\n            error = y_cand[:, qoi_ind] - y_curr[:, qoi_ind]\n            rel_l2 = np.sqrt(np.nanmean(error ** 2, axis=0)) / np.sqrt(np.nanmean(y_cand[:, qoi_ind] ** 2, axis=0))\n            rel_l2 = np.nan_to_num(rel_l2, nan=np.nan, posinf=np.nan, neginf=np.nan)\n            delta_error = np.nanmax(rel_l2)  # Max relative L2 error over all system QoIs\n            delta_work = max(1, node_obj['surrogate'].get_cost(alpha, beta))  # Cpu time (s)\n\n            return ymin, ymax, delta_error, delta_work\n\n        if len(candidates) &gt; 0:\n            ret = ppool(delayed(compute_error)(alpha, beta) for alpha, beta in candidates) if ppool is not None \\\n                else [compute_error(alpha, beta) for alpha, beta in candidates]\n\n            for i, (ymin, ymax, d_error, d_work) in enumerate(ret):\n                if update_bounds:\n                    y_min = np.min(np.concatenate((y_min, ymin), axis=0), axis=0, keepdims=True)\n                    y_max = np.max(np.concatenate((y_max, ymax), axis=0), axis=0, keepdims=True)\n                alpha, beta = candidates[i]\n                error_indicator = d_error / d_work\n                self.logger.info(f\"Candidate multi-index: {(alpha, beta)}. L2 error: {d_error}. Error indicator: \"\n                                 f\"{error_indicator}.\")\n\n                if error_indicator &gt; error_max:\n                    error_max = error_indicator\n                    node_star, alpha_star, beta_star, l2_star, cost_star = node, alpha, beta, d_error, d_work\n        else:\n            self.logger.info(f\"Component '{node}' has no available candidates left!\")\n\n    # Update all coupling variable ranges\n    if update_bounds:\n        for i in range(y_curr.shape[-1]):\n            self._update_coupling_bds(i, (y_min[0, i], y_max[0, i]))\n\n    # Add the chosen multi-index to the chosen component\n    self.set_executor(temp_exc)\n    if node_star is not None:\n        self.logger.info(f\"Candidate multi-index {(alpha_star, beta_star)} chosen for component '{node_star}'\")\n        self.graph.nodes[node_star]['surrogate'].activate_index(alpha_star, beta_star)\n        self.refine_level += 1\n        num_evals = round(cost_star / self[node_star].get_sub_surrogate(alpha_star, beta_star).model_cost)\n    else:\n        self.logger.info(f\"No candidates left for refinement, iteration: {self.refine_level}\")\n        num_evals = 0\n\n    return l2_star, node_star, alpha_star, beta_star, num_evals, cost_star\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.sample_inputs","title":"<code>sample_inputs(size, comp='System', use_pdf=False, nominal=None, constants=None)</code>","text":"<p>Return samples of the inputs according to provided options.</p> PARAMETER DESCRIPTION <code>size</code> <p>tuple or integer specifying shape or number of samples to obtain</p> <p> TYPE: <code>tuple | int</code> </p> <code>comp</code> <p>which component to sample inputs for (defaults to full system exogenous inputs)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'System'</code> </p> <code>use_pdf</code> <p>whether to sample from each variable's pdf, defaults to random samples over input domain instead</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>nominal</code> <p><code>dict(var_id=value)</code> of nominal values for params with relative uncertainty, also can use to specify constant values for a variable listed in <code>constants</code></p> <p> TYPE: <code>dict[str:float]</code> DEFAULT: <code>None</code> </p> <code>constants</code> <p>set of param types to hold constant while sampling (i.e. calibration, design, etc.), can also put a <code>var_id</code> string in here to specify a single variable to hold constant</p> <p> TYPE: <code>set[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(*size, x_dim)</code> samples of the inputs for the given component/system</p> Source code in <code>src/amisc/system.py</code> <pre><code>def sample_inputs(self, size: tuple | int, comp: str = 'System', use_pdf: bool = False,\n                  nominal: dict[str: float] = None, constants: set[str] = None) -&gt; np.ndarray:\n    \"\"\"Return samples of the inputs according to provided options.\n\n    :param size: tuple or integer specifying shape or number of samples to obtain\n    :param comp: which component to sample inputs for (defaults to full system exogenous inputs)\n    :param use_pdf: whether to sample from each variable's pdf, defaults to random samples over input domain instead\n    :param nominal: `dict(var_id=value)` of nominal values for params with relative uncertainty, also can use\n                    to specify constant values for a variable listed in `constants`\n    :param constants: set of param types to hold constant while sampling (i.e. calibration, design, etc.),\n                      can also put a `var_id` string in here to specify a single variable to hold constant\n    :returns x: `(*size, x_dim)` samples of the inputs for the given component/system\n    \"\"\"\n    size = (size, ) if isinstance(size, int) else size\n    if nominal is None:\n        nominal = dict()\n    if constants is None:\n        constants = set()\n    x_vars = self.exo_vars if comp == 'System' else self[comp].x_vars\n    x = np.empty((*size, len(x_vars)))\n    for i, var in enumerate(x_vars):\n        # Set a constant value for this variable\n        if var.param_type in constants or var in constants:\n            x[..., i] = nominal.get(var, var.nominal)  # Defaults to variable's nominal value if not specified\n\n        # Sample from this variable's pdf or randomly within its domain bounds (reject if outside bounds)\n        else:\n            lb, ub = var.bounds()\n            x_sample = var.sample(size, nominal=nominal.get(var, None)) if use_pdf \\\n                else var.sample_domain(size)\n            good_idx = (x_sample &lt; ub) &amp; (x_sample &gt; lb)\n            num_reject = np.sum(~good_idx)\n\n            while num_reject &gt; 0:\n                new_sample = var.sample((num_reject,), nominal=nominal.get(var, None)) if use_pdf \\\n                    else var.sample_domain((num_reject,))\n                x_sample[~good_idx] = new_sample\n                good_idx = (x_sample &lt; ub) &amp; (x_sample &gt; lb)\n                num_reject = np.sum(~good_idx)\n\n            x[..., i] = x_sample\n\n    return x\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.save_to_file","title":"<code>save_to_file(filename, save_dir=None)</code>","text":"<p>Save the <code>SystemSurrogate</code> object to a <code>.pkl</code> file.</p> PARAMETER DESCRIPTION <code>filename</code> <p>filename of the <code>.pkl</code> file to save to</p> <p> TYPE: <code>str</code> </p> <code>save_dir</code> <p>overrides existing surrogate root directory if provided, otherwise defaults to '.'</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>def save_to_file(self, filename: str, save_dir: str | Path = None):\n    \"\"\"Save the `SystemSurrogate` object to a `.pkl` file.\n\n    :param filename: filename of the `.pkl` file to save to\n    :param save_dir: overrides existing surrogate root directory if provided, otherwise defaults to '.'\n    \"\"\"\n    if save_dir is None:\n        save_dir = '.' if self.root_dir is None else str(Path(self.root_dir) / 'sys')\n    if not Path(save_dir).is_dir():\n        save_dir = '.'\n\n    exec_temp = self.executor   # Temporarily save executor obj (can't pickle it)\n    self.set_executor(None)\n    with open(Path(save_dir) / filename, 'wb') as dill_file:\n        dill.dump(self, dill_file)\n    self.set_executor(exec_temp)\n    self.logger.info(f'SystemSurrogate saved to {(Path(save_dir) / filename).resolve()}')\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.set_executor","title":"<code>set_executor(executor)</code>","text":"<p>Set a new <code>concurrent.futures.Executor</code> object for parallel calls.</p> PARAMETER DESCRIPTION <code>executor</code> <p>the new <code>Executor</code> object</p> <p> TYPE: <code>Executor | None</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>def set_executor(self, executor: Executor | None):\n    \"\"\"Set a new `concurrent.futures.Executor` object for parallel calls.\n\n    :param executor: the new `Executor` object\n    \"\"\"\n    self.executor = executor\n    for node, node_obj in self.graph.nodes.items():\n        node_obj['surrogate'].executor = executor\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.set_logger","title":"<code>set_logger(name=None, log_file=None, stdout=True)</code>","text":"<p>Set a new <code>logging.Logger</code> object with the given unique <code>name</code>.</p> PARAMETER DESCRIPTION <code>name</code> <p>the name of the new logger object</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>stdout</code> <p>whether to connect the logger to console (default)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>log_file</code> <p>log file (if provided)</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>def set_logger(self, name: str = None, log_file: str | Path = None, stdout: bool = True):\n    \"\"\"Set a new `logging.Logger` object with the given unique `name`.\n\n    :param name: the name of the new logger object\n    :param stdout: whether to connect the logger to console (default)\n    :param log_file: log file (if provided)\n    \"\"\"\n    if log_file is None:\n        log_file = self.log_file\n    if name is None:\n        name = self.__class__.__name__\n    self.log_file = log_file\n    self.logger = get_logger(name, log_file=log_file, stdout=stdout)\n\n    for node, node_obj in self.graph.nodes.items():\n        surr = node_obj['surrogate']\n        surr.logger = self.logger.getChild('Component')\n        surr.log_file = self.log_file\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.set_root_directory","title":"<code>set_root_directory(root_dir=None, stdout=True, logger_name=None)</code>","text":"<p>Set the root to a new directory, for example if you move to a new filesystem.</p> PARAMETER DESCRIPTION <code>root_dir</code> <p>new root directory, don't save build products if None</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>stdout</code> <p>whether to connect the logger to console (default)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>logger_name</code> <p>the logger name to use, defaults to class name</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>def set_root_directory(self, root_dir: str | Path = None, stdout: bool = True, logger_name: str = None):\n    \"\"\"Set the root to a new directory, for example if you move to a new filesystem.\n\n    :param root_dir: new root directory, don't save build products if None\n    :param stdout: whether to connect the logger to console (default)\n    :param logger_name: the logger name to use, defaults to class name\n    \"\"\"\n    if root_dir is None:\n        self.root_dir = None\n        self.log_file = None\n    else:\n        self.root_dir = str(Path(root_dir).resolve())\n        log_file = None\n        if not (Path(self.root_dir) / 'sys').is_dir():\n            os.mkdir(Path(self.root_dir) / 'sys')\n        if not (Path(self.root_dir) / 'components').is_dir():\n            os.mkdir(Path(self.root_dir) / 'components')\n        for f in os.listdir(self.root_dir):\n            if f.endswith('.log'):\n                log_file = str((Path(self.root_dir) / f).resolve())\n                break\n        if log_file is None:\n            fname = (datetime.datetime.now(tz=timezone.utc).isoformat().split('.')[0].replace(':', '.') +\n                     'UTC_sys.log')\n            log_file = str((Path(self.root_dir) / fname).resolve())\n        self.log_file = log_file\n\n    self.set_logger(logger_name, stdout=stdout)\n\n    # Update model output directories\n    for node, node_obj in self.graph.nodes.items():\n        surr = node_obj['surrogate']\n        if self.root_dir is not None and surr.save_enabled():\n            output_dir = str((Path(self.root_dir) / 'components' / node).resolve())\n            if not Path(output_dir).is_dir():\n                os.mkdir(output_dir)\n            surr._set_output_dir(output_dir)\n</code></pre>"},{"location":"reference/system/#amisc.system.SystemSurrogate.swap_component","title":"<code>swap_component(component, exo_add=None, exo_remove=None, qoi_add=None, qoi_remove=None)</code>","text":"<p>Swap a new component into the system, updating all connections/inputs.</p> <p>Beta feature, proceed with caution</p> <p>If you are swapping a new component in, you cannot remove any inputs that are expected by other components, including the coupling variables output by the current model.</p> PARAMETER DESCRIPTION <code>component</code> <p>specs of new component model (must replace an existing component with matching <code>name</code>)</p> <p> TYPE: <code>ComponentSpec</code> </p> <code>exo_add</code> <p>variables to add to system exogenous inputs (will be appended to end)</p> <p> TYPE: <code>BaseRV | list[BaseRV]</code> DEFAULT: <code>None</code> </p> <code>exo_remove</code> <p>indices of system exogenous inputs to delete (can't be shared by other components)</p> <p> TYPE: <code>IndicesRV</code> DEFAULT: <code>None</code> </p> <code>qoi_add</code> <p>system output QoIs to add (will be appended to end of <code>coupling_vars</code>)</p> <p> TYPE: <code>BaseRV | list[BaseRV]</code> DEFAULT: <code>None</code> </p> <code>qoi_remove</code> <p>indices of system <code>coupling_vars</code> to delete (can't be shared by other components)</p> <p> TYPE: <code>IndicesRV</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>def swap_component(self, component: ComponentSpec, exo_add: BaseRV | list[BaseRV] = None,\n                   exo_remove: IndicesRV = None, qoi_add: BaseRV | list[BaseRV] = None,\n                   qoi_remove: IndicesRV = None):\n    \"\"\"Swap a new component into the system, updating all connections/inputs.\n\n    !!! Warning \"Beta feature, proceed with caution\"\n        If you are swapping a new component in, you cannot remove any inputs that are expected by other components,\n        including the coupling variables output by the current model.\n\n    :param component: specs of new component model (must replace an existing component with matching `name`)\n    :param exo_add: variables to add to system exogenous inputs (will be appended to end)\n    :param exo_remove: indices of system exogenous inputs to delete (can't be shared by other components)\n    :param qoi_add: system output QoIs to add (will be appended to end of `coupling_vars`)\n    :param qoi_remove: indices of system `coupling_vars` to delete (can't be shared by other components)\n    \"\"\"\n    # Delete system exogenous inputs\n    if exo_remove is None:\n        exo_remove = []\n    exo_remove = [exo_remove] if not isinstance(exo_remove, list) else exo_remove\n    exo_remove = [self.exo_vars.index(var) for var in exo_remove] if exo_remove and isinstance(\n        exo_remove[0], str | BaseRV) else exo_remove\n\n    exo_remove = sorted(exo_remove)\n    for j, exo_var_idx in enumerate(exo_remove):\n        # Adjust exogenous indices for all components to account for deleted system inputs\n        for node, node_obj in self.graph.nodes.items():\n            if node != component['name']:\n                for i, idx in enumerate(node_obj['exo_in']):\n                    if idx == exo_var_idx:\n                        error_msg = f\"Can't delete system exogenous input at idx {exo_var_idx}, since it is \" \\\n                                    f\"shared by component '{node}'.\"\n                        self.logger.error(error_msg)\n                        raise ValueError(error_msg)\n                    if idx &gt; exo_var_idx:\n                        node_obj['exo_in'][i] -= 1\n\n        # Need to update the remaining delete indices by -1 to account for each sequential deletion\n        del self.exo_vars[exo_var_idx]\n        for i in range(j+1, len(exo_remove)):\n            exo_remove[i] -= 1\n\n    # Append any new exogenous inputs to the end\n    if exo_add is not None:\n        exo_add = [exo_add] if not isinstance(exo_add, list) else exo_add\n        self.exo_vars.extend(exo_add)\n\n    # Delete system qoi outputs (if not shared by other components)\n    qoi_remove = sorted(self._get_qoi_ind(qoi_remove))\n    for j, qoi_idx in enumerate(qoi_remove):\n        # Adjust coupling indices for all components to account for deleted system outputs\n        for node, node_obj in self.graph.nodes.items():\n            if node != component['name']:\n                for i, idx in enumerate(node_obj['global_in']):\n                    if idx == qoi_idx:\n                        error_msg = f\"Can't delete system QoI at idx {qoi_idx}, since it is an input to \" \\\n                                    f\"component '{node}'.\"\n                        self.logger.error(error_msg)\n                        raise ValueError(error_msg)\n                    if idx &gt; qoi_idx:\n                        node_obj['global_in'][i] -= 1\n\n                for i, idx in enumerate(node_obj['global_out']):\n                    if idx &gt; qoi_idx:\n                        node_obj['global_out'][i] -= 1\n\n        # Need to update the remaining delete indices by -1 to account for each sequential deletion\n        del self.coupling_vars[qoi_idx]\n        for i in range(j+1, len(qoi_remove)):\n            qoi_remove[i] -= 1\n\n    # Append any new system QoI outputs to the end\n    if qoi_add is not None:\n        qoi_add = [qoi_add] if not isinstance(qoi_add, list) else qoi_add\n        self.coupling_vars.extend(qoi_add)\n\n    # Build and initialize the new component surrogate\n    indices, surr = self._build_component(component)\n    surr.init_coarse()\n\n    # Make changes to adj matrix if coupling inputs changed\n    prev_neighbors = list(self.graph.nodes[component['name']]['local_in'].keys())\n    new_neighbors = list(indices['local_in'].keys())\n    for neighbor in new_neighbors:\n        if neighbor not in prev_neighbors:\n            self.graph.add_edge(neighbor, component['name'])\n        else:\n            prev_neighbors.remove(neighbor)\n    for neighbor in prev_neighbors:\n        self.graph.remove_edge(neighbor, component['name'])\n\n    self.logger.info(f\"Swapped component '{component['name']}'.\")\n    nx.set_node_attributes(self.graph, {component['name']: {'exo_in': indices['exo_in'], 'local_in':\n                                                            indices['local_in'], 'global_in': indices['global_in'],\n                                                            'global_out': indices['global_out'],\n                                                            'surrogate': surr, 'is_computed': False}})\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/#amisc.utils","title":"<code>amisc.utils</code>","text":"<p>Provides some basic utilities for the package.</p> <p>Includes:</p> <ul> <li><code>load_variables</code>: convenience function for loading RVs from a .json config file</li> <li><code>get_logger</code>: logging utility with nice formatting</li> </ul>"},{"location":"reference/utils/#amisc.utils.get_logger","title":"<code>get_logger(name, stdout=True, log_file=None)</code>","text":"<p>Return a file/stdout logger with the given name.</p> PARAMETER DESCRIPTION <code>name</code> <p>the name of the logger to return</p> <p> TYPE: <code>str</code> </p> <code>stdout</code> <p>whether to add a stdout handler to the logger</p> <p> DEFAULT: <code>True</code> </p> <code>log_file</code> <p>add file logging to this file (optional)</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Logger</code> <p>the logger</p> Source code in <code>src/amisc/utils.py</code> <pre><code>def get_logger(name: str, stdout=True, log_file: str | Path = None) -&gt; logging.Logger:\n    \"\"\"Return a file/stdout logger with the given name.\n\n    :param name: the name of the logger to return\n    :param stdout: whether to add a stdout handler to the logger\n    :param log_file: add file logging to this file (optional)\n    :returns: the logger\n    \"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    logger.handlers.clear()\n    if stdout:\n        std_handler = logging.StreamHandler(sys.stdout)\n        std_handler.setFormatter(LOG_FORMATTER)\n        logger.addHandler(std_handler)\n    if log_file is not None:\n        f_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')\n        f_handler.setLevel(logging.DEBUG)\n        f_handler.setFormatter(LOG_FORMATTER)\n        logger.addHandler(f_handler)\n\n    return logger\n</code></pre>"},{"location":"reference/utils/#amisc.utils.load_variables","title":"<code>load_variables(variables, file)</code>","text":"<p>Load a list of BaseRV objects from a variables json <code>file</code>.</p> PARAMETER DESCRIPTION <code>variables</code> <p>a list of str ids for variables to find in <code>file</code></p> <p> TYPE: <code>list[str]</code> </p> <code>file</code> <p>json file to search for variable definitions</p> <p> TYPE: <code>Path | str</code> </p> RETURNS DESCRIPTION <code>list[BaseRV]</code> <p>a list of corresponding <code>BaseRV</code> objects</p> Source code in <code>src/amisc/utils.py</code> <pre><code>def load_variables(variables: list[str], file: Path | str) -&gt; list[BaseRV]:\n    \"\"\"Load a list of BaseRV objects from a variables json `file`.\n\n    :param variables: a list of str ids for variables to find in `file`\n    :param file: json file to search for variable definitions\n    :returns rvs: a list of corresponding `BaseRV` objects\n    \"\"\"\n    with open(Path(file), 'r') as fd:\n        data = json.load(fd)\n\n    rvs = []\n    keys = ['id', 'tex', 'description', 'units', 'param_type', 'nominal', 'domain']\n    for str_id in variables:\n        if str_id in data:\n            var_info = data.get(str_id)\n            kwargs = {key: var_info.get(key) for key in keys if var_info.get(key)}\n            match var_info.get('rv_type', 'none'):\n                case 'uniform_bds':\n                    bds = var_info.get('rv_params')\n                    rvs.append(UniformRV(bds[0], bds[1], **kwargs))\n                case 'uniform_pct':\n                    rvs.append(UniformRV(var_info.get('rv_params'), 'pct', **kwargs))\n                case 'uniform_tol':\n                    rvs.append(UniformRV(var_info.get('rv_params'), 'tol', **kwargs))\n                case 'normal':\n                    mu, std = var_info.get('rv_params')\n                    rvs.append(NormalRV(mu, std, **kwargs))\n                case 'none':\n                    # Make a plain stand-in scalar RV object (no uncertainty)\n                    rvs.append(ScalarRV(**kwargs))\n                case other:\n                    raise NotImplementedError(f'RV type \"{other}\" is not known.')\n        else:\n            raise ValueError(f'You have requested the variable {str_id}, but it was not found in {file}. '\n                             f'Please add a definition of {str_id} to {file} or construct it on your own.')\n\n    return rvs\n</code></pre>"}]}