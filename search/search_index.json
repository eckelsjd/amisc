{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v073-2025-02-10","title":"v0.7.3 (2025-02-10)","text":""},{"location":"changelog/#fix","title":"Fix","text":"<ul> <li>fixes #45 typo in call_model</li> <li>handle None/nan samples during compression</li> </ul>"},{"location":"changelog/#v072-2025-02-03","title":"v0.7.2 (2025-02-03)","text":""},{"location":"changelog/#fix_1","title":"Fix","text":"<ul> <li>fixes indexing error in system predict when handling exceptions</li> </ul>"},{"location":"changelog/#v071-2025-01-23","title":"v0.7.1 (2025-01-23)","text":""},{"location":"changelog/#fix_2","title":"Fix","text":"<ul> <li>all component outputs returned by system predict</li> </ul>"},{"location":"changelog/#v070-2025-01-16","title":"v0.7.0 (2025-01-16)","text":""},{"location":"changelog/#feat","title":"Feat","text":"<ul> <li>adds training data caching and performance improvements</li> </ul>"},{"location":"changelog/#fix_3","title":"Fix","text":"<ul> <li>makes test the default index set for test set performance</li> <li>bug with error indicator model cost</li> </ul>"},{"location":"changelog/#v060-2025-01-09","title":"v0.6.0 (2025-01-09)","text":""},{"location":"changelog/#feat_1","title":"Feat","text":"<ul> <li>clean sample inputs and plot slice api</li> <li>overhead tracking and sorted fidelity evaluation</li> <li>adds flag for custom weighting functions during training</li> <li>adds reconstruction tolerance for svd</li> </ul>"},{"location":"changelog/#fix_4","title":"Fix","text":"<ul> <li>bug with component serializer dict validation</li> <li>more consistent tracking of model costs and allocation</li> <li>rank starts at 1 for svd reconstruction tolerance</li> <li>serialize system model_extra if builtin</li> <li>allow skipping nan in relative error</li> <li>fixes bug in load_from_file and allows nd object arrays for compression</li> <li>cushion turbojet test tolerance</li> <li>integrate field coords and object arrays for changing field shapes</li> </ul>"},{"location":"changelog/#refactor","title":"Refactor","text":"<ul> <li>update PEP735 pyproject dependency groups</li> <li>rename sweep plots to slice</li> </ul>"},{"location":"changelog/#v052-2024-11-05","title":"v0.5.2 (2024-11-05)","text":""},{"location":"changelog/#fix_5","title":"Fix","text":"<ul> <li>fixes interactive function inspection and cleans dependencies</li> </ul>"},{"location":"changelog/#v051-2024-11-04","title":"v0.5.1 (2024-11-04)","text":""},{"location":"changelog/#fix_6","title":"Fix","text":"<ul> <li>init versioning and gh release</li> </ul>"},{"location":"changelog/#v050-2024-11-04","title":"v0.5.0 (2024-11-04)","text":""},{"location":"changelog/#breaking-change","title":"BREAKING CHANGE","text":"<ul> <li>mostly all APIs affected, resolves #23, #20, #19, #17, #16, #12, #11, #10, and JANUS-Institute/HallThrusterPEM#6</li> <li>Remove BaseRV interface and rename rv.py to variable.py</li> </ul>"},{"location":"changelog/#feat_2","title":"Feat","text":"<ul> <li>completes overhaul to v0.5.0</li> <li>adds component pydantic validation, serialization, and model wrapper</li> <li>add pydantic validation to Variable, add Transform and Distribution abstractions</li> <li>replace BaseRV with Variable and normalization and compression</li> </ul>"},{"location":"changelog/#v040-2024-08-29","title":"v0.4.0 (2024-08-29)","text":""},{"location":"changelog/#feat_3","title":"Feat","text":"<ul> <li>migrate to copier-numpy template</li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing-to-amisc","title":"Contributing to amisc","text":"<p>You might be here if you want to:</p> <ul> <li>Report a bug</li> <li>Discuss the current state of the code</li> <li>Submit a fix</li> <li>Propose a new feature</li> <li>Write unit tests</li> <li>Add to the documentation.</li> </ul> <p>We use Github to host code and documentation, to track issues and feature requests, and to accept pull requests.</p>"},{"location":"contributing/#submitting-pull-requests","title":"Submitting pull requests","text":"<p>Pull requests are the best way to propose changes to the codebase (bug fixes, new features, docs, etc.)</p> <ol> <li>Fork the repo and create a branch from <code>main</code>.</li> <li>If you are adding a feature or making major changes, first create the issue in Github.</li> <li>If you've added code that should be tested, add to <code>/tests</code>.</li> <li>If you've made major changes, update the <code>/docs</code>.</li> <li>Ensure the test suite passes (<code>pdm run test</code>).</li> <li>Follow Conventional commits guidelines when adding a commit message.</li> <li>Ensure all <code>pre-commit</code> checks pass. Pro tip: use <code>pdm lint</code> to help.</li> <li>Issue the pull request!</li> </ol> <p>Use pdm to set up your development environment. An example contribution workflow is shown here:</p> <pre><code># Fork the repo on Github\ngit clone https://github.com/&lt;your-user-name&gt;/amisc.git\ncd amisc\npdm install\ngit checkout -b &lt;your-branch-name&gt;\n\n# Make local changes\n\npdm run test  # make sure tests pass\ngit add -A\ngit commit -m \"fix: adding a bugfix\"\ngit push -u origin &lt;your-branch-name&gt;\n\n# Go to Github and \"Compare &amp; Pull Request\" on your fork\n# For your PR to be merged:\n  # squash all your commits on your branch (interactively in an IDE most likely)\n  # rebase to the top of origin/main to include new changes from others\n\ngit fetch\ngit rebase -i main your-branch  # for example\n\n# Resolve any conflicts\n# Your history now looks something like this:\n#              o your-branch\n#             /\n# ---o---o---o main\n\n# You can delete the branch and fork when your PR has been merged!\n</code></pre> <p>You can also find a good tutorial here.</p>"},{"location":"contributing/#report-bugs-using-issues","title":"Report bugs using issues","text":"<p>Open a new issue and describe your problem using the template. Provide screenshots where possible and example log files. Add labels to help categorize and describe your issue.</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the GPL-3.0 license.</p>"},{"location":"coverage/","title":"Coverage report","text":""},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#two-component-system","title":"Two component system","text":"<p>Here is a simple example of a two-component multidisciplinary system. <pre><code>import numpy as np\n\nfrom amisc import System\n\ndef fun1(x):\n    y = x * np.sin(np.pi * x)\n    return y\n\ndef fun2(y):\n    z = 1 / (1 + 25 * y ** 2)\n    return z\n\nsystem = System(fun1, fun2)\nsystem.inputs()['x'].update_domain((0, 1))\nsystem.outputs()['y'].update_domain((0, 1))\n\nsystem.fit()\n\nx_test = system.sample_inputs(10)\ny_test = system.predict(x_test)\n</code></pre> The first component computes \\(y=x\\sin(\\pi x)\\). The second component takes the output of the first and computes \\(z=1 / (1 + 25y^2)\\). Note that we must specify the domains of the variables over which we wish to build the surrogate (here they are just set to (0, 1)).</p>"},{"location":"examples/#random-variable","title":"Random variable","text":"<p>Here is an example of interpolating a function of a random variable \\(x\\sim U(-1, 1)\\). We manually construct the <code>Variable</code> and assign it a PDF. Then, we define the <code>Component</code> model and its inputs and outputs. Finally, we construct the MD system (which has just one component) and train the surrogate.  <pre><code>from amisc import Component, System, Variable\n\ndef fun(inputs):\n    return {'y': inputs['x'] ** 2}\n\nx = Variable(distribution='U(-1, 1)')\ny = Variable()\ncomponent = Component(fun, x, y, data_fidelity=(2,))\nsystem = System(component)\n\nsystem.fit()\nsystem.predict({'x': 0.5})  # {y: 0.25}\n</code></pre> Note, the <code>data_fidelity</code> parameter sets the maximum refinement level for the surrogate.</p>"},{"location":"examples/#fire-detection-satellite","title":"Fire detection satellite","text":"<p>Here is an example of a three-component fire detection satellite system from Chauduri (2018):  We define the system in a yaml configuration file and load the <code>System</code> object using a helper function (see the source code for details). Since the fire-sat system has complicated couplings between models, we generate a test set and estimate the coupling variable bounds while training the surrogate. Finally, we plot some diagnostics to determine the performance of the surrogate. <pre><code>from amisc.examples.models import fire_sat_system\n\nsystem = fire_sat_system()\n\nxtest = system.sample_inputs(100, use_pdf=True)     # --&gt; (100, xdim)\nytest = system.predict(xtest, use_model='best')     # --&gt; (100, ydim)\n\nsystem.fit(test_set=(xtest, ytest), estimate_bounds=True)\n\nprint(f'Inputs: {system.inputs()}')\nprint(f'Outputs: {system.outputs()}')\n\n# Plots\ninput_vars = ['H', 'Po']\noutput_vars = ['Vsat', 'Asa']\nsystem.plot_allocation()\nsystem.plot_slice(input_vars, output_vars, show_model=['best', 'worst'], random_walk=True)\n</code></pre> Here is the output of <code>plot_slice()</code>:  We see that the model has a smooth response over the inputs, and our surrogate is able to accurately approximate the high-fidelity model.</p>"},{"location":"examples/#field-quantity","title":"Field quantity","text":"<p>A field quantity is specified as a <code>Variable</code> (just like a scalar), except it is given a <code>Compression</code> map that tells <code>amisc</code> how to reduce the field quantity to a set of \"latent\" coefficients over which to build the surrogate approximation. This would be desirable if you want to approximate the output of a high-dimensional solution, for example, from a PDE mesh.</p> <p>In this example, we generate a random data matrix to simulate a high-dimensional field quantity, and construct a rank-4 SVD compression map. Note, we must provide the coordinates (Cartesian or otherwise) on which the field quantity is defined. We then approximate the field quantity as a function of the <code>x</code> scalar input. <pre><code>import numpy as np\n\nfrom amisc import Component, System, Variable, to_model_dataset\nfrom amisc.compression import SVD\n\ndef my_model(inputs):\n    \"\"\"Compute a field quantity as a function of `x`.\"\"\"\n    field = np.sin(inputs['x'] * np.linspace(0, 1, 100))\n    return {'f': field}\n\ndof = 100         # Number of degrees of freedom (i.e. size of the field qty)\nnum_samples = 50  # Number of samples\ndata_matrix = np.random.rand(dof, num_samples)\nfield_coords = np.linspace(0, 1, dof)\n\ncompression = SVD(rank=4, coords=field_coords, data_matrix=data_matrix)\n\nscalar = Variable('x', domain=(0, 1))\nfield_qty = Variable('f', compression=compression)\n\nmodel = Component(my_model, inputs=scalar, outputs=field_qty, data_fidelity=(2,))\nsystem = System(model)\n\nsystem.fit(max_iter=10)\n\nxtest = system.sample_inputs(1000)\nytest = system.predict(xtest)  # Will estimate the field qty in the latent/compressed space\n\n# Reconstruct the full field quantity\nytest_reconstructed, _ = to_model_dataset(ytest, system.outputs())\n</code></pre> Note that the surrogate will predict the field quantity in its \"latent\" or compressed space. To obtain the full field quantity, we must reconstruct the field using the inverse of the compression map (which is stored in the <code>field_qty</code> variable). The <code>to_model_dataset</code> utility provided by <code>amisc</code> will do this for you.</p>"},{"location":"start/","title":"Getting started","text":"<p>Efficient framework for building surrogates of multidisciplinary systems using the adaptive multi-index stochastic collocation (AMISC)  technique.</p>"},{"location":"start/#installation","title":"\u2699\ufe0f Installation","text":"<p>Ensure you are using Python 3.11 or later. You can install the package from PyPI using pip: <pre><code>pip install amisc\n</code></pre> If you are using pdm in your own project, then you can use: <pre><code>pdm add amisc\n\n# Or in editable mode from a local clone...\npdm add -e ./amisc --dev\n</code></pre></p>"},{"location":"start/#quickstart","title":"\ud83d\udccd Quickstart","text":"<pre><code>import numpy as np\n\nfrom amisc import System\n\ndef fun1(x):\n    y = x * np.sin(np.pi * x)\n    return y\n\ndef fun2(y):\n    z = 1 / (1 + 25 * y ** 2)\n    return z\n\nsystem = System(fun1, fun2)\n\nsystem.inputs()['x'].domain = (0, 1)   # set domain of surrogate for `x`\nsystem.outputs()['y'].domain = (0, 1)  # set domain of surrogate for `y`\n\nsystem.fit()\n\nx_test = system.sample_inputs(10)\ny_test = system.predict(x_test)\n</code></pre>"},{"location":"start/#contributing","title":"\ud83c\udfd7\ufe0f Contributing","text":"<p>See the contribution guidelines.</p>"},{"location":"start/#reference","title":"\ud83d\udcd6 Reference","text":"<p>AMISC paper [1].</p> <p><sup><sub>Made with the copier-numpy template.</sub></sup></p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>The basic goal of this tutorial is to get up and running with the <code>amisc</code> package. It assumes you have a working Python 3.11+ environment with <code>amisc &gt;= v0.5.0</code> properly installed.</p> <p>Contents:</p> <ul> <li>Introduce the basic classes</li> <li>Interfacing an external model</li> <li>Connecting models together</li> <li>Working with model fidelities</li> <li>Training a surrogate model</li> <li>Evaluate surrogate performance</li> <li>Saving and loading from file</li> </ul> <p>Estimated time to complete: 1 hour  Download Notebook </p> In\u00a0[1]: Copied! <pre>from amisc import Variable\n\nx = Variable()\ny = Variable(description='My variable')\nz = Variable(description='Another variable', units='m/s')\n\nprint(x, y, z)  # will show \"x, y, z\"\n</pre> from amisc import Variable  x = Variable() y = Variable(description='My variable') z = Variable(description='Another variable', units='m/s')  print(x, y, z)  # will show \"x, y, z\" <pre>x y z\n</pre> <p>As you can see, variables are nothing more than a placeholder, just like in any equation. You can treat them mostly as a string with some extra optional properties (like units or a description). In fact, comparing variables to their string counterparts will return True!</p> In\u00a0[2]: Copied! <pre>print(x == 'x')\nprint(x == x.name)  # alternatively\n</pre> print(x == 'x') print(x == x.name)  # alternatively <pre>True\nTrue\n</pre> <p>Of course, these variables can do more than just plain strings -- we will see some more examples later.</p> <p>For now, let's move to constructing a component model. A model is simply a function that takes a set of inputs and computes a set of outputs. For example, say you have an algebraic \"model\" (i.e. function) that predicts the force of gravity between two objects:</p> <p>$$ F_g = G\\frac{m_1 m_2}{r^2} $$</p> <p>We can write this as a Python function fairly simply:</p> In\u00a0[3]: Copied! <pre>def force_of_gravity(m1, m2, r):\n    G = 6.6743e-11                \n    Fg = G * (m1 * m2) / r ** 2\n    return Fg\n</pre> def force_of_gravity(m1, m2, r):     G = 6.6743e-11                     Fg = G * (m1 * m2) / r ** 2     return Fg <p>And now, we can integrate this model into <code>amisc</code> by wrapping it with the <code>Component</code>:</p> In\u00a0[4]: Copied! <pre>from amisc import Component\n\ngravity_model = Component(force_of_gravity)\n</pre> from amisc import Component  gravity_model = Component(force_of_gravity) <p>We can view the input and output variables by printing our new component:</p> In\u00a0[5]: Copied! <pre>print(gravity_model)\n</pre> print(gravity_model) <pre>---- gravity_model ----\nInputs:  [m1, m2, r]\nOutputs: [Fg]\nModel:   &lt;function force_of_gravity at 0x7f4cfd80c900&gt;\n</pre> <p>Note how the <code>m1, m2, r</code> inputs were inspected directly from the function signature, as well as the <code>Fg</code> output from the return statement -- this highlights that <code>Component</code> models are nothing more than a callable function with a set of <code>Variable</code> inputs and <code>Variable</code> outputs.</p> <p>Let's now say we have a second model that predicts the acceleration of our second object under only the influence of gravity:</p> <p>$$ A_g = F_g / m_2 $$</p> <p>And the corresponding <code>Component</code>:</p> In\u00a0[6]: Copied! <pre>def acceleration(Fg, m2):\n    Ag = Fg / m2\n    return Ag\n\naccel_model = Component(acceleration)\n</pre> def acceleration(Fg, m2):     Ag = Fg / m2     return Ag  accel_model = Component(acceleration) <p>We would now like to predict both models in concert, i.e. first compute $F_g$ using the gravity model and then $A_g$ using the acceleration model. We construct this MD system as:</p> In\u00a0[7]: Copied! <pre>from amisc import System\n\nmd_system = System(gravity_model, accel_model)\n</pre> from amisc import System  md_system = System(gravity_model, accel_model) <p>We can now make all of our predictions with a single call on the <code>System</code>:</p> In\u00a0[8]: Copied! <pre>pred = md_system.predict({'m1': 5.972e24,  # kg  (the Earth)\n                          'm2': 68,        # kg  (and you)\n                          'r' : 6.37e6     # m   (on the surface of Earth)\n                         }, use_model='best')\n</pre> pred = md_system.predict({'m1': 5.972e24,  # kg  (the Earth)                           'm2': 68,        # kg  (and you)                           'r' : 6.37e6     # m   (on the surface of Earth)                          }, use_model='best') <p>Now, printing the outputs, we see the value of the force as <code>Fg</code> in Newtons and the expected acceleration due to gravity of $A_g = 9.8\\ m/s^2$:</p> In\u00a0[9]: Copied! <pre>print(pred)\n</pre> print(pred) <pre>{'Fg': array([667.96786664]), 'Ag': array([9.82305686])}\n</pre> In\u00a0[10]: Copied! <pre>from amisc import Component\n\ndef thermal_analysis(Ta, d):\n    \"\"\"Run FEA thermal study of a heater component.\n    \n    :param Ta: the ambient temperature (K)\n    :param d: the heating component diameter (m)\n    :returns: sigma, the maximum thermal stress (kPA)\n    \"\"\"\n    import subprocess\n    \n    with open('input.ini', 'a') as fd:\n        fd.writelines([f'Ambient temperature = {Ta}',\n                       f'Component diameter = {d}'\n                      ])\n\n    subprocess.run(['my_fea_binary', '--input_file', 'input.ini', '--out', 'results.out'])\n\n    with open('results.out', 'r') as fd:\n        results = fd.readlines()\n\n    sigma = float(results[-1])  # hypothetical, if the result was stored on the last line\n\n    return sigma\n\nthermal_model = Component(thermal_analysis)\nprint(thermal_model)\n</pre> from amisc import Component  def thermal_analysis(Ta, d):     \"\"\"Run FEA thermal study of a heater component.          :param Ta: the ambient temperature (K)     :param d: the heating component diameter (m)     :returns: sigma, the maximum thermal stress (kPA)     \"\"\"     import subprocess          with open('input.ini', 'a') as fd:         fd.writelines([f'Ambient temperature = {Ta}',                        f'Component diameter = {d}'                       ])      subprocess.run(['my_fea_binary', '--input_file', 'input.ini', '--out', 'results.out'])      with open('results.out', 'r') as fd:         results = fd.readlines()      sigma = float(results[-1])  # hypothetical, if the result was stored on the last line      return sigma  thermal_model = Component(thermal_analysis) print(thermal_model) <pre>---- thermal_model ----\nInputs:  [Ta, d]\nOutputs: [sigma]\nModel:   &lt;function thermal_analysis at 0x7f4cfd852c00&gt;\n</pre> <p>As the variables grow in number and complexity, it will often be more convenient to batch all inputs or outputs together and unpack them inside the model where needed. For example, if the thermal model above had many more inputs or outputs, we would define the <code>Component</code> to take a single set of inputs and return a single set of outputs:</p> In\u00a0[11]: Copied! <pre>def thermal_analysis(inputs):             # single `dict` of inputs\n    Ta = inputs['Ta']\n    d = inputs['d']\n    other_inputs = ...\n\n    # Compute the model\n\n    sigma = float(results[-1])\n    other = ...\n\n    return {'sigma': sigma, 'other': ...}  # single `dict` of outputs\n\ninputs = ['Ta', 'd', 'x1', 'x2']  # ...\noutputs = ['sigma', 'other']      # ...\nthermal_model = Component(thermal_analysis, inputs, outputs)\n\nprint(thermal_model)\n</pre> def thermal_analysis(inputs):             # single `dict` of inputs     Ta = inputs['Ta']     d = inputs['d']     other_inputs = ...      # Compute the model      sigma = float(results[-1])     other = ...      return {'sigma': sigma, 'other': ...}  # single `dict` of outputs  inputs = ['Ta', 'd', 'x1', 'x2']  # ... outputs = ['sigma', 'other']      # ... thermal_model = Component(thermal_analysis, inputs, outputs)  print(thermal_model) <pre>---- thermal_model ----\nInputs:  [Ta, d, x1, x2]\nOutputs: [sigma, other]\nModel:   &lt;function thermal_analysis at 0x7f4d4882fce0&gt;\n</pre> <p>As one last note about models, <code>amisc</code> will always treat positional arguments as numeric input <code>Variable's</code>. If you need to pass extra arguments or configs to your model, but don't necessarily want them to be considered as numeric variables, then you can pass them as keyword arguments to <code>Component</code>. For example, say your model needs to load additional settings from a config file, then you would pass <code>config_file</code> as a <code>kwarg</code> to <code>Component</code>:</p> In\u00a0[12]: Copied! <pre>def thermal_analysis(numeric_inputs, config_file=None, **kwargs):\n    Ta = numeric_inputs['Ta']\n    d = ...\n    # etc.\n\n    with open(config_file, 'r') as fd:\n        # Read in additional configs for the model\n        configs = fd.readlines()\n\n    return numeric_outputs\n\ninputs = ['Ta', 'd']\noutputs = ['sigma']\nkwargs = {'extra': 'configs', 'go': 'here'}\nthermal_model = Component(thermal_analysis, inputs, outputs, config_file='my_config.json', **kwargs)\n</pre> def thermal_analysis(numeric_inputs, config_file=None, **kwargs):     Ta = numeric_inputs['Ta']     d = ...     # etc.      with open(config_file, 'r') as fd:         # Read in additional configs for the model         configs = fd.readlines()      return numeric_outputs  inputs = ['Ta', 'd'] outputs = ['sigma'] kwargs = {'extra': 'configs', 'go': 'here'} thermal_model = Component(thermal_analysis, inputs, outputs, config_file='my_config.json', **kwargs) In\u00a0[13]: Copied! <pre>import numpy as np\n\ndef f1(x):\n    y1 = x * np.sin(np.pi * x)\n    return y1\n    \ndef f2(y1):\n    y2 = 1 / (1 + 25*y1**2)\n    return y2\n\ndef f3(x, y2):\n    y3 = x * np.cos(np.pi * y2)\n    return y3\n\nmd_system = System(f1, f2, f3)\n</pre> import numpy as np  def f1(x):     y1 = x * np.sin(np.pi * x)     return y1      def f2(y1):     y2 = 1 / (1 + 25*y1**2)     return y2  def f3(x, y2):     y3 = x * np.cos(np.pi * y2)     return y3  md_system = System(f1, f2, f3) <p>We can inspect the <code>System</code> graph to show that we do indeed get the $f_1\\rightarrow f_2\\rightarrow f_3$ coupling that we expected:</p> In\u00a0[14]: Copied! <pre>import networkx as nx\n\nnx.draw(md_system.graph(), with_labels=True)\nprint(md_system)\n</pre> import networkx as nx  nx.draw(md_system.graph(), with_labels=True) print(md_system) <pre>---- System_737 ----\namisc version: 0.7.3\nRefinement level: 0\nComponents: f1, f2, f3\nInputs:     x\nOutputs:    y3, y2, y1\n</pre> <p>We can now view the model outputs over the range $x\\in(0, 1)$.</p> In\u00a0[15]: Copied! <pre>import matplotlib.pyplot as plt\n\nx_grid = {'x': np.linspace(0, 1, 100)}\ny_grid = md_system.predict(x_grid, use_model='best')\n\nfig, ax = plt.subplots(1, 3, figsize=(8, 3), layout='tight', sharey='row')\nax[0].plot(x_grid['x'], y_grid['y1'], '-r')\nax[1].plot(x_grid['x'], y_grid['y2'], '-r')\nax[2].plot(x_grid['x'], y_grid['y3'], '-r')\nax[0].set_xlabel('$x$')\nax[1].set_xlabel('$x$')\nax[2].set_xlabel('$x$')\nax[0].set_ylabel('$f(x)$')\nplt.show()\n</pre> import matplotlib.pyplot as plt  x_grid = {'x': np.linspace(0, 1, 100)} y_grid = md_system.predict(x_grid, use_model='best')  fig, ax = plt.subplots(1, 3, figsize=(8, 3), layout='tight', sharey='row') ax[0].plot(x_grid['x'], y_grid['y1'], '-r') ax[1].plot(x_grid['x'], y_grid['y2'], '-r') ax[2].plot(x_grid['x'], y_grid['y3'], '-r') ax[0].set_xlabel('$x$') ax[1].set_xlabel('$x$') ax[2].set_xlabel('$x$') ax[0].set_ylabel('$f(x)$') plt.show() <p>One of the primary motivations for studying models in a \"decoupled\" way like this is that individual component models are typically much simpler when treated independently than when they are convolved with other models. As we will see, approximating the models in this decoupled fashion has many advantages over the traditional \"black-box\" approach.</p> <p>Before we get to building surrogates, we must first understand one of the core features of <code>amisc</code>, and that is multi-fidelity hierarchies.</p> <p>As an example, consider the model:</p> <p>\\begin{align} y &amp;= f(x) = \\cos\\left(\\frac{\\pi}{2}(x + \\frac{4}{5} + \\frac{\\epsilon}{5})\\right)\\\\ \\epsilon &amp;= 2^{-\\alpha_0}\\\\ \\text{for}\\ \\alpha_0 &amp;= (0, 1, 2, \\dots)\\\\ \\end{align}</p> <p>As the $\\alpha$ index increases, $\\epsilon\\rightarrow 0$ and the \"fidelity\" of the model increases. Let's build this model and show some predictions for varying $\\alpha$ fidelity. Note that we should request the <code>model_fidelity</code> keyword in our component model.</p> In\u00a0[16]: Copied! <pre>def multilevel_model(inputs, model_fidelity=(0,)):\n    alpha0 = model_fidelity[0]\n    eps = 2**(-alpha0)\n    return {'y': np.cos(np.pi/2 * (inputs['x'] + 4/5 + (1/5)*eps))}\n\nmf_comp = Component(multilevel_model, Variable('x'), Variable('y'), model_fidelity=(2,))\n\n# Plot for varying fidelity\nx_grid = {'x': np.linspace(-1, 1, 100)}\ny_truth = multilevel_model(x_grid, (10,))  # Use a really high alpha for the \"truth\"\n\nfig, ax = plt.subplots()\nax.plot(x_grid['x'], y_truth['y'], '-k', label='Truth')\n\nfor i in range(3):\n    y_grid = mf_comp.call_model(x_grid, model_fidelity=(i,))\n    ax.plot(x_grid['x'], y_grid['y'], label=f'$\\\\alpha$={i}')\n\nax.set_xlabel('Input ($x$)')\nax.set_ylabel('Output ($y$)')\nax.legend()\nplt.show()\n</pre> def multilevel_model(inputs, model_fidelity=(0,)):     alpha0 = model_fidelity[0]     eps = 2**(-alpha0)     return {'y': np.cos(np.pi/2 * (inputs['x'] + 4/5 + (1/5)*eps))}  mf_comp = Component(multilevel_model, Variable('x'), Variable('y'), model_fidelity=(2,))  # Plot for varying fidelity x_grid = {'x': np.linspace(-1, 1, 100)} y_truth = multilevel_model(x_grid, (10,))  # Use a really high alpha for the \"truth\"  fig, ax = plt.subplots() ax.plot(x_grid['x'], y_truth['y'], '-k', label='Truth')  for i in range(3):     y_grid = mf_comp.call_model(x_grid, model_fidelity=(i,))     ax.plot(x_grid['x'], y_grid['y'], label=f'$\\\\alpha$={i}')  ax.set_xlabel('Input ($x$)') ax.set_ylabel('Output ($y$)') ax.legend() plt.show() <p>Note how we chose to use the internal <code>Component.call_model</code> function, which is a lightweight wrapper around the actual model itself -- we will see later some of the benefits of using <code>call_model</code> over the raw model function.</p> In\u00a0[17]: Copied! <pre>import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n\nfig, ax = plt.subplots()\nax.set_xlabel(r'Parametric fidelity ($\\beta$)')\nax.set_ylabel(r'Model fidelity ($\\alpha$)')\nax.set_xlim(0, 3)\nax.set_ylim(0, 3)\n\nax.set_xticks([0.5, 1.5, 2.5])\nax.set_xticklabels(['0', '1', '2'])\nax.set_yticks([0.5, 1.5, 2.5])\nax.set_yticklabels(['0', '1', '2'])\n\n# Draw a box for each multi-index\nboxes = [\n    (0, 0, 'red'),\n    (1, 0, 'red'),\n    (0, 1, 'red'),\n    (1, 1, 'gray'),\n    (0, 2, 'gray'),\n    (2, 0, 'gray')\n]\nwidth  = 1\nheight = 1\nalpha  = 0.5\n\n# Add boxes to the plot\nfor i, (x, y, color) in enumerate(boxes):\n    label = 'Activated' if color == 'red' else 'Candidate'\n    rect = patches.Rectangle((x, y), width, height, linewidth=1, \n                             label=label if i in [0, 3] else '',\n                             edgecolor=color, facecolor=color, alpha=alpha)\n    ax.add_patch(rect)\n\nax.legend()\nplt.show()\n</pre> import matplotlib.pyplot as plt import matplotlib.patches as patches   fig, ax = plt.subplots() ax.set_xlabel(r'Parametric fidelity ($\\beta$)') ax.set_ylabel(r'Model fidelity ($\\alpha$)') ax.set_xlim(0, 3) ax.set_ylim(0, 3)  ax.set_xticks([0.5, 1.5, 2.5]) ax.set_xticklabels(['0', '1', '2']) ax.set_yticks([0.5, 1.5, 2.5]) ax.set_yticklabels(['0', '1', '2'])  # Draw a box for each multi-index boxes = [     (0, 0, 'red'),     (1, 0, 'red'),     (0, 1, 'red'),     (1, 1, 'gray'),     (0, 2, 'gray'),     (2, 0, 'gray') ] width  = 1 height = 1 alpha  = 0.5  # Add boxes to the plot for i, (x, y, color) in enumerate(boxes):     label = 'Activated' if color == 'red' else 'Candidate'     rect = patches.Rectangle((x, y), width, height, linewidth=1,                               label=label if i in [0, 3] else '',                              edgecolor=color, facecolor=color, alpha=alpha)     ax.add_patch(rect)  ax.legend() plt.show() <p>Note how the red boxes are the \"activated\" multi-indices (i.e. the ones we have built so far) and the gray boxes are the next \"candidate\" multi-indices, or the nearest neighbors that we may choose to build next.</p> <p>In higher dimensions, this diagram would get harder to visualize, but the premise is the same: we approximate a model by stacking up lower-fidelity approximations in an iterative fashion.</p> <p>Recall the multi-level model from earlier:</p> <p>\\begin{align} y &amp;= f(x) = \\cos\\left(\\frac{\\pi}{2}(x + \\frac{4}{5} + \\frac{\\epsilon}{5})\\right)\\\\ \\epsilon &amp;= 2^{-\\alpha_0}\\\\ \\text{for}\\ \\alpha_0 &amp;= (0, 1, 2, \\dots)\\\\ \\end{align}</p> <p>Let's iteratively construct a surrogate for this model, with $\\beta$ representing the amount of training data (note the <code>data_fidelity</code> keyword). We'll also train the surrogate over the domain $x\\in (-1, 1)$.</p> In\u00a0[18]: Copied! <pre>comp = Component(multilevel_model, \n                 Variable('x', domain=(-1, 1)), \n                 Variable('y'),\n                 model_fidelity=(2,), \n                 data_fidelity=(2,))\n\nimport itertools\n\nprint('alpha beta')\nfor multi_index in itertools.product(range(3), range(3)):\n    alpha = multi_index[:1]\n    beta = multi_index[1:]\n    print(f'{str(alpha):5s} {str(beta):4s}')\n\n    comp.activate_index(alpha, beta)  # 'training' happens here as gray boxes -&gt; red boxes\n</pre> comp = Component(multilevel_model,                   Variable('x', domain=(-1, 1)),                   Variable('y'),                  model_fidelity=(2,),                   data_fidelity=(2,))  import itertools  print('alpha beta') for multi_index in itertools.product(range(3), range(3)):     alpha = multi_index[:1]     beta = multi_index[1:]     print(f'{str(alpha):5s} {str(beta):4s}')      comp.activate_index(alpha, beta)  # 'training' happens here as gray boxes -&gt; red boxes <pre>alpha beta\n(0,)  (0,)\n(0,)  (1,)\n(0,)  (2,)\n(1,)  (0,)\n(1,)  (1,)\n(1,)  (2,)\n(2,)  (0,)\n(2,)  (1,)\n(2,)  (2,)\n</pre> <p>Now that we've \"activated\" all of the multi-indices, let's take a look at the results. In the following grid, we'll plot increasing $\\beta$ fidelity along the x-axis and increasing $\\alpha$ fidelity along the y-axis.</p> In\u00a0[19]: Copied! <pre>xg = {'x': np.linspace(-1, 1, 100)}\ny_truth = comp.call_model(xg, model_fidelity=(15,))\n\nfig, axs = plt.subplots(3, 3, sharey='row', sharex='col')\n\nfor alpha in range(3):\n    for beta in range(3):\n        ax = axs[2-alpha, beta]\n        \n        xi, yi = comp.training_data.get((alpha,), (beta,))\n        y_surr = comp.interpolator.predict(xg, comp.misc_states[(alpha,), (beta,)], (xi, yi))\n        \n        s = rf'$\\hat{{f}}_{{{alpha}, {beta}}}$'\n        ax.plot(xg['x'], y_surr['y'], '--k', label=r'{}'.format(s), linewidth=1.5)\n        \n        s = rf'$\\hat{{f}}_{alpha}$'\n        ax.plot(xg['x'], comp.call_model(xg, (alpha,))['y'], '--b', label=r'{}'.format(s), linewidth=2)\n        ax.plot(xg['x'], y_truth['y'], '-r', label=r'$f$', linewidth=2)\n        \n        ax.plot(xi['x'], yi['y'], 'ok')\n        ax.set_xlabel('$x$' if alpha == 0 else '')\n        ax.set_ylabel('$f(x)$' if beta == 0 else '')\n        ax.legend()\n\nfig.text(0.5, 0.02, r'Increasing parametric fidelity ($\\beta$) $\\rightarrow$', ha='center', fontweight='bold')\nfig.text(0.02, 0.5, r'Increasing model fidelity ($\\alpha$) $\\rightarrow$', va='center', fontweight='bold',\n         rotation='vertical')\nfig.set_size_inches(9, 9)\nfig.tight_layout(pad=3, w_pad=1, h_pad=1)\nplt.show()\n</pre> xg = {'x': np.linspace(-1, 1, 100)} y_truth = comp.call_model(xg, model_fidelity=(15,))  fig, axs = plt.subplots(3, 3, sharey='row', sharex='col')  for alpha in range(3):     for beta in range(3):         ax = axs[2-alpha, beta]                  xi, yi = comp.training_data.get((alpha,), (beta,))         y_surr = comp.interpolator.predict(xg, comp.misc_states[(alpha,), (beta,)], (xi, yi))                  s = rf'$\\hat{{f}}_{{{alpha}, {beta}}}$'         ax.plot(xg['x'], y_surr['y'], '--k', label=r'{}'.format(s), linewidth=1.5)                  s = rf'$\\hat{{f}}_{alpha}$'         ax.plot(xg['x'], comp.call_model(xg, (alpha,))['y'], '--b', label=r'{}'.format(s), linewidth=2)         ax.plot(xg['x'], y_truth['y'], '-r', label=r'$f$', linewidth=2)                  ax.plot(xi['x'], yi['y'], 'ok')         ax.set_xlabel('$x$' if alpha == 0 else '')         ax.set_ylabel('$f(x)$' if beta == 0 else '')         ax.legend()  fig.text(0.5, 0.02, r'Increasing parametric fidelity ($\\beta$) $\\rightarrow$', ha='center', fontweight='bold') fig.text(0.02, 0.5, r'Increasing model fidelity ($\\alpha$) $\\rightarrow$', va='center', fontweight='bold',          rotation='vertical') fig.set_size_inches(9, 9) fig.tight_layout(pad=3, w_pad=1, h_pad=1) plt.show() <p>In the grid above, the true function that we wish to approximate is the red $f(x)$. In each row, the dashed blue $\\hat{f}_{\\alpha}$ gives the various physical model fidelity approximations. The black markers give the surrogate training data. As we move from left to right, $\\beta$ increases and so more training data is provided for a given $\\hat{f}_{\\alpha}$ and the surrogate approximation (black dashed line) improves. Finally, as we move from bottom to top, the surrogate matches the highest physical model fidelity better and better.</p> <p>While we have shown that you can train the surrogate by iteratively calling <code>activate_index()</code> on the <code>Component</code>, the AMISC algorithm provides a globally adaptive procedure for doing this automatically. While we won't go into the details for the purposes of this tutorial, the general idea is that we search over all the possible combinations of $(\\alpha, \\beta)$ for every component and select the indices with the most potential for improvement.</p> <p>To do this, we'll wrap our component in the <code>System</code> and then call <code>System.fit()</code>. That's it! We'll turn on the logger so you can see how this adaptive procedure unfolds.</p> In\u00a0[20]: Copied! <pre>comp.clear()  # Reset\n\nsystem = System(comp, name='MF tutorial')\nsystem.set_logger(stdout=True)\n\nsystem.fit()\n</pre> comp.clear()  # Reset  system = System(comp, name='MF tutorial') system.set_logger(stdout=True)  system.fit() <pre>2025-03-10 15:12:22,886 \u2014 [INFO] \u2014 MF tutorial     \u2014 -------------------Refining system surrogate: iteration 1-------------------\n</pre> <pre>2025-03-10 15:12:22,887 \u2014 [INFO] \u2014 MF tutorial     \u2014 Initializing component multilevel_model: adding ((0,), (0,)) to active set\n</pre> <pre>2025-03-10 15:12:22,892 \u2014 [INFO] \u2014 multilevel_model \u2014 Running 4 total model evaluations for component 'multilevel_model' new candidate indices: [((1,), (0,)), ((0,), (0,)), ((0,), (1,))]...\n</pre> <pre>2025-03-10 15:12:22,893 \u2014 [INFO] \u2014 multilevel_model \u2014 Model evaluations complete for component 'multilevel_model'.\n</pre> <pre>2025-03-10 15:12:22,895 \u2014 [INFO] \u2014 MF tutorial     \u2014 -------------------Refining system surrogate: iteration 2-------------------\n</pre> <pre>2025-03-10 15:12:22,896 \u2014 [INFO] \u2014 MF tutorial     \u2014 Estimating error for component 'multilevel_model'...\n</pre> <pre>2025-03-10 15:12:22,898 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((1,), (0,)). Relative error: 2554768691661083.5. Error indicator: 2554768691661083.5.\n</pre> <pre>2025-03-10 15:12:22,899 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((0,), (1,)). Relative error: 9083586330063998.0. Error indicator: 4541793165031999.0.\n</pre> <pre>2025-03-10 15:12:22,899 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index ((0,), (1,)) chosen for component 'multilevel_model'.\n</pre> <pre>2025-03-10 15:12:22,907 \u2014 [INFO] \u2014 multilevel_model \u2014 Running 2 total model evaluations for component 'multilevel_model' new candidate indices: [((0,), (2,))]...\n</pre> <pre>2025-03-10 15:12:22,907 \u2014 [INFO] \u2014 multilevel_model \u2014 Model evaluations complete for component 'multilevel_model'.\n</pre> <pre>2025-03-10 15:12:22,908 \u2014 [INFO] \u2014 MF tutorial     \u2014 -------------------Refining system surrogate: iteration 3-------------------\n</pre> <pre>2025-03-10 15:12:22,910 \u2014 [INFO] \u2014 MF tutorial     \u2014 Estimating error for component 'multilevel_model'...\n</pre> <pre>2025-03-10 15:12:22,913 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((0,), (2,)). Relative error: 0.2512565849175013. Error indicator: 0.12562829245875065.\n</pre> <pre>2025-03-10 15:12:22,913 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((1,), (0,)). Relative error: 0.27462246333531765. Error indicator: 0.27462246333531765.\n</pre> <pre>2025-03-10 15:12:22,914 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index ((1,), (0,)) chosen for component 'multilevel_model'.\n</pre> <pre>2025-03-10 15:12:22,914 \u2014 [INFO] \u2014 multilevel_model \u2014 Running 3 total model evaluations for component 'multilevel_model' new candidate indices: [((2,), (0,)), ((1,), (1,))]...\n</pre> <pre>2025-03-10 15:12:22,915 \u2014 [INFO] \u2014 multilevel_model \u2014 Model evaluations complete for component 'multilevel_model'.\n</pre> <pre>2025-03-10 15:12:22,917 \u2014 [INFO] \u2014 MF tutorial     \u2014 -------------------Refining system surrogate: iteration 4-------------------\n</pre> <pre>2025-03-10 15:12:22,918 \u2014 [INFO] \u2014 MF tutorial     \u2014 Estimating error for component 'multilevel_model'...\n</pre> <pre>2025-03-10 15:12:22,922 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((1,), (1,)). Relative error: 0.11643367707089315. Error indicator: 0.058216838535446576.\n</pre> <pre>2025-03-10 15:12:22,923 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((0,), (2,)). Relative error: 0.23629914374609073. Error indicator: 0.11814957187304537.\n</pre> <pre>2025-03-10 15:12:22,924 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((2,), (0,)). Relative error: 0.1208563341790667. Error indicator: 0.1208563341790667.\n</pre> <pre>2025-03-10 15:12:22,924 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index ((2,), (0,)) chosen for component 'multilevel_model'.\n</pre> <pre>2025-03-10 15:12:22,925 \u2014 [INFO] \u2014 MF tutorial     \u2014 -------------------Refining system surrogate: iteration 5-------------------\n</pre> <pre>2025-03-10 15:12:22,926 \u2014 [INFO] \u2014 MF tutorial     \u2014 Estimating error for component 'multilevel_model'...\n</pre> <pre>2025-03-10 15:12:22,929 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((1,), (1,)). Relative error: 0.10792353340887337. Error indicator: 0.053961766704436684.\n</pre> <pre>2025-03-10 15:12:22,930 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((0,), (2,)). Relative error: 0.23599625862493312. Error indicator: 0.11799812931246656.\n</pre> <pre>2025-03-10 15:12:22,930 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index ((0,), (2,)) chosen for component 'multilevel_model'.\n</pre> <pre>2025-03-10 15:12:22,931 \u2014 [INFO] \u2014 MF tutorial     \u2014 -------------------Refining system surrogate: iteration 6-------------------\n</pre> <pre>2025-03-10 15:12:22,933 \u2014 [INFO] \u2014 MF tutorial     \u2014 Estimating error for component 'multilevel_model'...\n</pre> <pre>2025-03-10 15:12:22,935 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((1,), (1,)). Relative error: 0.09608236499496026. Error indicator: 0.04804118249748013.\n</pre> <pre>2025-03-10 15:12:22,935 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index ((1,), (1,)) chosen for component 'multilevel_model'.\n</pre> <pre>2025-03-10 15:12:22,936 \u2014 [INFO] \u2014 multilevel_model \u2014 Running 4 total model evaluations for component 'multilevel_model' new candidate indices: [((2,), (1,)), ((1,), (2,))]...\n</pre> <pre>2025-03-10 15:12:22,937 \u2014 [INFO] \u2014 multilevel_model \u2014 Model evaluations complete for component 'multilevel_model'.\n</pre> <pre>2025-03-10 15:12:22,939 \u2014 [INFO] \u2014 MF tutorial     \u2014 -------------------Refining system surrogate: iteration 7-------------------\n</pre> <pre>2025-03-10 15:12:22,940 \u2014 [INFO] \u2014 MF tutorial     \u2014 Estimating error for component 'multilevel_model'...\n</pre> <pre>2025-03-10 15:12:22,944 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((1,), (2,)). Relative error: 0.007443229987001587. Error indicator: 0.0037216149935007936.\n</pre> <pre>2025-03-10 15:12:22,945 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((2,), (1,)). Relative error: 0.052905485243811744. Error indicator: 0.026452742621905872.\n</pre> <pre>2025-03-10 15:12:22,947 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index ((2,), (1,)) chosen for component 'multilevel_model'.\n</pre> <pre>2025-03-10 15:12:22,948 \u2014 [INFO] \u2014 MF tutorial     \u2014 -------------------Refining system surrogate: iteration 8-------------------\n</pre> <pre>2025-03-10 15:12:22,950 \u2014 [INFO] \u2014 MF tutorial     \u2014 Estimating error for component 'multilevel_model'...\n</pre> <pre>2025-03-10 15:12:22,952 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((1,), (2,)). Relative error: 0.008204383578083632. Error indicator: 0.004102191789041816.\n</pre> <pre>2025-03-10 15:12:22,952 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index ((1,), (2,)) chosen for component 'multilevel_model'.\n</pre> <pre>2025-03-10 15:12:22,953 \u2014 [INFO] \u2014 multilevel_model \u2014 Running 2 total model evaluations for component 'multilevel_model' new candidate indices: [((2,), (2,))]...\n</pre> <pre>2025-03-10 15:12:22,954 \u2014 [INFO] \u2014 multilevel_model \u2014 Model evaluations complete for component 'multilevel_model'.\n</pre> <pre>2025-03-10 15:12:22,956 \u2014 [INFO] \u2014 MF tutorial     \u2014 -------------------Refining system surrogate: iteration 9-------------------\n</pre> <pre>2025-03-10 15:12:22,957 \u2014 [INFO] \u2014 MF tutorial     \u2014 Estimating error for component 'multilevel_model'...\n</pre> <pre>2025-03-10 15:12:22,959 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index: ((2,), (2,)). Relative error: 0.004874344301024019. Error indicator: 0.0024371721505120096.\n</pre> <pre>2025-03-10 15:12:22,960 \u2014 [INFO] \u2014 MF tutorial     \u2014 Candidate multi-index ((2,), (2,)) chosen for component 'multilevel_model'.\n</pre> <pre>2025-03-10 15:12:22,961 \u2014 [INFO] \u2014 MF tutorial     \u2014 -------------------Refining system surrogate: iteration 10-------------------\n</pre> <pre>2025-03-10 15:12:22,962 \u2014 [INFO] \u2014 MF tutorial     \u2014 Estimating error for component 'multilevel_model'...\n</pre> <pre>2025-03-10 15:12:22,962 \u2014 [INFO] \u2014 MF tutorial     \u2014 Component 'multilevel_model' has no available candidates left!\n</pre> <pre>2025-03-10 15:12:22,963 \u2014 [INFO] \u2014 MF tutorial     \u2014 No candidates left for refinement, iteration: 9\n</pre> <pre>2025-03-10 15:12:22,963 \u2014 [INFO] \u2014 MF tutorial     \u2014 -----------------------------Termination criteria reached: No candidates left to refine-----------------------------\n</pre> <pre>2025-03-10 15:12:22,964 \u2014 [INFO] \u2014 MF tutorial     \u2014 Model evaluation algorithm efficiency: 0.15%\n</pre> <pre>2025-03-10 15:12:22,964 \u2014 [INFO] \u2014 MF tutorial     \u2014 Final system surrogate: \n ---- MF tutorial ----\namisc version: 0.7.3\nRefinement level: 9\nComponents: multilevel_model\nInputs:     x\nOutputs:    y\n</pre> <p>At this point, you may be wondering \"what exactly is the surrogate and how exactly is it training?\"</p> <p>By default, we use the original multivariate Lagrange polynomials described in the AMISC journal paper. You can view this as the <code>Component.interpolator</code> property. Lagrange polynomials work well to interpolate smooth response functions up to an input dimension of around 12-15 inputs. The training data is selected by the Leja objective function in a sparse grid format, which is generally a \"space-filling\" design. You can view this as the <code>Component.training_data</code> property. While it is beyond the scope of this tutorial, these properties are designed to be extensible, so that different interpolation or experimental design strategies could be implemented on top of the <code>amisc</code> framework, such as kriging or latin hypercube sampling.</p> <p>The main idea that <code>amisc</code> brings to the proverbial surrogate table is the ability to build multifidelity surrogates of multiple models linked in a multidisciplinary fashion -- the specific mathematical interpolation/surrogate technique is abstracted out and left up to the user.</p> In\u00a0[21]: Copied! <pre>def f1(inputs):\n    x = inputs['x']\n    return {'y1': x * np.sin(np.pi * x)}\n    \ndef f2(inputs):\n    y1 = inputs['y1']\n    return {'y2': 1 / (1 + 25*y1**2)}\n\ndef f3(inputs):\n    x = inputs['x']\n    y2 = inputs['y2']\n    return {'y3': x * np.cos(np.pi * y2)}\n\n\nmd_system = System(Component(f1, Variable('x', domain=(0, 1)), Variable('y1'), data_fidelity=(2,)),\n                   Component(f2, Variable('y1', domain=(0, 1)), Variable('y2'), data_fidelity=(2,)),\n                   Component(f3, ['x', Variable('y2', domain=(0, 1))], Variable('y3'), data_fidelity=(2, 2)),\n                   name='3-component system'\n                  )\n</pre> def f1(inputs):     x = inputs['x']     return {'y1': x * np.sin(np.pi * x)}      def f2(inputs):     y1 = inputs['y1']     return {'y2': 1 / (1 + 25*y1**2)}  def f3(inputs):     x = inputs['x']     y2 = inputs['y2']     return {'y3': x * np.cos(np.pi * y2)}   md_system = System(Component(f1, Variable('x', domain=(0, 1)), Variable('y1'), data_fidelity=(2,)),                    Component(f2, Variable('y1', domain=(0, 1)), Variable('y2'), data_fidelity=(2,)),                    Component(f3, ['x', Variable('y2', domain=(0, 1))], Variable('y3'), data_fidelity=(2, 2)),                    name='3-component system'                   ) <p>Note how, for the default <code>Component.training_data = SparseGrid</code>, the data fidelity must be an indication of the amount of training data in each input dimension. As a result, <code>len(data_fidelity) = len(inputs)</code> for each component. For details, look into the <code>amisc.training</code> module.</p> <p>Also note that models may not have any $\\alpha$ fidelity indices -- this is completely fine! The surrogate will be built over any $\\beta$ indices instead. However, if you do not provide $\\alpha$ or $\\beta$ (i.e. <code>model_fidelity</code>, <code>data_fidelity</code>, or <code>surrogate_fidelity</code>) then <code>system.predict()</code> will just call the underlying models directly, since no surrogate can be built in the absence of fidelity indices.</p> <p>Now, let's run 10 training iterations.</p> In\u00a0[22]: Copied! <pre>md_system.set_logger(stdout=True)\nmd_system.fit(max_iter=10)\n</pre> md_system.set_logger(stdout=True) md_system.fit(max_iter=10) <pre>2025-03-10 15:12:23,053 \u2014 [INFO] \u2014 3-component system \u2014 -------------------Refining system surrogate: iteration 1-------------------\n</pre> <pre>2025-03-10 15:12:23,053 \u2014 [INFO] \u2014 3-component system \u2014 Initializing component f1: adding ((), (0,)) to active set\n</pre> <pre>2025-03-10 15:12:23,059 \u2014 [INFO] \u2014 f1              \u2014 Running 3 total model evaluations for component 'f1' new candidate indices: [((), (0,)), ((), (1,))]...\n</pre> <pre>2025-03-10 15:12:23,060 \u2014 [INFO] \u2014 f1              \u2014 Model evaluations complete for component 'f1'.\n</pre> <pre>2025-03-10 15:12:23,061 \u2014 [INFO] \u2014 3-component system \u2014 -------------------Refining system surrogate: iteration 2-------------------\n</pre> <pre>2025-03-10 15:12:23,062 \u2014 [INFO] \u2014 3-component system \u2014 Initializing component f2: adding ((), (0,)) to active set\n</pre> <pre>2025-03-10 15:12:23,067 \u2014 [INFO] \u2014 f2              \u2014 Running 3 total model evaluations for component 'f2' new candidate indices: [((), (0,)), ((), (1,))]...\n</pre> <pre>2025-03-10 15:12:23,068 \u2014 [INFO] \u2014 f2              \u2014 Model evaluations complete for component 'f2'.\n</pre> <pre>2025-03-10 15:12:23,069 \u2014 [INFO] \u2014 3-component system \u2014 -------------------Refining system surrogate: iteration 3-------------------\n</pre> <pre>2025-03-10 15:12:23,069 \u2014 [INFO] \u2014 3-component system \u2014 Initializing component f3: adding ((), (0, 0)) to active set\n</pre> <pre>2025-03-10 15:12:23,080 \u2014 [INFO] \u2014 f3              \u2014 Running 5 total model evaluations for component 'f3' new candidate indices: [((), (0, 0)), ((), (0, 1)), ((), (1, 0))]...\n</pre> <pre>2025-03-10 15:12:23,080 \u2014 [INFO] \u2014 f3              \u2014 Model evaluations complete for component 'f3'.\n</pre> <pre>2025-03-10 15:12:23,082 \u2014 [INFO] \u2014 3-component system \u2014 -------------------Refining system surrogate: iteration 4-------------------\n</pre> <pre>2025-03-10 15:12:23,084 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f1'...\n</pre> <pre>2025-03-10 15:12:23,086 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (1,)). Relative error: 0.4275084766474035. Error indicator: 0.21375423832370175.\n</pre> <pre>2025-03-10 15:12:23,086 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f2'...\n</pre> <pre>2025-03-10 15:12:23,090 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (1,)). Relative error: 0.0. Error indicator: 0.0.\n</pre> <pre>2025-03-10 15:12:23,090 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f3'...\n</pre> <pre>2025-03-10 15:12:23,094 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (0, 1)). Relative error: 1.182613049627404e+16. Error indicator: 5913065248137020.0.\n</pre> <pre>2025-03-10 15:12:23,095 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (1, 0)). Relative error: 0.5747290562591443. Error indicator: 0.28736452812957214.\n</pre> <pre>2025-03-10 15:12:23,096 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index ((), (0, 1)) chosen for component 'f3'.\n</pre> <pre>2025-03-10 15:12:23,103 \u2014 [INFO] \u2014 f3              \u2014 Running 2 total model evaluations for component 'f3' new candidate indices: [((), (0, 2))]...\n</pre> <pre>2025-03-10 15:12:23,104 \u2014 [INFO] \u2014 f3              \u2014 Model evaluations complete for component 'f3'.\n</pre> <pre>2025-03-10 15:12:23,105 \u2014 [INFO] \u2014 3-component system \u2014 -------------------Refining system surrogate: iteration 5-------------------\n</pre> <pre>2025-03-10 15:12:23,107 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f1'...\n</pre> <pre>2025-03-10 15:12:23,110 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (1,)). Relative error: 0.4051561773133064. Error indicator: 0.2025780886566532.\n</pre> <pre>2025-03-10 15:12:23,110 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f2'...\n</pre> <pre>2025-03-10 15:12:23,113 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (1,)). Relative error: 0.0. Error indicator: 0.0.\n</pre> <pre>2025-03-10 15:12:23,113 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f3'...\n</pre> <pre>2025-03-10 15:12:23,118 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (1, 0)). Relative error: 1.0169814820775418e-16. Error indicator: 5.084907410387709e-17.\n</pre> <pre>2025-03-10 15:12:23,119 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (0, 2)). Relative error: 0.2561815692839554. Error indicator: 0.1280907846419777.\n</pre> <pre>2025-03-10 15:12:23,119 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index ((), (1,)) chosen for component 'f1'.\n</pre> <pre>2025-03-10 15:12:23,126 \u2014 [INFO] \u2014 f1              \u2014 Running 2 total model evaluations for component 'f1' new candidate indices: [((), (2,))]...\n</pre> <pre>2025-03-10 15:12:23,127 \u2014 [INFO] \u2014 f1              \u2014 Model evaluations complete for component 'f1'.\n</pre> <pre>2025-03-10 15:12:23,128 \u2014 [INFO] \u2014 3-component system \u2014 -------------------Refining system surrogate: iteration 6-------------------\n</pre> <pre>2025-03-10 15:12:23,130 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f1'...\n</pre> <pre>2025-03-10 15:12:23,133 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (2,)). Relative error: 0.317833414010419. Error indicator: 0.1589167070052095.\n</pre> <pre>2025-03-10 15:12:23,134 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f2'...\n</pre> <pre>2025-03-10 15:12:23,136 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (1,)). Relative error: 2.4232460329177483. Error indicator: 1.2116230164588742.\n</pre> <pre>2025-03-10 15:12:23,136 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f3'...\n</pre> <pre>2025-03-10 15:12:23,141 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (1, 0)). Relative error: 9.070280731485629e-17. Error indicator: 4.5351403657428146e-17.\n</pre> <pre>2025-03-10 15:12:23,142 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (0, 2)). Relative error: 0.2561815692839554. Error indicator: 0.1280907846419777.\n</pre> <pre>2025-03-10 15:12:23,142 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index ((), (1,)) chosen for component 'f2'.\n</pre> <pre>2025-03-10 15:12:23,149 \u2014 [INFO] \u2014 f2              \u2014 Running 2 total model evaluations for component 'f2' new candidate indices: [((), (2,))]...\n</pre> <pre>2025-03-10 15:12:23,150 \u2014 [INFO] \u2014 f2              \u2014 Model evaluations complete for component 'f2'.\n</pre> <pre>2025-03-10 15:12:23,151 \u2014 [INFO] \u2014 3-component system \u2014 -------------------Refining system surrogate: iteration 7-------------------\n</pre> <pre>2025-03-10 15:12:23,153 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f1'...\n</pre> <pre>2025-03-10 15:12:23,157 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (2,)). Relative error: 0.7438344800546105. Error indicator: 0.37191724002730525.\n</pre> <pre>2025-03-10 15:12:23,157 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f2'...\n</pre> <pre>2025-03-10 15:12:23,160 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (2,)). Relative error: 0.10674596099028917. Error indicator: 0.053372980495144585.\n</pre> <pre>2025-03-10 15:12:23,160 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f3'...\n</pre> <pre>2025-03-10 15:12:23,165 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (1, 0)). Relative error: 1.0662947719876052e-16. Error indicator: 5.331473859938026e-17.\n</pre> <pre>2025-03-10 15:12:23,166 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (0, 2)). Relative error: 0.2999921500882681. Error indicator: 0.14999607504413404.\n</pre> <pre>2025-03-10 15:12:23,166 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index ((), (2,)) chosen for component 'f1'.\n</pre> <pre>2025-03-10 15:12:23,167 \u2014 [INFO] \u2014 3-component system \u2014 -------------------Refining system surrogate: iteration 8-------------------\n</pre> <pre>2025-03-10 15:12:23,169 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f1'...\n</pre> <pre>2025-03-10 15:12:23,170 \u2014 [INFO] \u2014 3-component system \u2014 Component 'f1' has no available candidates left!\n</pre> <pre>2025-03-10 15:12:23,170 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f2'...\n</pre> <pre>2025-03-10 15:12:23,174 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (2,)). Relative error: 0.07576500801598657. Error indicator: 0.037882504007993284.\n</pre> <pre>2025-03-10 15:12:23,174 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f3'...\n</pre> <pre>2025-03-10 15:12:23,179 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (1, 0)). Relative error: 9.007760012141484e-17. Error indicator: 4.503880006070742e-17.\n</pre> <pre>2025-03-10 15:12:23,180 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (0, 2)). Relative error: 0.22493153163957327. Error indicator: 0.11246576581978664.\n</pre> <pre>2025-03-10 15:12:23,180 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index ((), (0, 2)) chosen for component 'f3'.\n</pre> <pre>2025-03-10 15:12:23,181 \u2014 [INFO] \u2014 3-component system \u2014 -------------------Refining system surrogate: iteration 9-------------------\n</pre> <pre>2025-03-10 15:12:23,183 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f1'...\n</pre> <pre>2025-03-10 15:12:23,184 \u2014 [INFO] \u2014 3-component system \u2014 Component 'f1' has no available candidates left!\n</pre> <pre>2025-03-10 15:12:23,184 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f2'...\n</pre> <pre>2025-03-10 15:12:23,187 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (2,)). Relative error: 0.08217385931187739. Error indicator: 0.04108692965593869.\n</pre> <pre>2025-03-10 15:12:23,188 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f3'...\n</pre> <pre>2025-03-10 15:12:23,191 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (1, 0)). Relative error: 4.8004902412919126e-17. Error indicator: 2.4002451206459563e-17.\n</pre> <pre>2025-03-10 15:12:23,191 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index ((), (2,)) chosen for component 'f2'.\n</pre> <pre>2025-03-10 15:12:23,192 \u2014 [INFO] \u2014 3-component system \u2014 -------------------Refining system surrogate: iteration 10-------------------\n</pre> <pre>2025-03-10 15:12:23,195 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f1'...\n</pre> <pre>2025-03-10 15:12:23,195 \u2014 [INFO] \u2014 3-component system \u2014 Component 'f1' has no available candidates left!\n</pre> <pre>2025-03-10 15:12:23,196 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f2'...\n</pre> <pre>2025-03-10 15:12:23,196 \u2014 [INFO] \u2014 3-component system \u2014 Component 'f2' has no available candidates left!\n</pre> <pre>2025-03-10 15:12:23,197 \u2014 [INFO] \u2014 3-component system \u2014 Estimating error for component 'f3'...\n</pre> <pre>2025-03-10 15:12:23,200 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index: ((), (1, 0)). Relative error: 3.8740536685502204e-17. Error indicator: 1.9370268342751102e-17.\n</pre> <pre>2025-03-10 15:12:23,201 \u2014 [INFO] \u2014 3-component system \u2014 Candidate multi-index ((), (1, 0)) chosen for component 'f3'.\n</pre> <pre>2025-03-10 15:12:23,207 \u2014 [INFO] \u2014 f3              \u2014 Running 6 total model evaluations for component 'f3' new candidate indices: [((), (1, 1)), ((), (2, 0))]...\n</pre> <pre>2025-03-10 15:12:23,208 \u2014 [INFO] \u2014 f3              \u2014 Model evaluations complete for component 'f3'.\n</pre> <pre>2025-03-10 15:12:23,210 \u2014 [INFO] \u2014 3-component system \u2014 ------------------------Termination criteria reached: Max iteration 10/10------------------------\n</pre> <pre>2025-03-10 15:12:23,211 \u2014 [INFO] \u2014 3-component system \u2014 Model evaluation algorithm efficiency: 0.12%\n</pre> <pre>2025-03-10 15:12:23,211 \u2014 [INFO] \u2014 3-component system \u2014 Final system surrogate: \n ---- 3-component system ----\namisc version: 0.7.3\nRefinement level: 10\nComponents: f1, f2, f3\nInputs:     x\nOutputs:    y3, y2, y1\n</pre> <p>To determine the performance of the surrogate, let's randomly sample the inputs and compare the surrogate predictions to the ground truth model. For convenience, you can evaluate the underlying model(s) using the same <code>predict()</code> function with the <code>use_model</code> flag.</p> In\u00a0[23]: Copied! <pre>xtest = md_system.sample_inputs(100)\n\nytest_surr  = md_system.predict(xtest)\nytest_model = md_system.predict(xtest, use_model='best')\n\nfrom amisc.utils import relative_error\n\nfor output in ytest_surr:\n    error = relative_error(ytest_surr[output], ytest_model[output])\n    print(f'Relative error for {output} = {error:.3f}')\n</pre> xtest = md_system.sample_inputs(100)  ytest_surr  = md_system.predict(xtest) ytest_model = md_system.predict(xtest, use_model='best')  from amisc.utils import relative_error  for output in ytest_surr:     error = relative_error(ytest_surr[output], ytest_model[output])     print(f'Relative error for {output} = {error:.3f}') <pre>Relative error for y1 = 0.016\nRelative error for y2 = 0.064\nRelative error for y3 = 0.134\n</pre> <p>Not bad! We get around 2% and 5% error for $y_1$ and $y_2$. Since $y_3$ is fit over both $x$ and $y_2$, it is a bit more complicated and may require more training to improve.</p> <p>Another useful diagnostic is to plot the system outputs as functions of the inputs:</p> In\u00a0[24]: Copied! <pre>fig, ax = md_system.plot_slice(show_model='best', random_walk=True)\nplt.show()\n</pre> fig, ax = md_system.plot_slice(show_model='best', random_walk=True) plt.show() <p>These plots can only visualize 1d \"slices\", which provides limited information in higher input dimensions. However, it does provide a good indication of how \"smooth\" the output response is, which may help decide if Lagrange polynomials are sufficient or if some other surrogate method should be used.</p> In\u00a0[25]: Copied! <pre># Save to a temporary directory\nimport tempfile\nimport pathlib\n\nwith tempfile.TemporaryDirectory() as tmp_path:\n    md_system.save_to_file('my_system.yml', save_dir=tmp_path)\n\n    # Later on\n    loaded_system = System.load_from_file(pathlib.Path(tmp_path) / 'my_system.yml')\n</pre> # Save to a temporary directory import tempfile import pathlib  with tempfile.TemporaryDirectory() as tmp_path:     md_system.save_to_file('my_system.yml', save_dir=tmp_path)      # Later on     loaded_system = System.load_from_file(pathlib.Path(tmp_path) / 'my_system.yml') <p>These utility functions use the <code>amisc.YamlLoader</code> class by default to save/load the system from a YAML (<code>.yml</code>) file. Yaml files are very similar text-based formats as <code>.json</code>, but they integrate better with Python objects. You can open a <code>save.yml</code> file in any text-editor to view (and even edit) the <code>amisc</code> objects that are saved there.</p> <p>Important: To consistently save/load callable model functions, the model should be located in some global scope (i.e. no <code>lambda</code> or local functions). This follows the same rules as Python's built-in <code>pickle</code> module -- it only saves a reference to the function's import path, which must be available when you load back from file. If you move or rename the model functions, you can still use the same <code>.yml</code> save file! You'll just have to manually edit the save file to point the models to their new locations.</p> <p>One last note about files -- you can also completely configure an entire <code>amisc.System</code> object from a <code>.yml</code> config file, without having to define anything manually in Python. This is achievable through the use of the three built-in YAML \"tags\" for variables, components, and systems. A \"tag\" is just an exclamation followed by the name of the object, (e.g. <code>!Variable</code>).</p> <p>Our three-component system could be completely defined in a YAML file like this:</p> In\u00a0[26]: Copied! <pre># Loading from file\nyaml_config = \"\"\"\n!System\nname: 3-component system\ncomponents:\n  !Component\n  - model: !!python/name:amisc.examples.models.f1\n    data_fidelity: (2,)\n    inputs: !Variable\n      - name: x\n        domain: (0, 1)\n    outputs: !Variable\n      - name: y1\n    call_unpacked: true  # not reading from a `dict` input (just to silence some warnings)\n    ret_unpacked: true   # not returning a `dict` output\n  - model: !!python/name:amisc.examples.models.f2\n    data_fidelity: (2,)\n    inputs: !Variable\n      - name: y1\n        domain: (0, 1)\n    outputs: !Variable\n      - name: y2\n    call_unpacked: true\n    ret_unpacked: true\n  - model: !!python/name:amisc.examples.models.f3\n    data_fidelity: (2,)\n    inputs: !Variable\n      - name: x\n      - name: y2\n        domain: (0, 1)\n    outputs: !Variable\n      - name: y3\n    call_unpacked: true\n    ret_unpacked: true\n\"\"\"\n</pre> # Loading from file yaml_config = \"\"\" !System name: 3-component system components:   !Component   - model: !!python/name:amisc.examples.models.f1     data_fidelity: (2,)     inputs: !Variable       - name: x         domain: (0, 1)     outputs: !Variable       - name: y1     call_unpacked: true  # not reading from a `dict` input (just to silence some warnings)     ret_unpacked: true   # not returning a `dict` output   - model: !!python/name:amisc.examples.models.f2     data_fidelity: (2,)     inputs: !Variable       - name: y1         domain: (0, 1)     outputs: !Variable       - name: y2     call_unpacked: true     ret_unpacked: true   - model: !!python/name:amisc.examples.models.f3     data_fidelity: (2,)     inputs: !Variable       - name: x       - name: y2         domain: (0, 1)     outputs: !Variable       - name: y3     call_unpacked: true     ret_unpacked: true \"\"\" <p>Note the use of the <code>!!python/name</code> tag to indicate the importable path at which to find the respective component models -- we provide these three functions in the <code>amisc.examples.models</code> module for convenience. For you, one could make a <code>models.py</code> file in the current directory, for example, and then write some function <code>def my_model(inputs): ...</code>. You would then reference this function via yaml as:</p> <p><code>!!python/name:models.my_model</code></p> <p>which will work so long as Python can find the <code>models.py</code> file on its search path (which always includes the current directory).</p> <p>To verify our config file, let's load it using <code>YamlLoader</code>:</p> In\u00a0[27]: Copied! <pre>from amisc import YamlLoader\nimport io\n\nmd_system = YamlLoader.load(io.StringIO(yaml_config))\nprint(md_system)\n</pre> from amisc import YamlLoader import io  md_system = YamlLoader.load(io.StringIO(yaml_config)) print(md_system) <pre>---- 3-component system ----\namisc version: 0.7.3\nRefinement level: 0\nComponents: f1, f2, f3\nInputs:     x\nOutputs:    y3, y2, y1\n</pre>"},{"location":"tutorial/#the-basic-classes","title":"The basic classes\u00b6","text":"<p>Multidisciplinary (MD) systems are constructed from 3 objects:</p> <ul> <li>Variables - These are inputs, outputs, quantities of interest (QoIs), etc. They are the most basic element of any MD system and serve as the datapaths or connections between models. They can be random variables, scalars, field quantities, etc.</li> <li>Components - These are the individual discipline models. They map a set of inputs to a set of outputs.</li> <li>System - This is the top-level MD system. It connects multiple component models and manages the flow of data between them.</li> </ul> <p>So let's start by defining some variables.</p>"},{"location":"tutorial/#interfacing-a-model","title":"Interfacing a model\u00b6","text":"<p>The last example was fairly simplified, since we only had three inputs $m_1, m_2$ and $r$, and the model had a known, algebraic expression. In general, we might have many more inputs/outputs, each with more details like units, domains, nominal values, etc. and our model might be a more complex finite-element simulation, for example, that maybe runs on a remote machine or uses some software outside of Python.</p> <p>From the <code>Component's</code> perspective, all of these details are irrelevant -- the interface that we used for the simple gravity model applies to any model. That is to say, the <code>Component</code> is a black-box wrapper that interfaces any external model into the <code>amisc</code> framework. The beauty of Python as a \"glue\" language means we can make any external calls we need right from within our <code>Component</code> model.</p> <p>As an example, let's say we have some binary on our local machine that reads from an <code>input.ini</code> config file, runs a finite-element thermal analysis of a heater component, and writes to a <code>results.out</code> file. If we want to study the impact of the ambient temperature $T_a$ and the heating element diameter $d$ on the maximum thermal stress $\\sigma$ in the component, then we would interface the model with <code>amisc</code> like this:</p>"},{"location":"tutorial/#connecting-models-together","title":"Connecting models together\u00b6","text":"<p>We have already seen that connecting models together is as easy as wrapping them in a <code>System(comp1, comp2, ...)</code>. Since each individual <code>Component</code> specifies its inputs and outputs, the <code>System</code> automatically connects the components into a graph-like data structure by drawing edges between one component's outputs and another component's inputs.</p> <p>Let's look at an example of a three-component, purely feed-forward system:</p> <p>\\begin{align} y_1 &amp;= f_1(x) = x \\sin(\\pi x)\\\\ y_2 &amp;= f_2(y_1) = \\frac{1}{1 + 25y_1^2}\\\\ y_3 &amp;= f_3(x, y_2) = x \\cos(\\pi y_2) \\end{align}</p> <p>To implement this MD system in <code>amisc</code>, we define each component model and wrap them in a <code>System</code>. Since this is a simple example and there are no extra <code>kwargs</code>, we'll skip the definition of each <code>Component</code> and pass the models directly to <code>System</code>:</p>"},{"location":"tutorial/#model-fidelities","title":"Model fidelities\u00b6","text":"<p>The AMISC algorithm builds on the principle that we can approximate a high-fidelity model by summing or \"averaging\" over many lower-fidelity versions of the model. As such, the primary way to build surrogates in <code>amisc</code> is by specifying some set of fidelity levels.</p> <p>The fidelity \"level\" of a model can very simply be represented as a whole number, with 0 being the \"worst\" fidelity and counting up 1, 2, 3, ... to higher levels. For example, if you are building a model of a fluid, level 0 might be an analytical potential flow model, level 1 might be incompressible flow, level 2 might be a RANS solver, and so on up the scale, with each level bringing greater accuracy.</p> <p>More generally, you might have more than one knob by which to tune the fidelity of your model. So instead of one number associated with one fidelity \"level\", you would have a set of numbers associated with each level, i.e. $(0, 0, ...)$, or $(0, 1, ...)$ etc. We call each set of these numbers a multi-index. AMISC itself is an algorithm for adaptively building up groups of these multi-indices to approximate a model.</p> <p>For our purposes, overall fidelity is divided into two multi-indices: physical model fidelity and parametric model fidelity. Parametric fidelity is further grouped into training data and surrogate fidelities. Each are summarized below:</p> <ul> <li>Physical fidelity - denoted by the multi-index $\\alpha = (\\alpha_1, \\alpha_2, \\dots)$ \u2192 controls the approximation accuracy of the actual physics of the underlying model. This is what one typically thinks of when they hear model \"fidelity\". This includes PDE mesh refinement, time step size, simplifying physics, etc.</li> <li>Parametric fidelity - denoted by the multi-index $\\beta = (\\beta_1, \\beta_2, \\dots)$ \u2192 controls the approximation accuracy of the surrogate or metamodel itself. We further divide $\\beta$ into indices that control the amount of training data (data fidelity) and indices that control the complexity of the surrogate (surrogate fidelity). As $\\beta$ increases, both the amount of training data in the approximation increases, as well as the surrogate complexity (i.e. number of hidden layers, nodes, etc.), in both cases resulting in a more accurate surrogate model.</li> </ul>"},{"location":"tutorial/#training-a-surrogate","title":"Training a surrogate\u00b6","text":"<p>Now that we have an understanding of the component models and their fidelity multi-indices, let's get on with the main purpose of <code>amisc</code> -- building surrogate approximations of the models.</p> <p>As an example, let's say you have a model that can be refined up to $\\alpha=(2,)$ and $\\beta=(2,)$. We would start by building the $(\\alpha, \\beta)=(0, 0)$ multi-index, then the $(0, 1)$ index, then $(1, 0)$, and so on.</p> <p>Here is a graph that illustrates this surrogate building process:</p>"},{"location":"tutorial/#evaluate-surrogate-performance","title":"Evaluate surrogate performance\u00b6","text":"<p>Now that we know how to train a surrogate, we'd like to know how good the fit is. As an example, let's go back to our three-component system:</p> <p>\\begin{align} y_1 &amp;= f_1(x) = x \\sin(\\pi x)\\\\ y_2 &amp;= f_2(y_1) = \\frac{1}{1 + 25y_1^2}\\\\ y_3 &amp;= f_3(x, y_2) = x \\cos(\\pi y_2) \\end{align}</p>"},{"location":"tutorial/#saving-and-loading-from-file","title":"Saving and loading from file\u00b6","text":"<p>After fitting a surrogate, you'll want to save the results for later use (especially if you fit a fairly expensive model). We provide two utilities for data persistence: <code>save_to_file</code> and <code>load_from_file</code>.</p>"},{"location":"tutorial/#advanced-features","title":"Advanced features\u00b6","text":"<p>This completes the intro tutorial! If you have decided <code>amisc</code> might be useful for you, then you can view the online docs for detailed API reference and other advanced features.</p> <p>These include:</p> <ul> <li>Using random variables</li> <li>Normalizing inputs/outputs</li> <li>Fitting a surrogate for high-dimensional field-quantities</li> <li>Handling feedback between models</li> <li>Parallelizing the models and training</li> <li>Extending <code>amisc</code></li> </ul>"},{"location":"guides/config_file/","title":"Write a config file","text":"<p>Configuration files can be written to define <code>amisc</code> objects using the text-based YAML markup language. In general, YAML files contain mappings, sequences, and scalars, which correspond roughly to Python <code>dicts</code>, <code>lists</code>, and <code>strings</code>, respectively.</p> <p>YAML configuration file</p> <pre><code># Mapping (dictionary)\ndescription: My grocery list\nstore: Winco foods\nlocation: 1651 N 400 E\ndate: 11-4-2024\n\n# Sequence (list)\nitems:\n- Bananas\n- Bread\n- Milk\n- Eggs\n\n# Scalar (strings)\nnotes: All values like this one are treated as strings\nblock: |\n    Strings can also be written in \"block\" form\n    so that they may take up multiple lines.\nnumbers: 1.0  # numbers are also strings but will get converted upon loading\n</code></pre> <p>More complicated objects can be defined using YAML \"tags\", which are demarcated by an exclamation followed by the name of the object: <code>!ObjectName</code>. We provide access to three object tags for defining <code>amisc</code> objects in YAML: <code>!Variable</code>, <code>!Component</code>, and <code>!System</code>. The <code>YamlLoader</code> class contains rules for loading these tags into the <code>amisc</code> framework.</p>"},{"location":"guides/config_file/#variables","title":"Variables","text":"<p>Variable objects are constructed with the <code>!Variable</code> tag followed by a mapping of the variable properties: <pre><code>!Variable\nname: x\ndescription: My custom variable\ndomain: (0, 1)\nnominal: 0.5\ndistribution: U(0, 1)\nnorm: minmax\nunits: m/s\n</code></pre></p> <p>A list of variables may be defined with the <code>!VariableList</code> tag: <pre><code>!VariableList\n- name: x1\n  domain: (0, 1)\n- name: x2\n  description: another variable\n- name: x3\n</code></pre></p>"},{"location":"guides/config_file/#components","title":"Components","text":"<p>Component objects are constructed with the <code>!Component</code> tag followed by a mapping of the component properties. Lists of variable inputs and outputs may be defined by nesting the <code>!VariableList</code> tag: <pre><code>!Component\nname: My component\nmodel: !!python/name:my.importable.model_function\nmodel_kwargs:\n  extra_config: A config value\n  options: More options here passed as **kwarg to the model_function\ninputs: !VariableList\n  - name: x1\n  - name: x2\noutputs: !VariableList\n  - name: y1\n  - name: y2\ndata_fidelity: (2, 2)\nvectorized: true\n</code></pre></p> <p>A list of components can be constructed by listing several components in a sequence underneath the <code>!Component</code> tag: <pre><code>!Component\n- name: First component\n  model: !!python/name:amisc.examples.models.f1\n- name: Second component\n  model: !!python/name:amisc.examples.models.f2\n</code></pre></p> <p>Defining callable functions in YAML</p> <p>In the examples above, we defined callable Python functions using the <code>!!python/name</code> tag followed by the import path of the function. The import path must be defined in a global scope so that a Python <code>import my.model_function</code> statement is valid. For example, you might define your function in a local importable package, or simply in the current working directory (which is always searched by the Python module finder). If you had a local <code>module.py</code> file that contained the <code>my_model</code> file, then you would specify this in YAML as <code>!!python/name:module.my_model</code>. </p>"},{"location":"guides/config_file/#system","title":"System","text":"<p>The <code>System</code> surrogate object is constructed with the <code>!System</code> tag followed by a mapping of the system properties. Lists of components may be defined by nesting the <code>!Component</code> tag: <pre><code>!System\nname: My multidisciplinary system\ncomponents: !Component\n  - name: My first component\n    model: !!python/name:path.to.first_model\n    inputs: !VariableList\n      - name: x1\n      - name: x2\n    outputs: !VariableList\n      - name: y1\n      - name: y2\n  - name: My second component\n    model: !!python/name:path.to.second_model\n</code></pre></p> <p>Duplicate variables</p> <p>If multiple components take the same input variable, you only need to define the variable once in the YAML file. Then, you may simply reference the variable's <code>name</code> for any other component that uses the variable. Upon loading from file, the <code>System</code> will use the same <code>Variable</code> object for all components that reference the same variable <code>name</code>.</p>"},{"location":"guides/config_file/#loading-from-a-configuration-file","title":"Loading from a configuration file","text":"<p>The YamlLoader provides an interface for loading <code>amisc</code> objects from YAML config files: <pre><code>from amisc import YamlLoader\n\nconfig_file = \"\"\"\n!VariableList\n- name: x1\n  domain: (0, 1)\n- name: x2\n- name: x3\n\"\"\"\n\nvariables = YamlLoader.load(config_file)\n</code></pre></p> <p>The <code>load_from_file</code> and <code>save_to_file</code> convenience methods are also provided for loading and saving <code>System</code> objects to file (i.e. during surrogate training). These surrogate save files closely mirror the YAML format used for configuration -- they contain all the base properties of the surrogate as well as extra internal data for storing the state of the surrogate. These save files can be edited directly in a text editor to change or view properties of the surrogate.</p>"},{"location":"guides/extend/","title":"Extend amisc","text":"<p>This guide covers how to extend <code>amisc</code> for implementing custom surrogate methods, including compression methods, PDFs, and interpolation. We will walk through each of the 7 abstract interfaces that may be implemented by the user:</p> <ul> <li>Transform - methods for variable normalization,</li> <li>Distribution - methods for probability distribution functions,</li> <li>Compression - methods for field quantity compression,</li> <li>TrainingData - methods for training data selection (i.e. experimental design) and storage,</li> <li>Interpolator - methods for approximating the input \u2192 output mapping of a model,</li> <li>ModelKwargs - dataclass for passing extra arguments to the component models.</li> <li>FileLoader - methods for loading <code>amisc</code> objects to/from file.</li> </ul> <p>The class layout of these interfaces is summarized in the API reference.</p> <p>Important</p> <p>All custom objects implementing any of the interfaces above must also implement the <code>serialize</code> and <code>deserialize</code> mixin methods to allow saving and loading the custom objects from file. See the serialization section.</p>"},{"location":"guides/extend/#transform","title":"Transform","text":"<p>Transforms provide a method for normalizing data for surrogate training. <code>Transform</code> objects must implement the <code>transform</code> method: <pre><code>from amisc.transform import Transform\n\nclass CustomTransform(Transform):\n    \"\"\"A transform that adds 1.\"\"\"\n\n    def _transform(self, values, inverse=False):\n        return values - 1 if inverse else values + 1\n</code></pre></p> <p>The <code>transform</code> method should additionally handle the <code>inverse</code> flag to denormalize the provided values. Currently available transforms include log, linear, minmax, and z-score.</p>"},{"location":"guides/extend/#distribution","title":"Distribution","text":"<p>Distributions define a probability distribution function. <code>Distribution</code> objects should implement the <code>sample</code> and <code>pdf</code> methods: <pre><code>from amisc.distribution import Distribution\n\nclass CustomDistribution(Distribution):\n    \"\"\"A custom distribution that returns a constant value.\"\"\"\n\n    def sample(self, size=1):\n        \"\"\"Generate samples from the distribution.\"\"\"\n        return np.full(size, 42)\n\n    def pdf(self, x):\n        \"\"\"Probability density function of the distribution at `x` locations.\"\"\"\n        return np.ones_like(x)\n</code></pre></p> <p>Currently available distributions include uniform, normal, log-uniform, and log-normal.</p>"},{"location":"guides/extend/#compression","title":"Compression","text":"<p>Compression methods are responsible for compressing and reconstructing high-dimensional data to/from a compressed or \"latent\" space. Along with implementing the <code>compress</code> and <code>reconstruct</code> methods, <code>Compression</code> objects should keep track of the latent space size and implement a <code>compute_map</code> method, which generates the map to/from the latent space. The <code>Compression</code> object can also optionally provide a method for estimating the domain of the latent space. <pre><code>from amisc.compression import Compression\n\nclass CustomCompression(Compression):\n    \"\"\"A custom compression method.\"\"\"\n\n    def compute_map(self, **kwargs):\n        \"\"\"Compute and store the compression map.\"\"\"\n        self.coords = kwargs.get('coords', None)\n        self._is_computed = True\n\n    def compress(self, data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compress the data into a latent space.\"\"\"\n        return data / 2  # Example compression\n\n    def reconstruct(self, compressed: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Reconstruct the compressed data back into the full `dof` space.\"\"\"\n        return compressed * 2  # Example reconstruction\n\n    def latent_size(self) -&gt; int:\n        \"\"\"Return the size of the latent space.\"\"\"\n        return 10  # Example latent size\n\n    def estimate_latent_ranges(self) -&gt; list[tuple[float, float]]:\n        \"\"\"Estimate the range of the latent space coefficients.\"\"\"\n        return [(0.0, 1.0) for _ in range(self.latent_size())]\n</code></pre></p> <p>Only SVD compression is currently available.</p>"},{"location":"guides/extend/#training-data","title":"Training data","text":"<p>The <code>TrainingData</code> interface allows users to implement custom methods for training data selection, storage, and retrieval. This interface defines how training data is managed within the framework, including experimental design and data fidelity. Data storage and retrieval is generally guided by the \\((\\alpha, \\beta)\\) fidelity indices. See the model fidelity docs for more details.</p> <p>Custom training data methods should implement the <code>TrainingData</code> interface: <pre><code>from amisc.training import TrainingData\n\nclass CustomTrainingData(TrainingData):\n    \"\"\"A custom training data method.\"\"\"\n\n    def refine(self, alpha, beta, domains, weight_fcns):\n        \"\"\"Refine the training data based on the given (alpha, beta) fidelity and domains.\"\"\"\n        # Implement refinement (experimental design) logic here\n        pass\n\n    def set(self, alpha, beta, idx, data):\n        \"\"\"Set the training data for the given indices and (alpha, beta) fidelity.\"\"\"\n        # Implement data storage logic here\n        pass\n\n    def get(self, alpha, beta):\n        \"\"\"Retrieve training data by (alpha, beta) fidelity.\"\"\"\n        # Implement data retrieval logic here\n        pass\n\n    def clear(self):\n        \"\"\"Clear all data.\"\"\"\n        pass\n\n    def set_errors(self, alpha, beta, idx, errors):\n        \"\"\"Optionally store error information for failed model evaluations.\"\"\"\n        pass\n\n    def impute_missing_data(self, alpha, beta):\n        \"\"\"Optionally impute training data for failed model evaluations.\"\"\"\n        pass\n</code></pre></p> <p>Currently, the only available training data method is the <code>SparseGrid</code>. This method stores data by its location in a larger tensor-product grid of the input dimensions, which interfaces naturally with the MISC framework. The experimental design is guided by the space-filling Leja objective function. See SparseGrid for more details.</p>"},{"location":"guides/extend/#interpolator","title":"Interpolator","text":"<p>Interpolators approximate the input \u2192 output mapping of a model given a set of training data. <code>Interpolator</code> objects must implement the <code>refine</code> and <code>predict</code> methods:</p> <pre><code>from amisc.interpolator import Interpolator, InterpolatorState\n\nclass CustomInterpolator(Interpolator):\n    \"\"\"A custom interpolator.\"\"\"\n\n    def refine(self, beta, training_data, old_state, domains) -&gt; InterpolatorState:\n        \"\"\"Refine the old interpolator state with new training data for a given beta fidelity index.\"\"\"\n        # Implement refinement logic here\n        pass\n\n    def predict(self, x, state, training_data):\n        \"\"\"Predict the output for given input `x` using the interpolator state and training data.\"\"\"\n        # Implement prediction logic here\n        pass\n</code></pre> <p>The <code>predict</code> method computes the interpolator approximation at a new set of points <code>x</code> given an interpolator state and set of training data. The <code>refine</code> method takes new training data and updates the old interpolator state.</p> <p>The <code>InterpolatorState</code> is an interface for storing the internal state (e.g. weights and biases) of the interpolator method, and is used for computing the interpolator approximation along with a set of training data. If you implement the <code>Interpolator</code> interface, you should also implement a corresponding <code>InterpolatorState</code> that integrates with your custom interpolator: <pre><code>from amisc.interpolator import InterpolatorState\n\nclass CustomInterpolatorState(InterpolatorState):\n    \"\"\"The internal state for a custom interpolator.\"\"\"\n    # Define additional state variables here\n    pass\n</code></pre></p> <p>Currently, the available methods are <code>Lagrange</code> polynomial interpolation and <code>Linear</code> regression. The state of a <code>Lagrange</code> polynomial includes the 1d grids and barycentric weights for each input dimension. The state of a <code>Linear</code> regression includes the underlying scikit-learn linear model. See Lagrange and Linear for more details. Note that linear regression also includes options for polynomial features.</p>"},{"location":"guides/extend/#model-keyword-arguments","title":"Model keyword arguments","text":"<p>The <code>ModelKwarg</code> interface provides a dataclass for passing extra options to the underlying component models. The default is a simple <code>dict</code> that gets passed as a set of <code>key=value</code> pairs. The primary reason for overriding this class is if you have complicated arguments that require custom serialization. See the serialization section below.</p>"},{"location":"guides/extend/#serialization","title":"Serialization","text":"<p>The Serializable interface defines the <code>serialize</code> and <code>deserialize</code> methods for converting <code>amisc</code> objects to/from built-in Python types (such as <code>strings</code>, <code>floats</code>, and <code>dicts</code>).</p> <p>Important</p> <p>All custom objects implementing any of the interfaces above must also implement the <code>serialize</code> and <code>deserialize</code> mixin methods to allow saving and loading the custom objects from file.</p> <p>Note that the <code>Variable</code>, <code>Component</code>, and <code>System</code> classes themselves implement the <code>Serializable</code> interface, so that they too can be easily saved and loaded from file. In the context of <code>amisc</code>, \"serialization\" means \"convert to built-in Python types\" (and vice versa for deserialization). This typically means converting objects to dictionaries with <code>key=value</code> pairs for all of the object's properties. Several serialization methods are provided in the <code>amisc.serialize</code> module. These can be mixed-in to your custom objects by inheritance.</p> <p>Mixin serialization methods</p> <p>Our custom interpolator state can be serialized to base64 encoding by inheriting from the Base64Serializable mixin class: <pre><code>class CustomInterpolatorState(InterpolatorState, Base64Serializable):\n    \"\"\"The internal state of a custom interpolator.\"\"\"\n    # Define additional state variables here\n    pass\n</code></pre></p>"},{"location":"guides/extend/#fileloader","title":"FileLoader","text":"<p>The <code>FileLoader</code> interface defines the <code>dump</code> and <code>load</code> methods for saving and loading <code>amisc</code> objects to/from file. The recommended way to save objects to file is the provided YamlLoader class, which uses the registered <code>!Variable</code> (etc.) tags to construct <code>amisc</code> objects directly from <code>.yml</code> config files.</p> <p>However, more generally, you may define a custom <code>FileLoader</code> if you prefer to work with different file types. For example, you can make use of the fact that all <code>amisc</code> objects are serializable to/from built-in Python types to more easily transcribe objects to a new file format (built-in Python types like <code>strings</code> and <code>dicts</code> readily convert to most file types).</p> <p>A custom JSON file loader</p> <p>This example shows saving and loading a System object from a JSON file (using the <code>serialize</code> and <code>deserialize</code> methods). More generally, you would also want to handle <code>Variables</code> and <code>Components</code>. <pre><code>from amisc import FileLoader\nimport json\n\nclass JSONLoader(FileLoader):\n    \"\"\"Save and load amisc objects from JSON\"\"\"\n\n    def load(self, file)\n        \"\"\"Load an amisc.System object (for example)\"\"\"\n        with open(file, 'r') as fd:\n            data = json.load(fd)\n        return System.deserialize(data)\n\n    def dump(self, obj, file):\n        \"\"\"Dump an amisc.System object (for example)\"\"\"\n        with open(file, 'w') as fd:\n            json.dump(obj.serialize(), fd)\n</code></pre></p>"},{"location":"guides/interface_models/","title":"Interface external models","text":"<p>This guide will cover how to plug external models into the <code>amisc</code> framework using the <code>Component</code> class.</p>"},{"location":"guides/interface_models/#the-component-class","title":"The <code>Component</code> class","text":"<p>The <code>Component</code> class is a black-box wrapper around a single discipline model. For example, if you had a system composed of a compressor, combuster, and turbine for modeling a turbojet engine, then each of the three disciplines would be wrapped in their own <code>Component</code> object. Just like the underlying model, a <code>Component</code> needs a callable function, a set of input variables, and a set of output variables: <pre><code>from amisc import Component\n\ndef model(x):\n    y = x ** 2\n    return y\n\ncomponent = Component(model  = model,\n                      inputs = 'x',\n                      outputs= 'y')\n</code></pre></p> <p>For simple functions, the inputs and outputs can be inferred from the function signature: <pre><code>def my_model(x1, x2, x3):\n    y1 = x1 + x2\n    y2 = x2 + x3\n    return y1, y2\n\ncomponent = Component(my_model)  # inputs and outputs are inferred\n\nassert component.inputs == ['x1', 'x2', 'x3']\nassert component.outputs == ['y1', 'y2']\n</code></pre></p> <p>More generally, a list of strings or <code>Variable</code> objects can be passed as <code>Component(my_model, inputs=[...], outputs=[...])</code>. If the model needs extra configurations or arguments, they may be passed as keyword arguments:</p> <pre><code>def my_model(inputs, extra_config=None):\n    # Etc.\n    return outputs\n\ncomponent = Component(my_model, inputs, outputs, extra_config='My config value')\n</code></pre>"},{"location":"guides/interface_models/#python-wrapper-functions","title":"Python wrapper functions","text":"<p>The <code>Component.model</code> attribute is where the user can link their external model into the <code>amisc</code> framework; we call these \"Python wrapper functions\". The model must be written as a simple callable function in Python; however, it may make any external calls it needs to run simulation software that may exist outside of Python, i.e. such as a binary executable on the local machine. Several examples are provided in the tutorial and elsewhere in the documentation.</p>"},{"location":"guides/interface_models/#expected-format","title":"Expected format","text":"<p>A wrapper function may be written in one of the two following formats:</p> Packed inputs/outputsUnpacked inputs/outputs <p>The function accepts a single <code>dict</code> positional argument, from which all variables may be accessed by key. Similarly, the model returns a single <code>dict</code> from which all output variables may be accessed by key. <pre><code>def my_model(input_dict):\n    x1 = input_dict['x1']\n    x2 = input_dict['x2']\n    x3 = input_dict['x3']\n\n    # Compute model\n\n    output_dict = {}\n    output_dict['y1'] = ...\n    output_dict['y2'] = ...\n\n    return output_dict\n</code></pre></p> <p>This format is the default and should typically be preferred (since you don't have to worry about variable ordering).</p> <p>The function accepts several positional arguments, one for each input variable. Similarly, the model returns all output variables in a tuple. This format may be preferred as it makes the call and return signatures more explicit; however, you must keep track of the ordering of arguments and return values. <pre><code>def my_model(x1, x2, x3):\n    # Compute model\n\n    y1 = ...\n    y2 = ...\n\n    return y1, y2\n</code></pre></p> <p>This format is used when the inputs/outputs are inferred from the function signature (i.e. not provided directly to <code>Component()</code>). You may manually set the <code>Component(call_unpacked=True, ret_unpacked=True)</code> attributes if you prefer this format. Note that it is then up to you to keep track of ordering if you later decide to change the component's inputs or outputs.</p> <p>It is also possible to accept \"packed\" inputs but return \"unpacked\" outputs, or vice versa by specifying the <code>call_unpacked</code> and <code>ret_unpacked</code> attributes of <code>Component</code>.</p> <p>Important note</p> <p>We want to emphasize again that external models do not have to be written in Python to be used with <code>amisc</code>. The user needs only a basic familiarity with Python to write a wrapper function in one of the above formats. There are many built-in and third-party libraries that enable Python to integrate well with many external ecosystems (e.g. <code>juliacall</code> for Julia programs, <code>matlab.engine</code> for MATLAB, <code>PyAnsys</code> for ANSYS, <code>requests</code> for web-based resources, and the very general built-in <code>subprocess.run</code> for executing arbitrary code on the local machine.) The availability of these resources makes Python the ideal \"glue\" language for integrating models from a variety of platforms.</p> <p>Using a global scope</p> <p>Always specify the model at a global scope, i.e. don't use <code>lambda</code> or nested functions. When saving to file, only a symbolic reference to the function signature will be saved, which must be globally defined (i.e. importable) when loading back from that save file.</p>"},{"location":"guides/interface_models/#calling-the-model","title":"Calling the model","text":"<p>Regardless of how you prefer to format your model wrapper function, the <code>Component</code> class provides a consistent interface for calling the model function through the <code>Component.call_model()</code> function. As shown in the surrogate guide, the surrogate always expects to be called in the \"packed\" format, i.e. by passing a <code>dict</code> whose keys are the input variables and whose values are the numeric values of those inputs (see the Dataset class). For example: <pre><code>from amisc import Component\n\ncomp = Component(...)\n\ninputs = {'x1': 1.0, 'x2': 5.5}\n\noutputs_surr  = comp.predict(inputs)                    # prediction with the surrogate\noutputs_model = comp.call_model(inputs)                 # prediction with the underlying model\n\noutputs_model = comp.predict(inputs, use_model='best')  # alias for call_model\n</code></pre> As you can see, the <code>call_model</code> has the same signature as the <code>predict</code> function. We also provide the <code>use_model</code> alias within <code>predict</code> so that both surrogate and ground truth model predictions may be obtained using the exact same signature, regardless of how the wrapper function is written. The <code>call_model</code> interface also provides some convenience options for parallel and vectorized execution, which we cover later on in this guide.</p>"},{"location":"guides/interface_models/#special-model-arguments","title":"Special model arguments","text":"<p>Apart from providing a consistent interface for calling the model, the <code>call_model</code> function will also inspect your wrapper function for special keywords and pass extra data to your function as requested. There are 5 special keyword arguments that your wrapper function may request:</p> <ul> <li><code>model_fidelity</code> - will provide a set of fidelity indices to tune which physical fidelity of the model should be used (see the model fidelity section for details),</li> <li><code>input_vars</code> - will provide the actual <code>Variable</code> objects from <code>Component.inputs</code> in case your model needs extra details about the inputs,</li> <li><code>output_vars</code> - will similarly provide the <code>Variable</code> objects from <code>Component.outputs</code>,</li> <li><code>output_path</code> - will give your function a <code>Path</code> to a directory where it can write model outputs to file (e.g. if you have lots of extra save data you want to keep). This would be useful if you want to keep the context of each model evaluation with respect to the <code>amisc</code> surrogate, so that you might later construct a new surrogate for outputs you saved to file. This is similar to the <code>tmp_path</code> within the <code>pytest</code> framework.</li> <li><code>{{ var }}_coords</code> - for a given \"var\" name, this will pass the compression grid coordinates for a given field quantity input variable.</li> </ul> <p>You may additionally pass any extra keyword arguments on to the model directly through <code>call_model(inputs, **kwargs)</code>.</p> <p>Example</p> <pre><code>def my_model(inputs, model_fidelity=(0,), output_path=None, input_vars=None, x_coords=None, config=None):\n    x = inputs['x']                           # get the input values for 'x'\n\n    if x_coords is None:                      # read compression grid coordinates for the 'x' field quantity\n        x_coords = np.linspace(0, 1, 100)     # default\n\n    delta_t = 1e-8 / (model_fidelity[0] + 1)  # set the time step based on model fidelity level\n\n    if x &lt; 0:\n        x = input_vars['x'].get_nominal()     # set a threshold for x using info from the `Variable` objects\n\n    # Compute model\n    y = ...\n\n    if output_path is not None:               # write outputs to file\n        with open(output_path / 'my_outputs.txt', 'w') as fd:\n            fd.writelines(f'y = {y}')\n\n    return {'y': y, 'output_path': 'my_outputs.txt'}\n\ncomp = Component(my_model, inputs=Variable('x', nominal=1), outputs=Variable('y'))\npred = comp.call_model({'x': 0.5}, model_fidelity=(1,), config={'extra': 'configs'})\n</code></pre>"},{"location":"guides/interface_models/#special-model-return-values","title":"Special model return values","text":"<p>In addition to special keyword arguments, your wrapper function may also return special values. There are 4 values that may be returned by the wrapper function:</p> <ul> <li><code>{{ var }}</code> - for a given output \"var\" name -- these are just the usual numeric return values that match the variables in <code>Component.outputs</code>,</li> <li><code>{{ var_coords }}</code> - for a given field quantity output \"var\" name -- these are the grid coordinates corresponding to the locations of the \"var\" output field quantity (returned shape is \\((N, D)\\) where \\(N\\) is the number of points and \\(D\\) is the number of dimensions),</li> <li><code>model_cost</code> - a best estimate for total model evaluation time (in seconds of CPU time). For example, if your model makes continuous use of 16 CPUs and takes 1 hour to evaluate one set of inputs, then you would return <code>16 * 3600</code> as the model cost. This metric is used by <code>amisc</code> to help evaluate which are the most \"effective\" model evaluations, i.e. more expensive models are penalized greater than cheaper models.</li> <li><code>output_path</code> - if your function requested <code>output_path</code> and subsequently wrote some data to file, then you can return the new file name as <code>output_path</code> to keep track of the file's location within the <code>Component's</code> internal data storage.</li> </ul> <p>Your wrapper function may also return any extra data that you wish to store within the <code>Component</code> \u2014 <code>amisc</code> will keep track of this data but will not use it for any purpose. You might do this if you wish to return to old model evaluations later on and view data even if you did not explicitly build a surrogate for the data. This extra data will be stored in <code>numpy</code> object arrays, so the data does not have to be strictly numeric type \u2014 it could even be custom object types of your own. Field quantities will also be stored in <code>numpy</code> object arrays.</p> <p>Example</p> <pre><code>def my_model(inputs, output_path=None):\n    t1 = time.time()    # time the model evaluation\n    # Compute the model\n    y1 = ...\n    y2 = ...\n\n    t2 = time.time()\n\n    if output_path is not None:\n        with open(output_path / 'my_output.txt', 'w') as fd:\n            fd.writelines(f'I want to store some output data via file here')\n\n    extra_data = {'store': 'extra', 'data': 'here'}\n\n    return {'y1': y1, 'y2': y2, 'model_cost': t2 - t1, 'output_path': 'my_output.txt', **extra_data}\n</code></pre>"},{"location":"guides/interface_models/#handling-exceptions","title":"Handling exceptions","text":"<p>If your wrapper function raises an exception, this will be caught and handled by <code>Component.call_model()</code> \u2014 an <code>errors</code> dictionary will be returned whose keys are the indices of the input samples that failed and whose values contain traceback information. If an exception is raised by a component during <code>System.predict()</code>, then an <code>errors</code> array will be returned that matches the shape of all other output arrays \u2014 only indices in the array where an exception was raised will contain traceback information. By default, <code>amisc</code> will fill all other output arrays with <code>nan</code> for failed samples.</p>"},{"location":"guides/interface_models/#model-fidelities","title":"Model fidelities","text":"<p>As we saw in the special arguments section, your wrapper function may request the <code>model_fidelity</code> keyword argument. This will pass a tuple of integers (what we call a multi-index) to your function, for which you can use to \"tune\" the fidelity level of your model. For example, if you build a <code>Component</code> and specify a tuple of two integers: <code>Component(..., model_fidelity=(3, 2))</code>, then the \"maximum\" fidelity of your model is fully specified by the numbers \\((3, 2)\\). You can use these integers however you want to adjust model fidelity, e.g. <code>num_grid_pts = 100 * (model_fidelity[0] + 1)</code> would use the first integer to tune how many grid points are used in a simulation, with higher integers corresponding to more grid points, and therefore higher numerical accuracy. Your wrapper function should then request <code>model_fidelity</code> and use the multi-index as desired: <pre><code>def my_model(inputs, model_fidelity=(0, 0)):\n    num_grid_pts = 100 * (model_fidelity[0] + 1)      # controls number of grid points\n    time_step = 1e-8 * (1 / (model_fidelity[1] + 1))  # controls time step size\n\n    # etc.\n\n    return outputs\n</code></pre> During surrogate training, your wrapper function will then be passed tuples of varying model fidelity (from \\((0, 0)\\) up to \\((3, 2)\\) in the example). If you do not request <code>model_fidelity</code>, then your wrapper function will not be passed any fidelity indices, and will instead be treated as a single fidelity model.</p> <p>Apart from <code>model_fidelity</code>, you may also specify similar tuples of integers for <code>data_fidelity</code> and <code>surrogate_fidelity</code>, which correspond to fidelity levels for training data and the surrogate method, respectively. These indices are used internally by <code>amisc</code> during training and are not passed to your wrapper function like <code>model_fidelity</code>. Instead, together they tune the amount of training data collected and the refinement level of the surrogate method (e.g. number of hidden layers, weights, etc.) As such, their usage is dependent on the specific underlying methods for training data collection and surrogate evaluation.</p> <p>The tutorial provides more detail on how to interpret and use model fidelities. For the purposes of this guide, we summarize the fidelity options for the <code>Component</code> class in the example below: <pre><code>comp = Component(..., model_fidelity=(3, 2),       # the maximum model fidelity indices\n                      data_fidelity=(2, 3, 1),     # the maximum training data fidelity indices\n                      surrogate_fidelity=())       # the maximum surrogate fidelity indices\n</code></pre></p> <p>By default, the only training data method available is the <code>SparseGrid</code> -- the sparse grid method uses one index in <code>data_fidelity</code> per input, so that <code>len(data_fidelity) == len(inputs)</code>. Each index determines the maximum training data allowed along each input dimension. For a <code>data_fidelity</code> of \\(\\beta\\) for example, each input \\(x_i\\) for \\(i=0\\dots N\\) has a maximum number of training points of \\(2\\beta_i + 1\\), which for \\(\\beta = (2, 3, 1)\\) means a max grid size of \\((4, 7, 3)\\). These options are configurable, and more information can be found at SparseGrid.</p> <p>By default, the only surrogate method available is the <code>Lagrange</code> -- Lagrange polynomial interpolation does not use any method for tuning its own fidelity, so <code>surrogate_fidelity</code> may be left empty. See more details at Lagrange.</p> <p>We frequently refer to <code>model_fidelity</code> by the symbol \"\\(\\alpha\\)\", and to the combination of <code>data_fidelity</code> and <code>surrogate_fidelity</code> as \"\\(\\beta\\)\". For a given pair of \\((\\alpha, \\beta)\\), the <code>Component</code> surrogate can be trained by \"activating\" the index pair via: <pre><code>comp = Component(my_model, inputs, outputs, model_fidelity=..., data_fidelity=..., surrogate_fidelity=...)\n\nalpha, beta = (2, 1), (3, 2)      # for example\ncomp.activate_index(alpha, beta)  # 'train' the surrogate for this index pair\n</code></pre></p> <p>By calling <code>activate_index</code>, we will move \\((\\alpha, \\beta)\\) into the component's \"active\" index set, and then search for and compute model evaluations for all the \"neighboring\" index pairs. For most users, this will be done automatically on a call to <code>System.fit()</code>, but we provide this description here for completeness. To get a better understanding of \"active\" and \"neighboring\" index sets, please see the theory section.</p>"},{"location":"guides/interface_models/#storing-and-retrieving-training-data","title":"Storing and retrieving training data","text":"<p>The <code>Component.training_data</code> attribute provides the method by which training data is sampled (i.e. experimental design), stored, and retrieved. Currently, the only available method is the SparseGrid, which is set by default. For the most part, all aspects of training data manipulation is handled behind the scenes by the <code>Component</code> class. You may configure the training data method via: <pre><code>from amisc.training import SparseGrid\n\ncomp = Component(..., training_data=SparseGrid(collocation_rule='leja'))\n</code></pre></p> <p>New training data methods may be created by implementing the <code>amisc.training.TrainingData</code> interface. Data storage and retrieval is generally guided by the \\((\\alpha, \\beta)\\) fidelity indices described in the fidelity section. The <code>Component</code> class provides the <code>get_training_data</code> method to extract all input/output data associated with a given \\((\\alpha, \\beta)\\) pair: <pre><code>alpha, beta = (2, 1), (3, 2)      # for example\nxtrain, ytrain = component.get_training_data(alpha, beta)\n</code></pre></p> <p>By default <code>get_training_data</code> will return the training data associated with the highest-fidelity pair of \\((\\alpha, \\beta)\\). After fitting a surrogate, the user may wish to extract the training data for each fidelity level and attempt construction of other surrogates to compare performance; this is made possible by the <code>get_training_data</code> interface.</p> <p>As one last note, if your model returns field quantity data, this will be stored along with its compressed latent space representation. By default, both representations of the field quantity are returned by <code>get_training_data</code>.</p>"},{"location":"guides/interface_models/#specifying-a-surrogate-method","title":"Specifying a surrogate method","text":"<p>The <code>Component.interpolator</code> attribute provides the underlying surrogate \"interpolation\" method, i.e. the specific mathematical relationship that approximates the model's outputs as a function of its inputs. In this sense, we use the terms \"interpolator\" and \"surrogate\" interchangeably to mean the underlying approximation method -- the <code>Component.interpolator</code> does not necessarily have to \"interpolate\" the output by passing through all the training data directly. The naming convention mostly arises from the usage of polynomial interpolation in sparse grids.</p> <p>Currently, the available methods are Lagrange polynomial interpolation, which is set by default, and Linear regression. Multivariate Lagrange polynomials are formed by a tensor-product of univariate Lagrange polynomials in each input dimension, and integrate well with the <code>SparseGrid</code> data structure. Lagrange polynomials work well up to an input dimension of around 12-15 for sufficiently smooth functions. More details on how they work can be found in the theory section. Linear regression is implemented through the scikit-learn library, and may optionally include polynomial features.</p> <p>You may configure the interpolation method via: <pre><code>from amisc.interpolator import Lagrange\n\ncomp = Component(..., interpolator=Lagrange())\n</code></pre></p> <p>New interpolation/surrogate methods may be created by implementing the <code>amisc.interpolator.Interpolator</code> interface.</p>"},{"location":"guides/interface_models/#parallelize-the-model","title":"Parallelize the model","text":"<p>A large advantage of the <code>Component.call_model()</code> interface is the ability to evaluate multiple input samples with the model in parallel. If your model wrapper function only processes one set of inputs at a time (i.e. only single values in the <code>inputs</code> dictionary), then calling the model through <code>call_model</code> will automatically allow handling arrays of inputs. For example:</p> Single inputArrayed inputs <pre><code>def my_model(inputs):\n    x1 = inputs['x1']       # single float\n    x2 = inputs['x2']       # single float\n\n    return {'y1': x1 * x2}  # single float\n\noutput = my_model({ 'x1': 0.5, 'x2': 1.5 })\n\n# will give { 'y1': 0.75 }\n</code></pre> <pre><code>def my_model(inputs):\n    x1 = inputs['x1']       # single float\n    x2 = inputs['x2']       # single float\n\n    return {'y1': x1 * x2}  # single float\n\ncomp = Component(my_model, ['x1', 'x2'], 'y1')\noutput = comp.call_model({ 'x1': np.random.rand(100), 'x2': np.random.rand(100) })\n\n# will give { 'y1': np.array(shape=100) }\n</code></pre> <p>By default, this will run the input samples in a serial loop, which will not provide any speedup, but does allow your single-input wrapper function to be used with arrays of inputs.</p> <p>To parallelize the model over the input arrays, you may pass an instance of an <code>Executor</code> to <code>call_model</code> (i.e. something that implements the built-in Python <code>concurrent.futures.Executor</code> interface -- Python itself provides the <code>ProcessPoolExecutor</code> and <code>ThreadPoolExecutor</code> which use parallel process- or thread-based workers). The popular <code>mpi4py</code> library also provides an <code>MPIExecutor</code> to distribute parallel tasks over MPI-enabled workers. You may similarly pass an instance of <code>Executor</code> to any functions where you wish to parallelize model evaluations (such as <code>System.fit()</code> for training the surrogate).</p> <p>Finally, if your wrapper function can handle arrayed inputs on its own, then you may set <code>Component.vectorized=True</code>. Input dictionaries will then be passed to your wrapper function with <code>np.ndarrays</code> as values for each input variable rather than scalar <code>floats</code>. For example, you can take advantage of <code>numpy</code> vectorization to directly manipulate two arrays of inputs rather than looping over each element (e.g. <code>y = x1 * x2</code> rather than <code>y = [x1[i] * x2[i] for i in range(N)]</code>).</p> <p>If you do use <code>vectorized=True</code> and you're also using <code>model_fidelity</code>, you should expect <code>model_fidelity</code> as an <code>np.ndarray</code> of shape <code>(N, R)</code>, where <code>N</code> is the loop shape of the inputs and <code>R</code> is the number of fidelity indices; usually you would just expect a tuple of length <code>R</code> when <code>vectorized=False</code>.</p> SerialParallelVectorized <pre><code>def my_model(inputs):\n    x1 = inputs['x1']   # float\n    x2 = inputs['x2']   # float\n    return { 'y1': x1 * x2 }\n\ncomp = Component(my_model, ['x1', 'x2'], 'y1')\n\nin  = { 'x1': np.random.rand(100), 'x2': np.random.rand(100) }\nout = comp.call_model(in)\n</code></pre> <pre><code>def my_model(inputs):\n    x1 = inputs['x1']   # float\n    x2 = inputs['x2']   # float\n    return { 'y1': x1 * x2 }\n\ncomp = Component(my_model, ['x1', 'x2'], 'y1')\n\nwith concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n    in  = { 'x1': np.random.rand(100), 'x2': np.random.rand(100) }\n    out = comp.call_model(in, executor=executor)\n</code></pre> <pre><code>def my_model(inputs):\n    x1 = inputs['x1']   # np.ndarray (100,)\n    x2 = inputs['x2']   # np.ndarray (100,)\n    return { 'y1': x1 * x2 }\n\ncomp = Component(my_model, ['x1', 'x2'], 'y1', vectorized=True)\n\nin  = { 'x1': np.random.rand(100), 'x2': np.random.rand(100) }\nout = comp.call_model(in)\n</code></pre> <p>Generally, the surrogate predictions (e.g. <code>Component.predict</code>) make use of <code>numpy</code> vectorization to handle arrayed inputs, so <code>call_model</code> again acts as a consistent interface for calling the underlying model with the same signature as the surrogate.</p> <p>Loop dimensions</p> <p>If you pass a scalar input array of arbitrary shape (say \\((100, 2, 5)\\) for example), <code>Component.call_model()</code> treats all axes in the array as \"loop dimensions\". So effectively, all the axes will be flattened to a single 1d array of length <code>N = np.prod(input.shape)</code>, which would be \\(N=1000\\) for an input shape of \\((100, 2, 5)\\). Then, <code>call_model</code> will loop over \\(N\\), passing each input sample to the underlying model (unless <code>vectorized=True</code>). The outputs will be reshaped back to the original input \"loop\" shape before returning. If you do pass an input array for one variable, you should pass the same shape array for all other inputs, or at least an array that is broadcastable to the full loop shape. See <code>numpy</code> broadcasting rules.</p> <p>The situation is a bit more complicated for non-scalar inputs (i.e. field quantities). You may still pass arrays of field quantities as inputs, so that they are looped over in the same fashion as scalar inputs. However, which axes are a \"loop shape\" vs. what axes are part of the field quantity's shape itself must be inferred or indicated. Generally, as long as you provide at least one scalar input, then the loop shape is known and the first leading axes of the field quantity that match the loop shape will be used as such, while the rest are assumed to be the field quantity's own shape. For example: <pre><code>x = np.random.rand(100, 2)      # scalar\nf = np.random.rand(100, 2, 20)  # field quantity with shape (20,)\n</code></pre></p> <p>Clearly, the loop shape is (100, 2) and we can infer the field quantity shape of (20,). If the field quantity were provided by itself, then you should also provide <code>f_coords</code> to <code>call_model()</code> so that the field quantity shape for the <code>f</code> variable can be obtained from the shape of its grid coordinates. See the field quantity documentation for more details.</p>"},{"location":"guides/interface_models/#component-configuration-summary","title":"<code>Component</code> configuration summary","text":"<p>Here is a summary of all configurations that may be passed to <code>Component</code> by example: <pre><code>def my_model(inputs, model_fidelity=(0, 0), **extra_kwargs):\n    x1 = inputs['x1']\n    x2 = inputs['x2']\n    x3 = inputs['x3']\n\n    return {'y1': ..., 'y2': ...}\n\ncomp = Component(model=my_model,                # The Python wrapper function of the component model\n                 inputs=['x1', 'x2', 'x3'],     # List of input variables (str or Variable objects)\n                 outputs=['y1', 'y2'],          # List of output variables (str or Variable objects)\n                 model_fidelity=(2, 3),         # Physical model fidelity multi-index\n                 data_fidelity=(2, 2, 1),       # Data fidelity multi-index (length matches len(inputs) for SparseGrid)\n                 surrogate_fidelity=(),         # Surrogate fidelity multi-index (empty for Lagrange)\n                 training_data=SparseGrid(),    # Collocation/design and storage method for training data\n                 interpolator=Lagrange(),       # Underlying surrogate/interpolation method\n                 vectorized=True,               # If wrapper function handles arrayed inputs\n                 call_unpacked=False,           # Call signature of   `func(x1, x2, ...)`  if True\n                 ret_unpacked=False,            # Return signature of `return y1, y2, ...` if True\n                 name='My component',           # ID for log messages and referencing\n                 **extra_kwargs)                # Extra keyword arguments to pass to wrapper function\n</code></pre></p> <p>A few important things to keep in mind:</p> <ul> <li>The first three arguments (<code>model</code>, <code>inputs</code>, <code>outputs</code>) must always be specified (or at least inferred for inputs/outputs) and can be passed as positional arguments.</li> <li>If all of the <code>fidelity</code> options are left empty, then no surrogate will be built.</li> <li>The wrapper function must request <code>model_fidelity</code> as a keyword argument if you specify it in the <code>Component</code>.</li> <li>Data and surrogate fidelity usage are dependent on <code>training_data</code> and <code>interpolator</code>, respectively. The length of <code>data_fidelity</code> should match the number of inputs for <code>SparseGrid</code> training data.</li> <li>Setting <code>vectorized=True</code> means the wrapper function should handle arrayed inputs (as well as arrayed <code>model_fidelity</code> if applicable).</li> <li>The \"packed\" call and return signatures are preferred (i.e. the default is <code>call_unpacked=False</code> and <code>ret_unpacked=False</code>) -- the wrapper function should receive and return dictionaries, where the keys are the variables with their corresponding values.</li> <li>The wrapper function should additionally return the special variables <code>{{ var }}_coords</code> to provide the grid coordinates for field quantity outputs.</li> </ul>"},{"location":"guides/interface_models/#viewing-internal-data","title":"Viewing internal data","text":"<p>There are a few use cases for wanting to manipulate the internal data structures of a <code>Component</code>, so for completeness, we summarize the data structures here. Components store data with respect to the \\((\\alpha, \\beta)\\) fidelity indices; see the model fidelity section for more details.</p>"},{"location":"guides/interface_models/#index-sets","title":"Index sets","text":"<p>An index set is a set of concatenated multi-indices \\((\\alpha, \\beta)\\). For example, \\(\\mathcal{I}=\\{ ((0,), (0,)), ((0,), (1,)), ((1,), (0,)), ((1,), (1,)) \\}\\) is the set of all combinations of \\(\\alpha\\) and \\(\\beta\\) ranging from 0 to 1.</p> <p>A <code>Component</code> has two index sets: an <code>active_set</code> and a <code>candidate_set</code>. The active set is the set of fidelity levels that are currently being used in the surrogate approximation. The candidate set is all fidelity levels that are forward neighbors of the active set, e.g. \\((1, 0)\\) and \\((0, 1)\\) are forward neighbors of \\((0, 0)\\). While training the surrogate, only the active set is used for prediction. After training, both active and candidate sets are used together for prediction.</p>"},{"location":"guides/interface_models/#misc-trees","title":"MISC trees","text":"<p>For every \\((\\alpha, \\beta)\\) multi-index pair, the component stores four items in a tree-like structure:</p> <ul> <li>interpolator state - the state of the underlying interpolator (e.g. weights, biases, coefficients, etc.). Since a single \\((\\alpha, \\beta)\\) pair corresponds to a single set of training data, the interpolator state is the corresponding surrogate trained on this data.</li> <li>cost - the computational expense incurred by adding \\((\\alpha, \\beta)\\) to the surrogate approximation, i.e. the cost of evaluating the model on the new training data.</li> <li>train coefficients - the combination technique coefficients for the \\((\\alpha, \\beta)\\) surrogate under the active index set (see theory for more details).</li> <li>test coefficients - the combination technique coefficients for the \\((\\alpha, \\beta)\\) surrogate under both the active and candidate index sets (see theory for more details).</li> </ul> <p>Additionally, the computational expense (seconds of CPU time for one evaluation) of each \\(\\alpha\\) fidelity level is stored. The internal data structures of the <code>Component</code> are summarized below: <pre><code>comp = Component(...,\n                 active_set=IndexSet(...),          # \"active\" index set during training\n                 candidate_set=IndexSet(...),       # \"candidate\" index set (forward neighbors of the active set)\n                 misc_states=MiscTree(...),         # interpolator states (weights, biases, etc.)\n                 misc_costs=MiscTree(...),          # cost incurred to add each multi-index pair\n                 misc_coeff_train=MiscTree(...),    # combination technique coeffs (active)\n                 misc_coeff_test=MiscTree(...),     # combination technique coeffs (active+candidate)\n                 model_costs=dict(...))             # computational expense of each model fidelity level\n</code></pre></p>"},{"location":"guides/interface_models/#overriding-the-data-structures","title":"Overriding the data structures","text":"<p>All of the <code>Component</code> data structures are managed internally and do not need to be set by the user. However, the <code>Component.predict()</code> function accepts an <code>index_set</code> and a <code>misc_coeff</code> argument so that the user may predict with the surrogate using an alternate set of training data. In this case, the user would maintain their own <code>IndexSet</code> and <code>MiscTree</code> structures outside the <code>Component</code> and pass them as <code>Component.predict(..., index_set=index_set, misc_coeff=misc_coeff)</code>.</p> <p>For example, during surrogate training via <code>System.fit()</code>, the component surrogate is called with a modified index set while adaptively searching for the next best \\((\\alpha, \\beta)\\) pair to add to the surrogate approximation. See the System.refine documentation.</p>"},{"location":"guides/train_surrogate/","title":"Train a surrogate","text":"<p>This guide will cover how to link models together and train a surrogate in <code>amisc</code>.</p>"},{"location":"guides/train_surrogate/#define-a-multidisciplinary-system","title":"Define a multidisciplinary system","text":"<p>The primary object for surrogate construction is the <code>System</code>. A <code>System</code> is constructed by passing all the component models: <pre><code>from amisc import System\n\ndef first_model(x1, x2):\n    y1 = x1 * x2\n    return y1\n\ndef second_model(y1, x3):\n    y2 = y1 ** 2 + x3\n    return y2\n\nsystem = System(first_model, second_model)\n</code></pre></p> <p>More generally, you may pass the <code>Component</code> wrapper objects themselves with extra configurations to the system: <pre><code>from amisc import Component\n\nsystem = System(Component(first_model, data_fidelity=(2, 2), ...),\n                Component(second_model, data_fidelity=(3, 2), ...))\n</code></pre></p> <p>The <code>System</code> may also accept only a single component model as a limiting case. An MD system is compactly summarized by a directed graph data structure, with the nodes being the component models and the edges being the coupling variables passing between the components. You may view the system graph using <code>networkx</code> via: <pre><code>import networkx as nx\n\nnx.draw(system.graph())\n</code></pre></p> <p>If you want to save a variety of surrogate build products and logs, set the <code>root_dir</code> attribute: <pre><code>system = System(..., root_dir='.')\n\nsystem.root_dir = '/somewhere/else'  # alternatively\n</code></pre></p> <p>This will create a new <code>amisc_{timestamp}</code> save directory with the current timestamp under the specified directory, where all build products and save files will be written. The structure of the <code>amisc</code> root directory is summarized below: <pre><code>\ud83d\udcc1 amisc_2024-12-10T11.00.00/\n\u251c\u2500\u2500 \ud83d\udcc1 components/                     # folder for saving model outputs\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 comp1/                      # outputs for 'comp1'\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 comp2/                      # etc.\n\u251c\u2500\u2500 \ud83d\udcc1 surrogates/                     # surrogate save files\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 system_iter0/\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 system_iter1/\n\u251c\u2500\u2500 \ud83d\udcc4 amisc_2024-12-10T11.00.00.log   # log file\n\u2514\u2500\u2500 \ud83d\udcc4 plots.pdf                       # plots generated during training\n</code></pre></p> <p>Partial surrogates for an MD system</p> <p>By default, the <code>System</code> will try to build a surrogate for each component model in the system. If you don't want a surrogate to be built for a particular component model (e.g. if it's cheap to evaluate), then leave all <code>fidelity</code> options of the <code>Component</code> empty. This indicates that there is no way to \"refine\" your model, and so the <code>System</code> will skip building a surrogate for the component. You can check the <code>Component.has_surrogate</code> property to verify. During surrogate prediction, the underlying model function will be called instead for any components that do not have a surrogate.</p>"},{"location":"guides/train_surrogate/#train-a-surrogate","title":"Train a surrogate","text":"<p>Surrogate training is handled by <code>System.fit</code>. From a high level, surrogate training proceeds by taking a series of adaptive refinement steps until an end criterion is reached. There are three criteria for terminating the adaptive train loop:</p> <ul> <li>Maximum iteration - train for a set number of iterations,</li> <li>Runtime - train for at least a set length of time, then terminate at the end of the current iteration, or</li> <li>Tolerance - when relative improvement between iterations is below a tolerance level.</li> </ul> <p>For expensive models, it is highly recommended to parallelize model evaluations by passing an instance of a <code>concurrent.futures.Executor</code>. At each sequential iteration, the parallel executor will manage evaluating the models on all new training data in a parallel loop.</p> <p>It is also highly recommended to pass an independent test set to evaluate the surrogate's generalization on unseen data. A test set is a tuple of two Datasets: one dataset for model inputs and one dataset for the corresponding model outputs. Test sets do not guide any aspect of the training -- they are just used as a metric for monitoring performance during training.</p> <p>Coupling variable bounds</p> <p>Coupling variables are the inputs of any component model which are computed by and passed as outputs of an upstream component model. Since coupling variables are computed by a model, it may be difficult to know their domains a priori. When passing a test set to <code>System.fit()</code>, you may also set <code>estimate_bounds=True</code> to estimate all coupling variable bounds from the test set. Otherwise, you must manually set the coupling variable domains with a best guess for their expected ranges.</p> <p>We leave the details of the adaptive refinement in the AMISC journal paper, but you can view the logs and error plots during training to get an idea. Generally, at each iteration, the <code>System</code> loops over all candidate search directions for each component model, evaluates an \"error indicator\" metric that indicates potential improvement, and selects the most promising direction for more refinement. Once a direction is selected for refinement, new training points are sampled, the model is computed and stored, and the surrogate approximation is updated.</p> <p>Example</p> <pre><code>from concurrent.futures import ProcessPoolExecutor\n\nwith ProcessPoolExecutor(max_workers=8) as executor:\n    system.fit(max_iter=100,            # max number of refinement steps\n               runtime_hr=3,            # run for at least 3 hrs and stop at end of current iteration\n               max_tol=1e-3,            # or terminate once relative improvement falls below this threshold\n               save_interval=5,         # save the surrogate to file every 5 iterations\n               plot_interval=1,         # plot error indicators every iteration\n               test_set=(xtest, ytest), # test set of unseen inputs/outputs\n               estimate_bounds=True,    # estimate coupling variable bounds from ytest\n               executor=executor)       # parallelize model evaluations\n</code></pre>"},{"location":"guides/train_surrogate/#predict-with-a-surrogate","title":"Predict with a surrogate","text":"<p>Surrogate predictions are obtained using <code>System.predict</code>. The surrogate expects to be called with a Dataset of model inputs, which is a dictionary with variable names as keys and corresponding numeric values. The values for each input may be arrays, for which the surrogate will be computed over all inputs in the array. You may use <code>System.sample_inputs</code> to obtain a dataset of random input samples.</p> Single inputArrayed inputs <pre><code>system = System(...)  # computes y1 = x1 * x2, for example\n\ninputs = { 'x1': 0.5, 'x2': 1.5 }\noutput = system.predict(inputs)\n\n# will give { 'y1': 0.75 }\n</code></pre> <pre><code>system = System(...)  # computes y1 = x1 * x2, for example\n\ninputs = { 'x1': np.random.rand(100), 'x2': np.random.rand(100) }\noutput = system.predict(inputs)\n\n# will give { 'y1': np.array(shape=100) }\n</code></pre> <p>Important</p> <p>The input Dataset must contain both normalized and compressed inputs (for field quantities) before passing to <code>System.predict</code>. This is because the surrogate was trained in the normalized space for <code>Variables</code> with the <code>norm</code> attribute, and in the latent space for field quantity <code>Variables</code> with the <code>compression</code> attribute. Likewise, return values from <code>predict</code> will be normalized and compressed outputs. See the dataset conversion section for more information.</p> <p>You may also call <code>System.predict(..., use_model='best')</code> as an alias for calling the true component models instead of the surrogate (the inputs should still be normalized when passed in though -- they will get denormalized as needed under the hood).</p> <p>Finally, there are two \"modes\" for evaluating the surrogate:</p> Training modeEvaluation mode <p>In training mode, only the active index sets are used in the MISC combination technique approximation (see theory for details). Training mode uses only a subset of all available training data, and so its accuracy is generally worse than evaluation (or \"testing\") mode. <pre><code>outputs = system.predict(inputs, index_set='train')\n</code></pre></p> <p>In evaluation mode, all available training data is used, and so surrogate accuracy is generally higher than training mode. This is the default behavior of <code>predict</code>. <pre><code>outputs = system.predict(inputs, index_set='test')  # default\n</code></pre></p>"},{"location":"guides/train_surrogate/#evaluate-surrogate-performance","title":"Evaluate surrogate performance","text":"<p>The <code>System</code> object provides three methods for evaluating the surrogate performance:</p> <ul> <li><code>test_set_performance</code> - computes the relative error between the surrogate and the true model on a test set,</li> <li><code>plot_slice</code> - plots 1d slices of surrogate outputs over the inputs, and optionally compares to the true model,</li> <li><code>plot_allocation</code> - plots a bar chart that shows how computational resources were allocated during training.</li> </ul> <p>Example</p> <pre><code>system = System(...)  # define component models etc.\nsystem.fit()          # training\n\n# Evaluate surrogate performance\nrel_error = system.test_set_performance(xtest, ytest)\nsystem.plot_slice()\nsystem.plot_allocation()\n</code></pre>"},{"location":"guides/train_surrogate/#saving-to-file","title":"Saving to file","text":"<p>The <code>System</code> object provides two methods for saving and loading the surrogate from file: <pre><code>system.save_to_file('md_system.yml')\nsystem = System.load_from_file('md_system.yml')\n</code></pre></p> <p>By default, these methods use the YamlLoader class to read and write the <code>System</code> surrogate object from YAML files. If the <code>System.root_dir</code> property has been set, then save files will default to the <code>root_dir/surrogates</code> directory. If a save file is located within an <code>amisc_{timestamp}</code> directory, then the <code>root_dir</code> property will be set when loading from file.</p> <p>YAML files</p> <p>YAML files are a plain-text format, which allows easy inspection of the surrogate data saved in the file. You can also edit the surrogate properties directly in the file before loading back into memory. The save files closely mirror the format of configuration files and can be used as a template for future configurations.</p>"},{"location":"guides/train_surrogate/#convert-datasets-for-model-or-surrogate-usage","title":"Convert datasets for model or surrogate usage","text":"<p>Datasets for passing input/output arrays have two formats:</p> Model datasetSurrogate dataset <p>All values in the dataset are not normalized, and field quantities are in their full high-dimensional form (i.e. not compressed). This is how the model wrapper functions should expect their inputs to be formatted. <pre><code>x = Variable(norm='log10', domain=(10, 100))     # a scalar\nf = Variable(compression=...)                    # a field quantity\n\nmodel_dataset = { 'x': 100, 'f': np.array([1, 2, 3, ...]) }\nsystem.predict(model_dataset, use_model='best', normalized_inputs=False)\n</code></pre></p> <p>All values in the dataset are normalized, and field quantities are split into <code>r</code> arrays with the special <code>LATENT</code> ID string, enumerated from <code>0</code> to <code>r-1</code>, where <code>r</code> is the rank of the compressed latent space for the field quantity. <pre><code>x = Variable(norm='log10', domain=(10, 100))     # a scalar\nf = Variable(compression=...)                    # a field quantity\n\nsurrogate_dataset = { 'x': np.log10(100), 'f_LATENT0': 1.5, 'f_LATENT1': 0.5, ..., 'f_LATENT{r}': 0.01 }\nsystem.predict(surrogate_dataset)\n</code></pre></p> <p>By default, <code>System.predict</code> expects inputs in a normalized form for surrogate evaluation (but this may be toggled via the <code>normalized_inputs</code> flag). The <code>System.sample_inputs</code> method will also return normalized/compressed inputs by default.</p> <p>To convert between dataset formats (i.e. for comparing surrogate outputs to model outputs or vice versa), you may use the <code>to_model_dataset</code> and <code>to_surrogate_dataset</code> utilities. These methods will use the <code>Variable</code> objects to perform the appropriate normalizations and compression/reconstruction during conversion.</p> <p>Example</p> <pre><code>from amisc import to_model_dataset, to_surrogate_dataset\n\nx = Variable(norm='log10', domain=(10, 100))     # a scalar\nf = Variable(compression=...)                    # a field quantity\n\nmodel_dataset = { 'x': 100, 'f': np.array([1, 2, 3, ...]) }\n\nsurr_dataset, surr_vars = to_surrogate_dataset(model_dataset, [x, f])  # also returns the names of the latent variables\nmodel_dataset, coords = to_model_dataset(surr_dataset, [x, f])         # also returns grid coordinates for field quantities\n</code></pre> <p>Note that field quantities returned by <code>System.predict(use_model=...)</code> or <code>Component.call_model()</code> will be <code>numpy</code> object arrays. The <code>to_surrogate_dataset</code> only supports 1d object arrays for compression.</p>"},{"location":"guides/variables/","title":"Use variables","text":"<p>Variables are the basic objects used as inputs or outputs in a model. In this guide, we will learn how to construct and use variables within <code>amisc</code>.</p>"},{"location":"guides/variables/#construct-a-variable","title":"Construct a variable","text":"<p>In its most basic form, a variable is just a placeholder with a name, just like \\(x\\) in the equation \\(y=x^2\\).</p> <pre><code>from amisc import Variable\n\nx = Variable()     # implicitly named 'x'\ny = Variable('y')  # explicitly named 'y'\n</code></pre> <p>Variables can also have several descriptive attributes assigned to them. <pre><code>x = Variable(name='x1',                         # the main identification string of the variable\n             nominal=1,                         # a nominal value\n             description='My first variable',   # a lengthier description\n             units='rad/s',                     # units\n             tex='$x_1$',                       # a latex representation (for plotting/displaying)\n             category='calibration')            # for further classification (can be anything)\n</code></pre></p> <p>The variable's <code>name</code> is the key identifier of the variable, and allows the variable to be treated symbolically as a string. For example: <pre><code>x = Variable('x')\nassert x == 'x'\n\nd = {x: 'You can use the variable as a key in hash structures'}\nassert d[x] == d['x']\n</code></pre></p> <p>In addition, a useful data structure for lists of <code>Variables</code> is the <code>VariableList</code>: <pre><code>from amisc import VariableList, Variable\n\nvar_list = VariableList(['a', 'b', 'c'])\n\nassert var_list['a'] == 'a'                   # can use 'dict'-like access of variables\nassert isinstance(var_list['a'], Variable)    # stores the actual Variable objects\nassert var_list[2] == var_list['c']           # can also use normal 'list' indexing\n</code></pre></p> <p>An important attribute of <code>Variables</code> in the context of <code>amisc</code> is their domain, which must be defined when building surrogates: <pre><code>x = Variable('x', domain=(0, 1))  # domain over which surrogates will be built\n</code></pre></p> <p>There are three more attributes of variables that we will cover in the next sections: normalization, distributions (for random variables), and compression (for field quantities).</p>"},{"location":"guides/variables/#normalization","title":"Normalization","text":"<p>In the context of surrogates, it is sometimes advantageous to approximate over a transformed, or normalized input space. For example, a variable defined over the domain \\(x\\in (0.001, 100)\\) covers many orders of magnitude, which may be difficult to directly approximate using a polynomial surrogate. There are four basic normalizations provided by <code>amisc</code>: <pre><code>from amisc.transform import Log, Linear, Minmax, Zscore\n\nlog = Log((10, 0))                  # base 10 log with 0 offset\nlinear = Linear((0.5, 1))           # slope of 0.5 and offset of 1\nminmax = Minmax((-20, 20, 0, 1))    # scale from (-20, 20) -&gt; (0, 1)\nzscore = Zscore((5, 2))             # (x - mu) / sigma\n</code></pre></p> <p>These may also be specified as an equivalent string representation. The transform method should be passed as the <code>norm</code> attribute of the variable: <pre><code>for norm in ['log10', 'linear(0.5, 1)', 'minmax', 'zscore']:\n    x = Variable(norm=norm)\n</code></pre></p> <p>Values can then be normalized or denormalized directly by the variable: <pre><code>import numpy as np\n\nx = Variable(norm='log10')\nvalues = 10 ** (np.random.rand(20))\n\nassert np.allclose(x.denormalize(x.normalize(values)), values)\n</code></pre></p> <p>When a variable has a <code>norm</code>, the surrogate will select new training points in the transformed space and also compute the approximation on normalized inputs. If a variable is an output and has a <code>norm</code>, then the surrogate will fit the approximation to the normalized output.</p> <p>Building a surrogate in normalized space</p> <p>Consider the variable defined as: <pre><code>x = Variable(norm='log10', domain=(1e-3, 1e2))\n</code></pre> The surrogate will construct an approximation over the transformed domain \\((-3, 2)\\). When predicting with the surrogate, inputs will automatically have the same transform applied \\(\\tilde{x} = \\log_{10}(x)\\in(-3, 2)\\) before computing the surrogate.</p> <p>New transforms can be created by extending the <code>amisc.transform.Transform</code> base class. In addition, multiple transforms can be applied in series by passing a list of transforms to the <code>norm</code> attribute. For example, <code>x = Variable(norm=['log10', 'minmax'])</code> will apply a <code>minmax</code> transform over the <code>log10</code> space of <code>x</code>.</p>"},{"location":"guides/variables/#random-variables","title":"Random variables","text":"<p>A common use of surrogates is to permit propagating uncertain random variable inputs through a complicated simulation to quantify output uncertainty or to calibrate the model parameters. To this end, a <code>Variable</code> can be given a PDF through the <code>distribution</code> attribute. Several common PDFs are provided in <code>amisc.distribution</code>. <pre><code>uniform     = Variable(distribution='U(0, 1)')\nnormal      = Variable(distribution='N(0, 1)')\nlog_uniform = Variable(distribution='LU(1e-3, 1e2)')\nlog_normal  = Variable(distribution='LN(-2, 1)')\n</code></pre></p> <p>With a distribution, variable's can sample from the PDF or evaluate the PDF of values under the distribution: <pre><code>x = Variable(distribution='N(0, 1)')\n\nsamples = x.sample(1000)\npdfs    = x.pdf(samples)\n</code></pre></p> <p>When a variable has a distribution, the surrogate will select new training points during <code>fit()</code> that are clustered closer to areas of greater weight. New distributions can be created by extending the <code>amisc.distribution.Distribution</code> base class.</p>"},{"location":"guides/variables/#field-quantities","title":"Field quantities","text":"<p>By default, all variables are treated as scalar quantities. However, it is sometimes possible to have high-dimensional variables, such as the solution of a simulation on a PDE mesh -- we refer to these variables as \"field quantities\". For field quantities to be useful in the context of <code>amisc</code> surrogates, we must be able to \"compress\" them to lower dimension such that we can effectively build surrogate approximations in an appropriate low-dimensional \"latent\" space.</p> <p>To this end, a field quantity is defined by giving a <code>compression</code> attribute to a variable. A compression method must:</p> <ul> <li>define a set of coordinates on which the field quantity exists (i.e. the Cartesian points from a simulation mesh grid),</li> <li>define a \"map\" that both compresses field quantity data into the latent space and reconstructs the full field quantity back from the latent space, and</li> <li>have a predetermined size (or \"rank\") of the latent space.</li> </ul> <p>Compression coordinates should be an array of shape \\((N, D)\\), where \\(N\\) is the total number of grid points and \\(D\\) is the Cartesian dimension (i.e. 1d, 2d, etc.). A single field quantity <code>Variable</code> may contain several QoIs on the same grid coordinates, so that the total number of \"degrees of freedom\" (DoF) of the variable is equal to \\(N\\times Q\\), where \\(Q\\) is the number of QoIs.</p> <p>For example, say a simulation outputs the \\(x, y, z\\) components of velocity on an unstructured mesh of 1000 nodes. We might define a velocity field quantity as: <pre><code>vel = Variable('velocity', compression=dict(coords=sim_coords,\n                                            fields=['ux', 'uy', 'uz'],\n                                            method='svd'))\n\nprint(sim_coords.shape)                 # (num_pts, dim)\nassert vel.compression.dof == 1000 * 3  # (num_pts * num_qoi)\n</code></pre> for some predefined set of Cartesian <code>sim_coords</code> that we extracted from our simulation. Currently, SVD is the only available compression method, but other methods can be used by implementing the <code>amisc.compression.Compression</code> base class.</p> <p>In order to make use of this field quantity when building surrogates, we'll need to call <code>compression.compute_map()</code>, which for <code>SVD</code> requires passing a data matrix and a desired <code>rank</code> of the truncation.</p> <p>SVD compression</p> <p>To use the SVD compression method, we need to form a \"data matrix\" of shape <code>(dof, num_samples)</code>, where <code>dof</code> is the original <code>(N, Q)</code> field quantity flattened to <code>dof</code>, and <code>num_samples</code> are several samples of the full field quantity (such as for varying simulation inputs). In other words, each column of the data matrix is a \"snapshot\" of the simulation output for this field quantity.</p> <pre><code>sim_coords = np.random.rand(1000, 3)  # (i.e. load actual Cartesian coords from a result file)\nnum_samples = 200\ndof = 3000\ndata_matrix = np.empty((dof, num_samples))\n\nfor i in range(num_samples):\n    simulation_data = np.random.rand(1000, 3)  # (N, Q) simulation data (i.e. load from a result file)\n    data_matrix[:, i] = np.ravel(simulation_data)\n\nvel = Variable(compression=dict(coords=sim_coords, fields=['ux', 'uy', 'uz'], method='svd'))\nvel.compression.compute_map(data_matrix, rank=10)\n\n# Now we can use the compression map to compress/reconstruct new values\nnew_sim_data  = {'ux': np.random.rand(1000), 'uy': ..., 'uz': ...}\nlatent_data   = vel.compress(new_sim_data)\nreconstructed = vel.reconstruct(latent_data)\n</code></pre> <p>Once the compression map has been computed, we can compress or reconstruct new field quantity data: <pre><code>new_sim_data  = {'field1': ..., 'field2': ...}  # arrays of shape (num_pts,) for each QoI in compression.fields\nlatent_data   = vel.compress(new_sim_data)      # a single array of shape (rank,) with the key 'latent'\nreconstructed = vel.reconstruct(latent_data)    # arrays of shape (num_pts,) for each reconstructed QoI\n</code></pre></p> <p>You can optionally pass new coordinates to <code>compress()</code> and <code>reconstruct()</code>, so that the data will be interpolated to/from any set of coordinates to the original <code>compression.coords</code> (e.g. if the new data is not defined on the same grid).</p> <p>If you also pass a <code>norm</code> method to a field quantity <code>Variable</code>, then raw simulation data will be normalized first by the indicated method before compression. In general, the compression workflow is interpolate \u2192 normalize \u2192 compress and vice versa for reconstruction. The interpolate step is required to make sure the data aligns with the compression map's coordinates. See <code>Variable.compress</code> for more details.</p> <p>Unlike scalar variables, the domain of a field quantity <code>Variable</code> should be a list of domains, one for each \"latent\" dimension. Since it's typically not practical to know these domains ahead of time, you can either:</p> <ol> <li>Use the <code>Variable</code> to compress some example data and extract the latent domains manually,</li> <li>Use the built-in <code>Compression.estimate_latent_ranges()</code> function (which for <code>SVD</code> will compress the <code>data_matrix</code> and estimate latent ranges from there),</li> <li>Specify a single, conservative domain (like <code>(-10, 10)</code>) that will be used for all the latent dimensions at runtime, or</li> <li>Leave the domain empty, and have <code>System.fit()</code> estimate and update the domains from a test set.</li> </ol> <p>The only time you would need to worry about specifying the latent domains is if you are intending on using a field quantity as an input to any of your component models.</p> <p>As a final note on field quantities, once you've defined and computed the compression map, <code>amisc</code> will internally use the compression map during training or prediction to convert the field quantity to/from the latent space. If you have a field quantity named <code>\"vel\"</code> for example, <code>amisc</code> will generate latent coefficients with the names <code>\"vel_LATENT0\" ... \"vel_LATENT1\"</code> and so on up to the total size of the latent space. These temporary latent coefficients will be used as inputs and outputs until they are converted back to the full field quantity. So if you ever inspect raw data arrays returned by <code>amisc</code>, you may find these temporary latent coefficients floating around. See the <code>amisc.to_model_dataset</code> utility function for reconstructing such arrays.</p> <p>Object arrays for field quantities</p> <p>If you call <code>System.predict(use_model=...)</code> and inspect the true model return values, you will find that field quantities get stored in <code>numpy</code> arrays with <code>dtype=object</code>. The shape of the object arrays will match the \"loop\" dimension of the inputs, which is also the same shape as all scalar return values. Each element of the object array will be the field quantity array corresponding to the given input. This allows the field quantity to possibly take on different shapes for different inputs, such as if the model computes a new mesh for each set of inputs.</p> <p>For example: <pre><code>inputs = {'x': np.random.rand(100)}\noutputs = System.predict(inputs, use_model='best')\nscalar = outputs['y']       # numeric array of shape (100,)\nfield = outputs['y_field']  # object array of shape  (100,)\n\nfield[0].shape              # (20, 20) for example\nfield[1].shape              # (19, 25) for example, if the mesh changed between inputs\n</code></pre></p> <p>Your component models should generally return the grid coordinates for field quantities in a special variable name suffixed by <code>\"_coords\"</code>. For example, if your model returns a field quantity named <code>u_ion</code>, you would also return the grid coordinates as <code>u_ion_coords</code>.</p>"},{"location":"reference/","title":"<code>amisc</code>","text":"<p>Efficient framework for building surrogates of multidisciplinary systems using the adaptive multi-index stochastic collocation (AMISC) technique.</p> <ul> <li>Author - Joshua Eckels (eckelsjd@umich.edu)</li> <li>License - GPL-3.0</li> </ul> <p>The <code>amisc</code> package takes an object-oriented approach to building a surrogate of a multidisciplinary system. From the bottom up, you have:</p> <ul> <li>variables that serve as inputs and outputs for the models,</li> <li>interpolators that define a specific input \u2192 output mathematical relationship to approximate a function,</li> <li>components that wrap a model for a single discipline, and a</li> <li>system that defines the connections between components in a multidisciplinary system.</li> </ul> <p>The system is ultimately independent of the specific models, interpolation methods, or underlying variables. As such, the primary top-level object that users of the <code>amisc</code> package will interact with is the <code>System</code>.</p> <p>Variables additionally use <code>Transform</code>, <code>Distribution</code>, and <code>Compression</code> interfaces to manage normalization, PDFs, and field quantity compression, respectively.</p> <p>Currently, only Lagrange polynomial interpolation and Linear regression are implemented as the underlying surrogate methods with a sparse grid data structure. SVD is also the only currently implemented method for compression. However, interfaces are provided for <code>Interpolator</code>, <code>TrainingData</code>, and <code>Compression</code> to allow for easy extension to other methods.</p> <p>Here is a class diagram summary of this workflow:</p> <pre><code>classDiagram\n    namespace Core {\n        class System {\n          +list[Component] components\n          +TrainHistory train_history\n          +fit()\n          +predict(x)\n        }\n        class Component {\n          +callable model\n          +list[Variable] inputs\n          +list[Variable] outputs\n          +Interpolator interpolator\n          +TrainingData training_data\n          +activate_index(alpha, beta)\n          +predict(x)\n        }\n        class Variable {\n          +str name\n          +tuple domain\n          +Distribution dist\n          +Transform norm\n          +Compression comp\n          +sample()\n          +normalize()\n          +compress()\n        }\n    }\n    class Interpolator {\n      &lt;&lt;abstract&gt;&gt;\n      + refine()\n      + predict(x)\n    }\n    class TrainingData {\n      &lt;&lt;abstract&gt;&gt;\n      +get()\n      +set()\n      +refine()\n    }\n    class Transform {\n      &lt;&lt;abstract&gt;&gt;\n      +transform(values)\n    }\n    class Distribution {\n      &lt;&lt;abstract&gt;&gt;\n      +sample(size)\n      +pdf(x)\n    }\n    class Compression {\n      &lt;&lt;abstract&gt;&gt;\n      +compress(values)\n      +reconstruct(values)\n    }\n    System --o \"1..n\" Component\n    Component --o \"1..n\" Variable\n    direction TD\n    Component --* Interpolator\n    Component --* TrainingData\n    Variable --o Transform\n    Variable --o Distribution\n    Variable --o Compression</code></pre> <p>Note how the <code>System</code> aggregates the <code>Component</code>, which aggregates the <code>Variable</code>. In other words, variables can act independently of components, and components can act independently of systems. Components make use of <code>Interpolator</code> and <code>TrainingData</code> interfaces to manage the underlying surrogate method and training data, respectively. Similarly, <code>Variables</code> use <code>Transform</code>, <code>Distribution</code>, and <code>Compression</code> interfaces to manage normalization, PDFs, and field quantity compression, respectively.</p> <p>The <code>amisc</code> package also includes a <code>FileLoader</code> interface for loading and dumping <code>amisc</code> objects to/from file. We recommend using the built-in <code>YamlLoader</code> for this purpose, as it includes custom YAML tags for reading/writing <code>amisc</code> objects from file.</p>"},{"location":"reference/#amisc.FileLoader","title":"<code>FileLoader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Common interface for loading and dumping <code>amisc</code> objects to/from file.</p>"},{"location":"reference/#amisc.FileLoader.dump","title":"<code>dump(obj, stream, **kwargs)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Save an <code>amisc</code> object to a stream. If a file path is given, will attempt to write to the file.</p> Source code in <code>src/amisc/__init__.py</code> <pre><code>@classmethod\n@_abstractmethod\ndef dump(cls, obj, stream: str | _Path | _Any, **kwargs):\n    \"\"\"Save an `amisc` object to a stream. If a file path is given, will attempt to write to the file.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#amisc.FileLoader.load","title":"<code>load(stream, **kwargs)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Load an <code>amisc</code> object from a stream. If a file path is given, will attempt to open the file.</p> Source code in <code>src/amisc/__init__.py</code> <pre><code>@classmethod\n@_abstractmethod\ndef load(cls, stream: str | _Path | _Any, **kwargs):\n    \"\"\"Load an `amisc` object from a stream. If a file path is given, will attempt to open the file.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#amisc.YamlLoader","title":"<code>YamlLoader</code>","text":"<p>               Bases: <code>FileLoader</code></p> <p>YAML file loader for <code>amisc</code> objects.</p>"},{"location":"reference/#amisc.YamlLoader.dump","title":"<code>dump(obj, stream, **kwargs)</code>  <code>classmethod</code>","text":"<p>Extra kwargs get passed to <code>yaml.dump</code>.</p> Source code in <code>src/amisc/__init__.py</code> <pre><code>@classmethod\ndef dump(cls, obj, stream, **kwargs):\n    \"\"\"Extra kwargs get passed to `yaml.dump`.\"\"\"\n    kwargs['allow_unicode'] = kwargs.get('allow_unicode', True)\n    kwargs['sort_keys'] = kwargs.get('sort_keys', False)\n    try:\n        p = _Path(stream).with_suffix('.yml')\n    except (TypeError, ValueError):\n        pass\n    else:\n        with open(p, 'w', encoding='utf-8') as fd:\n            return _yaml.dump(obj, fd, Dumper=cls._yaml_dumper(), **kwargs)\n\n    return _yaml.dump(obj, stream, Dumper=cls._yaml_dumper(), **kwargs)\n</code></pre>"},{"location":"reference/#amisc.YamlLoader.load","title":"<code>load(stream, **kwargs)</code>  <code>classmethod</code>","text":"<p>Extra kwargs ignored for YAML loading (since they are not used by <code>yaml.load</code>).</p> Source code in <code>src/amisc/__init__.py</code> <pre><code>@classmethod\ndef load(cls, stream, **kwargs):\n    \"\"\"Extra kwargs ignored for YAML loading (since they are not used by `yaml.load`).\"\"\"\n    try:\n        p = _Path(stream).with_suffix('.yml')\n    except (TypeError, ValueError):\n        pass\n    else:\n        with open(p, 'r', encoding='utf-8') as fd:\n            return _yaml.load(fd, Loader=cls._yaml_loader())\n\n    return _yaml.load(stream, Loader=cls._yaml_loader())\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> amisc</li> <li> component</li> <li> compression</li> <li> distribution</li> <li> interpolator</li> <li> serialize</li> <li> system</li> <li> training</li> <li> transform</li> <li> typing</li> <li> utils</li> <li> variable</li> </ul>"},{"location":"reference/component/","title":"component","text":""},{"location":"reference/component/#amisc.component","title":"<code>amisc.component</code>","text":"<p>A <code>Component</code> is an <code>amisc</code> wrapper around a single discipline model. It manages surrogate construction and a hierarchy of modeling fidelities.</p> <p>Multi-indices in the MISC approximation</p> <p>A multi-index is a tuple of natural numbers, each specifying a level of fidelity. You will frequently see two multi-indices: <code>alpha</code> and <code>beta</code>. The <code>alpha</code> (or \\(\\alpha\\)) indices specify physical model fidelity and get passed to the model as an additional argument (e.g. things like discretization level, time step size, etc.). The <code>beta</code> (or \\(\\beta\\)) indices specify surrogate refinement level, so typically an indication of the amount of training data used or the complexity of the surrogate model. We divide \\(\\beta\\) into <code>data_fidelity</code> and <code>surrogate_fidelity</code> for specifying training data and surrogate model complexity, respectively.</p> <p>Includes:</p> <ul> <li><code>ModelKwargs</code> \u2014 a dataclass for storing model keyword arguments</li> <li><code>StringKwargs</code> \u2014 a dataclass for storing model keyword arguments as a string</li> <li><code>IndexSet</code> \u2014 a dataclass that maintains a list of multi-indices</li> <li><code>MiscTree</code> \u2014 a dataclass that maintains MISC data in a <code>dict</code> tree, indexed by <code>alpha</code> and <code>beta</code></li> <li><code>Component</code> \u2014 a class that manages a single discipline model and its surrogate hierarchy</li> </ul>"},{"location":"reference/component/#amisc.component.Component","title":"<code>Component(model, *args, inputs=None, outputs=None, name=None, **kwargs)</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Serializable</code></p> <p>A <code>Component</code> wrapper around a single discipline model. It manages MISC surrogate construction and a hierarchy of modeling fidelities.</p> <p>A <code>Component</code> can be constructed by specifying a model, input and output variables, and additional configurations such as the maximum fidelity levels, the interpolator type, and the training data type. If <code>model_fidelity</code>, <code>data_fidelity</code>, and <code>surrogate_fidelity</code> are all left empty, then the <code>Component</code> will not use a surrogate model, instead calling the underlying model directly. The <code>Component</code> can be serialized to a YAML file and deserialized back into a Python object.</p> <p>A simple <code>Component</code></p> <pre><code>from amisc import Component, Variable\n\nx = Variable(domain=(0, 1))\ny = Variable()\nmodel = lambda x: {'y': x['x']**2}\ncomp = Component(model=model, inputs=[x], outputs=[y])\n</code></pre> <p>Each fidelity index in \\(\\alpha\\) increases in refinement from \\(0\\) up to <code>model_fidelity</code>. Each fidelity index in \\(\\beta\\) increases from \\(0\\) up to <code>(data_fidelity, surrogate_fidelity)</code>. From the <code>Component's</code> perspective, the concatenation of \\((\\alpha, \\beta)\\) fully specifies a single fidelity \"level\". The <code>Component</code> forms an approximation of the model by summing up over many of these concatenated sets of \\((\\alpha, \\beta)\\).</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>the name of the <code>Component</code></p> <p> TYPE: <code>Optional[str]</code> </p> <code>model</code> <p>the model or function that is to be approximated, callable as <code>y = f(x)</code></p> <p> TYPE: <code>str | Callable[[dict | Dataset, ...], dict | Dataset]</code> </p> <code>inputs</code> <p>the input variables to the model</p> <p> TYPE: <code>_VariableLike</code> </p> <code>outputs</code> <p>the output variables from the model</p> <p> TYPE: <code>_VariableLike</code> </p> <code>model_kwargs</code> <p>extra keyword arguments to pass to the model</p> <p> TYPE: <code>str | dict | ModelKwargs</code> </p> <code>model_fidelity</code> <p>the maximum level of refinement for each fidelity index in \\(\\alpha\\) for model fidelity</p> <p> TYPE: <code>str | tuple</code> </p> <code>data_fidelity</code> <p>the maximum level of refinement for each fidelity index in \\(\\beta\\) for training data</p> <p> TYPE: <code>str | tuple</code> </p> <code>surrogate_fidelity</code> <p>the max level of refinement for each fidelity index in \\(\\beta\\) for the surrogate</p> <p> TYPE: <code>str | tuple</code> </p> <code>interpolator</code> <p>the interpolator to use as the underlying surrogate model</p> <p> TYPE: <code>Any | Interpolator</code> </p> <code>vectorized</code> <p>whether the model supports vectorized input/output (i.e. datasets with arbitrary shape <code>(...,)</code>)</p> <p> TYPE: <code>bool</code> </p> <code>call_unpacked</code> <p>whether the model expects unpacked input arguments (i.e. <code>func(x1, x2, ...)</code>)</p> <p> TYPE: <code>Optional[bool]</code> </p> <code>ret_unpacked</code> <p>whether the model returns unpacked output arguments (i.e. <code>func() -&gt; (y1, y2, ...)</code>)</p> <p> TYPE: <code>Optional[bool]</code> </p> <code>active_set</code> <p>the current active set of multi-indices in the MISC approximation</p> <p> TYPE: <code>list | set | IndexSet</code> </p> <code>candidate_set</code> <p>all neighboring multi-indices that are candidates for inclusion in <code>active_set</code></p> <p> TYPE: <code>list | set | IndexSet</code> </p> <code>misc_states</code> <p>the interpolator states for each multi-index in the MISC approximation</p> <p> TYPE: <code>dict | MiscTree</code> </p> <code>misc_costs</code> <p>the computational cost associated with each multi-index in the MISC approximation</p> <p> TYPE: <code>dict | MiscTree</code> </p> <code>misc_coeff_train</code> <p>the combination technique coefficients for the active set multi-indices</p> <p> TYPE: <code>dict | MiscTree</code> </p> <code>misc_coeff_test</code> <p>the combination technique coefficients for the active and candidate set multi-indices</p> <p> TYPE: <code>dict | MiscTree</code> </p> <code>model_costs</code> <p>the tracked average single fidelity model costs for each \\(\\alpha\\)</p> <p> TYPE: <code>dict</code> </p> <code>model_evals</code> <p>the tracked number of evaluations for each \\(\\alpha\\)</p> <p> TYPE: <code>dict</code> </p> <code>training_data</code> <p>the training data storage structure for the surrogate model</p> <p> TYPE: <code>Any | TrainingData</code> </p> <code>serializers</code> <p>the custom serializers for the <code>[model_kwargs, interpolator, training_data]</code> <code>Component</code> attributes -- these should be the types of the serializer objects, which will be inferred from the data passed in if not explicitly set</p> <p> TYPE: <code>Optional[ComponentSerializers]</code> </p> <code>_logger</code> <p>the logger for the <code>Component</code></p> <p> TYPE: <code>Optional[Logger]</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def __init__(self, /, model, *args, inputs=None, outputs=None, name=None, **kwargs):\n    if name is None:\n        name = _inspect_assignment('Component')  # try to assign the name from inspection\n    name = name or model.__name__ or \"Component_\" + \"\".join(random.choices(string.digits, k=3))\n\n    # Determine how the model expects to be called and gather inputs/outputs\n    _ = self._validate_model_signature(model, args, inputs, outputs, kwargs.get('call_unpacked', None),\n                                       kwargs.get('ret_unpacked', None))\n    model, inputs, outputs, call_unpacked, ret_unpacked = _\n    kwargs['call_unpacked'] = call_unpacked\n    kwargs['ret_unpacked'] = ret_unpacked\n\n    # Gather all model kwargs (anything else passed in for kwargs is assumed to be a model kwarg)\n    model_kwargs = kwargs.get('model_kwargs', {})\n    for key in kwargs.keys() - self.model_fields.keys():\n        model_kwargs[key] = kwargs.pop(key)\n    kwargs['model_kwargs'] = model_kwargs\n\n    # Gather data serializers from type checks (if not passed in as a kwarg)\n    serializers = kwargs.get('serializers', {})  # directly passing serializers will override type checks\n    for key in ComponentSerializers.__annotations__.keys():\n        field = kwargs.get(key, None)\n        if isinstance(field, dict):\n            field_super = next(filter(lambda x: issubclass(x, Serializable),\n                                      typing.get_args(self.model_fields[key].annotation)), None)\n            field = field_super.from_dict(field) if field_super is not None else field\n            kwargs[key] = field\n        if not serializers.get(key, None):\n            serializers[key] = type(field) if isinstance(field, Serializable) else (\n                type(self.model_fields[key].default))\n    kwargs['serializers'] = serializers\n\n    super().__init__(model=model, inputs=inputs, outputs=outputs, name=name, **kwargs)  # Runs pydantic validation\n\n    # Set internal properties\n    assert self.is_downward_closed(self.active_set.union(self.candidate_set))\n    self.set_logger()\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.has_surrogate","title":"<code>has_surrogate: bool</code>  <code>property</code>","text":"<p>The component has no surrogate model if there are no fidelity indices.</p>"},{"location":"reference/component/#amisc.component.Component.max_alpha","title":"<code>max_alpha: MultiIndex</code>  <code>property</code>","text":"<p>The maximum model fidelity multi-index (alias for <code>model_fidelity</code>).</p>"},{"location":"reference/component/#amisc.component.Component.max_beta","title":"<code>max_beta: MultiIndex</code>  <code>property</code>","text":"<p>The maximum surrogate fidelity multi-index is a combination of training and interpolator indices.</p>"},{"location":"reference/component/#amisc.component.Component.activate_index","title":"<code>activate_index(alpha, beta, model_dir=None, executor=None, weight_fcns='pdf')</code>","text":"<p>Add a multi-index to the active set and all neighbors to the candidate set.</p> <p>Warning</p> <p>The user of this function is responsible for ensuring that the index set maintains downward-closedness. That is, only activate indices that are neighbors of the current active set.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>A multi-index specifying model fidelity</p> <p> TYPE: <code>MultiIndex</code> </p> <code>beta</code> <p>A multi-index specifying surrogate fidelity</p> <p> TYPE: <code>MultiIndex</code> </p> <code>model_dir</code> <p>Directory to save model output files</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>executor</code> <p>Executor for parallel execution of model on training data if the model is not vectorized</p> <p> TYPE: <code>Executor</code> DEFAULT: <code>None</code> </p> <code>weight_fcns</code> <p>Dictionary of weight functions for each input variable (defaults to the variable PDFs); each function should be callable as <code>fcn(x: np.ndarray) -&gt; np.ndarray</code>, where the input is an array of normalized input data and the output is an array of weights. If None, then no weighting is applied.</p> <p> TYPE: <code>dict[str, callable] | Literal['pdf'] | None</code> DEFAULT: <code>'pdf'</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def activate_index(self, alpha: MultiIndex, beta: MultiIndex, model_dir: str | Path = None,\n                   executor: Executor = None, weight_fcns: dict[str, callable] | Literal['pdf'] | None = 'pdf'):\n    \"\"\"Add a multi-index to the active set and all neighbors to the candidate set.\n\n    !!! Warning\n        The user of this function is responsible for ensuring that the index set maintains downward-closedness.\n        That is, only activate indices that are neighbors of the current active set.\n\n    :param alpha: A multi-index specifying model fidelity\n    :param beta: A multi-index specifying surrogate fidelity\n    :param model_dir: Directory to save model output files\n    :param executor: Executor for parallel execution of model on training data if the model is not vectorized\n    :param weight_fcns: Dictionary of weight functions for each input variable (defaults to the variable PDFs);\n                        each function should be callable as `fcn(x: np.ndarray) -&gt; np.ndarray`, where the input\n                        is an array of normalized input data and the output is an array of weights. If None, then\n                        no weighting is applied.\n    \"\"\"\n    if (alpha, beta) in self.active_set:\n        self.logger.warning(f'Multi-index {(alpha, beta)} is already in the active index set. Ignoring...')\n        return\n    if (alpha, beta) not in self.candidate_set and (sum(alpha) + sum(beta)) &gt; 0:\n        # Can only activate the initial index (0, 0, ... 0) without it being in the candidate set\n        self.logger.warning(f'Multi-index {(alpha, beta)} is not a neighbor of the active index set, so it '\n                            f'cannot be activated. Please only add multi-indices from the candidate set. '\n                            f'Ignoring...')\n        return\n\n    # Collect all neighbor candidate indices; sort by largest model cost first\n    neighbors = self._neighbors(alpha, beta, forward=True)\n    indices = list(itertools.chain([(alpha, beta)] if (alpha, beta) not in self.candidate_set else [], neighbors))\n    indices.sort(key=lambda ele: self.model_costs.get(ele[0], sum(ele[0])), reverse=True)\n\n    # Refine and collect all new model inputs (i.e. training points) requested by the new candidates\n    alpha_list = []    # keep track of model fidelities\n    design_list = []   # keep track of training data coordinates/locations/indices\n    model_inputs = {}  # concatenate all model inputs\n    field_coords = {f'{var}{COORDS_STR_ID}': self.model_kwargs.get(f'{var}{COORDS_STR_ID}', None)\n                    for var in self.inputs}\n    domains = self.inputs.get_domains()\n\n    if weight_fcns == 'pdf':\n        weight_fcns = self.inputs.get_pdfs()\n\n    for a, b in indices:\n        if ((a, b[:len(self.data_fidelity)] + (0,) * len(self.surrogate_fidelity)) in\n                self.active_set.union(self.candidate_set)):\n            # Don't refine training data if only updating surrogate fidelity indices\n            # Training data is the same for all surrogate fidelity indices, given constant data fidelity\n            design_list.append([])\n            continue\n\n        design_coords, design_pts = self.training_data.refine(a, b[:len(self.data_fidelity)],\n                                                              domains, weight_fcns)\n        design_pts, fc = to_model_dataset(design_pts, self.inputs, del_latent=True, **field_coords)\n\n        # Remove duplicate (alpha, coords) pairs -- so you don't evaluate the model twice for the same input\n        i = 0\n        del_idx = []\n        for other_design in design_list:\n            for other_coord in other_design:\n                for j, curr_coord in enumerate(design_coords):\n                    if curr_coord == other_coord and a == alpha_list[i] and j not in del_idx:\n                        del_idx.append(j)\n                i += 1\n        design_coords = [design_coords[j] for j in range(len(design_coords)) if j not in del_idx]\n        design_pts = {var: np.delete(arr, del_idx, axis=0) for var, arr in design_pts.items()}\n\n        alpha_list.extend([tuple(a)] * len(design_coords))\n        design_list.append(design_coords)\n        field_coords.update(fc)\n        for var in design_pts:\n            model_inputs[var] = design_pts[var] if model_inputs.get(var) is None else (\n                np.concatenate((model_inputs[var], design_pts[var]), axis=0))\n\n    # Evaluate model at designed training points\n    if len(alpha_list) &gt; 0:\n        self.logger.info(f\"Running {len(alpha_list)} total model evaluations for component \"\n                         f\"'{self.name}' new candidate indices: {indices}...\")\n        model_outputs = self.call_model(model_inputs, model_fidelity=alpha_list, output_path=model_dir,\n                                        executor=executor, track_costs=True, **field_coords)\n        self.logger.info(f\"Model evaluations complete for component '{self.name}'.\")\n        errors = model_outputs.pop('errors', {})\n    else:\n        self._model_start_time = -1.0\n        self._model_end_time = -1.0\n\n    # Unpack model outputs and update states\n    start_idx = 0\n    for i, (a, b) in enumerate(indices):\n        num_train_pts = len(design_list[i])\n        end_idx = start_idx + num_train_pts  # Ensure loop dim of 1 gets its own axis (might have been squeezed)\n\n        if num_train_pts &gt; 0:\n            yi_dict = {var: arr[np.newaxis, ...] if len(alpha_list) == 1 and arr.shape[0] != 1 else\n                       arr[start_idx:end_idx, ...] for var, arr in model_outputs.items()}\n\n            # Check for errors and store\n            err_coords = []\n            err_list = []\n            for idx in list(errors.keys()):\n                if idx &lt; end_idx:\n                    err_info = errors.pop(idx)\n                    err_info['index'] = idx - start_idx\n                    err_coords.append(design_list[i][idx - start_idx])\n                    err_list.append(err_info)\n            if len(err_list) &gt; 0:\n                self.logger.warning(f\"Model errors occurred while adding candidate ({a}, {b}) for component \"\n                                    f\"{self.name}. Leaving NaN values in training data...\")\n                self.training_data.set_errors(a, b[:len(self.data_fidelity)], err_coords, err_list)\n\n            # Compress field quantities and normalize\n            yi_dict, y_vars = to_surrogate_dataset(yi_dict, self.outputs, del_fields=False, **field_coords)\n\n            # Store training data, computational cost, and new interpolator state\n            self.training_data.set(a, b[:len(self.data_fidelity)], design_list[i], yi_dict)\n            self.training_data.impute_missing_data(a, b[:len(self.data_fidelity)])\n\n        else:\n            y_vars = self._surrogate_outputs()\n\n        self.misc_costs[a, b] = num_train_pts\n        self.misc_states[a, b] = self.interpolator.refine(b[len(self.data_fidelity):],\n                                                          self.training_data.get(a, b[:len(self.data_fidelity)],\n                                                                                 y_vars=y_vars, skip_nan=True),\n                                                          self.misc_states.get((alpha, beta)),\n                                                          domains)\n        start_idx = end_idx\n\n    # Move to the active index set\n    s = set()\n    s.add((alpha, beta))\n    self.update_misc_coeff(IndexSet(s), index_set='train')\n    if (alpha, beta) in self.candidate_set:\n        self.candidate_set.remove((alpha, beta))\n    else:\n        # Only for initial index which didn't come from the candidate set\n        self.update_misc_coeff(IndexSet(s), index_set='test')\n    self.active_set.update(s)\n\n    self.update_misc_coeff(neighbors, index_set='test')  # neighbors will only ever pass through here once\n    self.candidate_set.update(neighbors)\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.cache","title":"<code>cache(kind='training')</code>","text":"<p>Cache data for quicker access. Only <code>\"training\"</code> is supported.</p> PARAMETER DESCRIPTION <code>kind</code> <p>the type(s) of data to cache (only \"training\" is supported). This will cache the surrogate training data with nans removed.</p> <p> TYPE: <code>list | Literal['training']</code> DEFAULT: <code>'training'</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def cache(self, kind: list | Literal[\"training\"] = \"training\"):\n    \"\"\"Cache data for quicker access. Only `\"training\"` is supported.\n\n    :param kind: the type(s) of data to cache (only \"training\" is supported). This will cache the\n                 surrogate training data with nans removed.\n    \"\"\"\n    if not isinstance(kind, list):\n        kind = [kind]\n\n    if \"training\" in kind:\n        self._cache.setdefault(\"training\", {})\n        y_vars = self._surrogate_outputs()\n        for alpha, beta in self.active_set.union(self.candidate_set):\n            self._cache[\"training\"].setdefault(alpha, {})\n\n            if beta not in self._cache[\"training\"][alpha]:\n                self._cache[\"training\"][alpha][beta] = self.training_data.get(alpha, beta[:len(self.data_fidelity)],\n                                                                              y_vars=y_vars, skip_nan=True)\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.call_model","title":"<code>call_model(inputs, model_fidelity=None, output_path=None, executor=None, track_costs=False, **kwds)</code>","text":"<p>Wrapper function for calling the underlying component model.</p> <p>This function formats the input data, calls the model, and processes the output data. It supports vectorized calls, parallel execution using an executor, or serial execution. These options are checked in that order, with the first available method used. Must set <code>Component.vectorized=True</code> if the model supports input arrays of the form <code>(N,)</code> or even arbitrary shape <code>(...,)</code>.</p> <p>Parallel Execution</p> <p>The underlying model must be defined in a global module scope if <code>pickle</code> is the serialization method for the provided <code>Executor</code>.</p> <p>Additional return values</p> <p>The model can return additional items that are not part of <code>Component.outputs</code>. These items are returned as object arrays in the output <code>dict</code>. Two special return values are <code>model_cost</code> and <code>output_path</code>. Returning <code>model_cost</code> will store the computational cost of a single model evaluation (which is used by <code>amisc</code> adaptive surrogate training). Returning <code>output_path</code> will store the output file name if the model wrote any files to disk.</p> <p>Handling errors</p> <p>If the underlying component model raises an exception, the error is stored in <code>output_dict['errors']</code> with the index of the input data that caused the error. The output data for that index is set to <code>np.nan</code> for each output variable.</p> PARAMETER DESCRIPTION <code>inputs</code> <p>The input data for the model, formatted as a <code>dict</code> with a key for each input variable and a corresponding value that is an array of the input data. If specified as a plain list, then the order is assumed the same as <code>Component.inputs</code>.</p> <p> TYPE: <code>dict | Dataset</code> </p> <code>model_fidelity</code> <p>Fidelity indices to tune the model fidelity (model must request this in its keyword arguments).</p> <p> TYPE: <code>Literal['best', 'worst'] | tuple | list</code> DEFAULT: <code>None</code> </p> <code>output_path</code> <p>Directory to save model output files (model must request this in its keyword arguments).</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>executor</code> <p>Executor for parallel execution if the model is not vectorized (optional).</p> <p> TYPE: <code>Executor</code> DEFAULT: <code>None</code> </p> <code>track_costs</code> <p>Whether to track the computational cost of each model evaluation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>kwds</code> <p>Additional keyword arguments to pass to the model (model must request these in its keyword args).</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>The output data from the model, formatted as a <code>dict</code> with a key for each output variable and a corresponding value that is an array of the output data.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def call_model(self, inputs: dict | Dataset,\n               model_fidelity: Literal['best', 'worst'] | tuple | list = None,\n               output_path: str | Path = None,\n               executor: Executor = None,\n               track_costs: bool = False,\n               **kwds) -&gt; Dataset:\n    \"\"\"Wrapper function for calling the underlying component model.\n\n    This function formats the input data, calls the model, and processes the output data.\n    It supports vectorized calls, parallel execution using an executor, or serial execution. These options are\n    checked in that order, with the first available method used. Must set `Component.vectorized=True` if the\n    model supports input arrays of the form `(N,)` or even arbitrary shape `(...,)`.\n\n    !!! Warning \"Parallel Execution\"\n        The underlying model must be defined in a global module scope if `pickle` is the serialization method for\n        the provided `Executor`.\n\n    !!! Note \"Additional return values\"\n        The model can return additional items that are not part of `Component.outputs`. These items are returned\n        as object arrays in the output `dict`. Two special return values are `model_cost` and `output_path`.\n        Returning `model_cost` will store the computational cost of a single model evaluation (which is used by\n        `amisc` adaptive surrogate training). Returning `output_path` will store the output file name if the model\n        wrote any files to disk.\n\n    !!! Note \"Handling errors\"\n        If the underlying component model raises an exception, the error is stored in `output_dict['errors']` with\n        the index of the input data that caused the error. The output data for that index is set to `np.nan`\n        for each output variable.\n\n    :param inputs: The input data for the model, formatted as a `dict` with a key for each input variable and\n                   a corresponding value that is an array of the input data. If specified as a plain list, then the\n                   order is assumed the same as `Component.inputs`.\n    :param model_fidelity: Fidelity indices to tune the model fidelity (model must request this\n                           in its keyword arguments).\n    :param output_path: Directory to save model output files (model must request this in its keyword arguments).\n    :param executor: Executor for parallel execution if the model is not vectorized (optional).\n    :param track_costs: Whether to track the computational cost of each model evaluation.\n    :param kwds: Additional keyword arguments to pass to the model (model must request these in its keyword args).\n    :returns: The output data from the model, formatted as a `dict` with a key for each output variable and a\n              corresponding value that is an array of the output data.\n    \"\"\"\n    # Format inputs to a common loop shape (fail if missing any)\n    if len(inputs) == 0:\n        return {}  # your fault\n    if isinstance(inputs, list | np.ndarray):\n        inputs = np.atleast_1d(inputs)\n        inputs = {var.name: inputs[..., i] for i, var in enumerate(self.inputs)}\n\n    var_shape = {}\n    for var in self.inputs:\n        s = None\n        if (arr := kwds.get(f'{var.name}{COORDS_STR_ID}')) is not None:\n            if not np.issubdtype(arr.dtype, np.object_):  # if not object array, then it's a single coordinate set\n                s = arr.shape if len(arr.shape) == 1 else arr.shape[:-1]  # skip the coordinate dim (last axis)\n        if var.compression is not None:\n            for field in var.compression.fields:\n                var_shape[field] = s\n        else:\n            var_shape[var.name] = s\n    inputs, loop_shape = format_inputs(inputs, var_shape=var_shape)\n\n    N = int(np.prod(loop_shape))\n    list_alpha = isinstance(model_fidelity, list | np.ndarray)\n    alpha_requested = self.model_kwarg_requested('model_fidelity')\n    for var in self.inputs:\n        if var.compression is not None:\n            for field in var.compression.fields:\n                if field not in inputs:\n                    raise ValueError(f\"Missing field '{field}' for input variable '{var}'.\")\n        elif var.name not in inputs:\n            raise ValueError(f\"Missing input variable '{var.name}'.\")\n\n    # Pass extra requested items to the model kwargs\n    kwargs = copy.deepcopy(self.model_kwargs.data)\n    if self.model_kwarg_requested('output_path'):\n        kwargs['output_path'] = output_path\n    if self.model_kwarg_requested('input_vars'):\n        kwargs['input_vars'] = self.inputs\n    if self.model_kwarg_requested('output_vars'):\n        kwargs['output_vars'] = self.outputs\n    if alpha_requested:\n        if not list_alpha:\n            model_fidelity = [model_fidelity] * N\n        for i in range(N):\n            if model_fidelity[i] == 'best':\n                model_fidelity[i] = self.max_alpha\n            elif model_fidelity[i] == 'worst':\n                model_fidelity[i] = (0,) * len(self.model_fidelity)\n\n    for k, v in kwds.items():\n        if self.model_kwarg_requested(k):\n            kwargs[k] = v\n\n    # Compute model (vectorized, executor parallel, or serial)\n    errors = {}\n    if self.vectorized:\n        if alpha_requested:\n            kwargs['model_fidelity'] = np.atleast_1d(model_fidelity).reshape((N, -1))\n\n        self._model_start_time = time.time()\n        output_dict = self.model(*[inputs[var.name] for var in self.inputs], **kwargs) if self.call_unpacked \\\n            else self.model(inputs, **kwargs)\n        self._model_end_time = time.time()\n\n        if self.ret_unpacked:\n            output_dict = (output_dict,) if not isinstance(output_dict, tuple) else output_dict\n            output_dict = {out_var.name: output_dict[i] for i, out_var in enumerate(self.outputs)}\n    else:\n        self._model_start_time = time.time()\n        if executor is None:  # Serial\n            results = deque(maxlen=N)\n            for i in range(N):\n                try:\n                    if alpha_requested:\n                        kwargs['model_fidelity'] = model_fidelity[i]\n                    ret = self.model(*[{k: v[i] for k, v in inputs.items()}[var.name] for var in self.inputs],\n                                     **kwargs) if self.call_unpacked else (\n                        self.model({k: v[i] for k, v in inputs.items()}, **kwargs))\n                    if self.ret_unpacked:\n                        ret = (ret,) if not isinstance(ret, tuple) else ret\n                        ret = {out_var.name: ret[i] for i, out_var in enumerate(self.outputs)}\n                    results.append(ret)\n                except Exception:\n                    results.append({'inputs': {k: v[i] for k, v in inputs.items()}, 'index': i,\n                                    'model_kwargs': kwargs.copy(), 'error': traceback.format_exc()})\n        else:  # Parallel\n            results = deque(maxlen=N)\n            futures = []\n            for i in range(N):\n                if alpha_requested:\n                    kwargs['model_fidelity'] = model_fidelity[i]\n                fs = executor.submit(self.model,\n                                     *[{k: v[i] for k, v in inputs.items()}[var.name] for var in self.inputs],\n                                     **kwargs) if self.call_unpacked else (\n                    executor.submit(self.model, {k: v[i] for k, v in inputs.items()}, **kwargs))\n                futures.append(fs)\n            wait(futures, timeout=None, return_when=ALL_COMPLETED)\n\n            for i, fs in enumerate(futures):\n                try:\n                    if alpha_requested:\n                        kwargs['model_fidelity'] = model_fidelity[i]\n                    ret = fs.result()\n                    if self.ret_unpacked:\n                        ret = (ret,) if not isinstance(ret, tuple) else ret\n                        ret = {out_var.name: ret[i] for i, out_var in enumerate(self.outputs)}\n                    results.append(ret)\n                except Exception:\n                    results.append({'inputs': {k: v[i] for k, v in inputs.items()}, 'index': i,\n                                    'model_kwargs': kwargs.copy(), 'error': traceback.format_exc()})\n        self._model_end_time = time.time()\n\n        # Collect parallel/serial results\n        output_dict = {}\n        for i in range(N):\n            res = results.popleft()\n            if 'error' in res:\n                errors[i] = res\n            else:\n                for key, val in res.items():\n                    # Save this component's variables\n                    is_component_var = False\n                    for var in self.outputs:\n                        if var.compression is not None:  # field quantity return values (save as object arrays)\n                            if key in var.compression.fields or key == f'{var}{COORDS_STR_ID}':\n                                if output_dict.get(key) is None:\n                                    output_dict.setdefault(key, np.full((N,), None, dtype=object))\n                                output_dict[key][i] = np.atleast_1d(val)\n                                is_component_var = True\n                                break\n                        elif key == var:\n                            if output_dict.get(key) is None:\n                                output_dict.setdefault(key, np.full((N, *np.atleast_1d(val).shape), np.nan))\n                            output_dict[key][i, ...] = np.atleast_1d(val)\n                            is_component_var = True\n                            break\n\n                    # Otherwise, save other objects\n                    if not is_component_var:\n                        # Save singleton numeric values as numeric arrays (model costs, etc.)\n                        _val = np.atleast_1d(val)\n                        if key == 'model_cost' or (np.issubdtype(_val.dtype, np.number)\n                                                   and len(_val.shape) == 1 and _val.shape[0] == 1):\n                            if output_dict.get(key) is None:\n                                output_dict.setdefault(key, np.full((N,), np.nan))\n                            output_dict[key][i] = _val[0]\n                        else:\n                            # Otherwise save into a generic object array\n                            if output_dict.get(key) is None:\n                                output_dict.setdefault(key, np.full((N,), None, dtype=object))\n                            output_dict[key][i] = val\n\n    # Save average model costs for each alpha fidelity\n    if track_costs:\n        if model_fidelity is not None and output_dict.get('model_cost') is not None:\n            alpha_costs = {}\n            for i, cost in enumerate(output_dict['model_cost']):\n                alpha_costs.setdefault(MultiIndex(model_fidelity[i]), [])\n                alpha_costs[MultiIndex(model_fidelity[i])].append(cost)\n            for a, costs in alpha_costs.items():\n                self.model_evals.setdefault(a, 0)\n                self.model_costs.setdefault(a, 0.0)\n                num_evals_prev = self.model_evals.get(a)\n                num_evals_new = len(costs)\n                prev_avg = self.model_costs.get(a)\n                costs = np.nan_to_num(costs, nan=prev_avg)\n                new_avg = (np.sum(costs) + prev_avg * num_evals_prev) / (num_evals_prev + num_evals_new)\n                self.model_evals[a] += num_evals_new\n                self.model_costs[a] = float(new_avg)\n\n    # Reshape loop dimensions to match the original input shape\n    output_dict = format_outputs(output_dict, loop_shape)\n\n    for var in self.outputs:\n        if var.compression is not None:\n            for field in var.compression.fields:\n                if field not in output_dict:\n                    self.logger.warning(f\"Model return missing field '{field}' for output variable '{var}'. \"\n                                        f\"This may indicate an error during model evaluation. Returning NaNs...\")\n                    output_dict.setdefault(field, np.full((N,), np.nan))\n        elif var.name not in output_dict:\n            self.logger.warning(f\"Model return missing output variable '{var.name}'. This may indicate \"\n                                f\"an error during model evaluation. Returning NaNs...\")\n            output_dict[var.name] = np.full((N,), np.nan)\n\n    # Return the output dictionary and any errors\n    if errors:\n        output_dict['errors'] = errors\n    return output_dict\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.clear","title":"<code>clear()</code>","text":"<p>Clear the component of all training data, index sets, and MISC states.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def clear(self):\n    \"\"\"Clear the component of all training data, index sets, and MISC states.\"\"\"\n    self.active_set.clear()\n    self.candidate_set.clear()\n    self.misc_states.clear()\n    self.misc_costs.clear()\n    self.misc_coeff_train.clear()\n    self.misc_coeff_test.clear()\n    self.model_costs.clear()\n    self.model_evals.clear()\n    self.training_data.clear()\n    self._model_start_time = -1.0\n    self._model_end_time = -1.0\n    self.clear_cache()\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear cached data.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear cached data.\"\"\"\n    self._cache.clear()\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.deserialize","title":"<code>deserialize(serialized_data, search_paths=None, search_keys=None)</code>  <code>classmethod</code>","text":"<p>Return a <code>Component</code> from <code>data</code>. Let pydantic handle field validation and conversion. If any component data has been saved to file and the save file doesn't exist, then the loader will search for the file in the current working directory and any additional search paths provided.</p> PARAMETER DESCRIPTION <code>serialized_data</code> <p>the serialized data to construct the object from</p> <p> TYPE: <code>dict</code> </p> <code>search_paths</code> <p>paths to try and find any save files (i.e. if they moved since they were serialized), will always search in the current working directory by default</p> <p> TYPE: <code>list[str | Path]</code> DEFAULT: <code>None</code> </p> <code>search_keys</code> <p>keys to search for save files in each component (default is all keys in <code>ComponentSerializers</code>, in addition to variable inputs and outputs)</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>@classmethod\ndef deserialize(cls, serialized_data: dict, search_paths: list[str | Path] = None,\n                search_keys: list[str] = None) -&gt; Component:\n    \"\"\"Return a `Component` from `data`. Let pydantic handle field validation and conversion. If any component\n    data has been saved to file and the save file doesn't exist, then the loader will search for the file\n    in the current working directory and any additional search paths provided.\n\n    :param serialized_data: the serialized data to construct the object from\n    :param search_paths: paths to try and find any save files (i.e. if they moved since they were serialized),\n                         will always search in the current working directory by default\n    :param search_keys: keys to search for save files in each component (default is all keys in\n                        [`ComponentSerializers`][amisc.component.ComponentSerializers], in addition to variable\n                        inputs and outputs)\n    \"\"\"\n    if isinstance(serialized_data, Component):\n        return serialized_data\n    elif callable(serialized_data):\n        # try to construct a component from a raw function (assume data fidelity is (2,) for each inspected input)\n        return cls(serialized_data, data_fidelity=(2,) * len(_inspect_function(serialized_data)[0]))\n\n    search_paths = search_paths or []\n    search_keys = search_keys or []\n    search_keys.extend(ComponentSerializers.__annotations__.keys())\n    comp = serialized_data\n\n    for key in search_keys:\n        if (filename := comp.get(key, None)) is not None:\n            comp[key] = search_for_file(filename, search_paths=search_paths)  # will ret original str if not found\n\n    for key in ['inputs', 'outputs']:\n        for var in comp.get(key, []):\n            if isinstance(var, dict):\n                if (compression := var.get('compression', None)) is not None:\n                    var['compression'] = search_for_file(compression, search_paths=search_paths)\n\n    return cls(**comp)\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.get_cost","title":"<code>get_cost(alpha, beta)</code>","text":"<p>Return the total cost (i.e. number of model evaluations) required to add \\((\\alpha, \\beta)\\) to the MISC approximation.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>A multi-index specifying model fidelity</p> <p> TYPE: <code>MultiIndex</code> </p> <code>beta</code> <p>A multi-index specifying surrogate fidelity</p> <p> TYPE: <code>MultiIndex</code> </p> RETURNS DESCRIPTION <code>int</code> <p>the total number of model evaluations required for adding this multi-index to the MISC approximation</p> Source code in <code>src/amisc/component.py</code> <pre><code>def get_cost(self, alpha: MultiIndex, beta: MultiIndex) -&gt; int:\n    \"\"\"Return the total cost (i.e. number of model evaluations) required to add $(\\\\alpha, \\\\beta)$ to the\n    MISC approximation.\n\n    :param alpha: A multi-index specifying model fidelity\n    :param beta: A multi-index specifying surrogate fidelity\n    :returns: the total number of model evaluations required for adding this multi-index to the MISC approximation\n    \"\"\"\n    try:\n        return self.misc_costs[alpha, beta]\n    except Exception:\n        return 0\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.get_model_timestamps","title":"<code>get_model_timestamps()</code>","text":"<p>Return a tuple with the (start, end) timestamps for the most recent call to <code>call_model</code>. This is useful for tracking the duration of model evaluations. Will return (None, None) if no model has been called.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def get_model_timestamps(self):\n    \"\"\"Return a tuple with the (start, end) timestamps for the most recent call to `call_model`. This\n    is useful for tracking the duration of model evaluations. Will return (None, None) if no model has been called.\n    \"\"\"\n    if self._model_start_time &lt; 0 or self._model_end_time &lt; 0:\n        return None, None\n    else:\n        return self._model_start_time, self._model_end_time\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.get_training_data","title":"<code>get_training_data(alpha='best', beta='best', y_vars=None, cached=False)</code>","text":"<p>Get all training data for a given multi-index pair <code>(alpha, beta)</code>.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>the model fidelity index (defaults to the maximum available model fidelity)</p> <p> TYPE: <code>Literal['best', 'worst'] | MultiIndex</code> DEFAULT: <code>'best'</code> </p> <code>beta</code> <p>the surrogate fidelity index (defaults to the maximum available surrogate fidelity)</p> <p> TYPE: <code>Literal['best', 'worst'] | MultiIndex</code> DEFAULT: <code>'best'</code> </p> <code>y_vars</code> <p>the training data to return (defaults to all stored data)</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>cached</code> <p>if True, will get cached training data if available (this will ignore <code>y_vars</code> and only grab whatever is in the cache, which is surrogate outputs only and no nans)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple[Dataset, Dataset]</code> <p><code>(xtrain, ytrain)</code> - the training data for the given multi-indices</p> Source code in <code>src/amisc/component.py</code> <pre><code>def get_training_data(self, alpha: Literal['best', 'worst'] | MultiIndex = 'best',\n                      beta: Literal['best', 'worst'] | MultiIndex = 'best',\n                      y_vars: list = None,\n                      cached: bool = False) -&gt; tuple[Dataset, Dataset]:\n    \"\"\"Get all training data for a given multi-index pair `(alpha, beta)`.\n\n    :param alpha: the model fidelity index (defaults to the maximum available model fidelity)\n    :param beta: the surrogate fidelity index (defaults to the maximum available surrogate fidelity)\n    :param y_vars: the training data to return (defaults to all stored data)\n    :param cached: if True, will get cached training data if available (this will ignore `y_vars` and\n                   only grab whatever is in the cache, which is surrogate outputs only and no nans)\n    :returns: `(xtrain, ytrain)` - the training data for the given multi-indices\n    \"\"\"\n    # Find the best alpha\n    if alpha == 'best':\n        alpha_best = ()\n        for a, _ in self.active_set.union(self.candidate_set):\n            if sum(a) &gt; sum(alpha_best):\n                alpha_best = a\n        alpha = alpha_best\n    elif alpha == 'worst':\n        alpha = (0,) * len(self.max_alpha)\n\n    # Find the best beta for the given alpha\n    if beta == 'best':\n        beta_best = ()\n        for a, b in self.active_set.union(self.candidate_set):\n            if a == alpha and sum(b) &gt; sum(beta_best):\n                beta_best = b\n        beta = beta_best\n    elif beta == 'worst':\n        beta = (0,) * len(self.max_beta)\n\n    try:\n        if cached and (data := self._cache.get(\"training\", {}).get(alpha, {}).get(beta)) is not None:\n            return data\n        else:\n            return self.training_data.get(alpha, beta[:len(self.data_fidelity)], y_vars=y_vars, skip_nan=True)\n    except Exception as e:\n        self.logger.error(f\"Error getting training data for alpha={alpha}, beta={beta}.\")\n        raise e\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.gradient","title":"<code>gradient(inputs, index_set='test', misc_coeff=None, derivative='first', executor=None)</code>","text":"<p>Evaluate the Jacobian or Hessian of the MISC surrogate approximation at new <code>inputs</code>, i.e. the first or second derivatives, respectively.</p> PARAMETER DESCRIPTION <code>inputs</code> <p><code>dict</code> of input arrays for each variable input</p> <p> TYPE: <code>dict | Dataset</code> </p> <code>index_set</code> <p>the active index set, defaults to <code>self.active_set</code> if <code>'train'</code> or both <code>self.active_set + self.candidate_set</code> if <code>'test'</code></p> <p> TYPE: <code>Literal['train', 'test'] | IndexSet</code> DEFAULT: <code>'test'</code> </p> <code>misc_coeff</code> <p>the data structure holding the MISC coefficients to use, which defaults to the training or testing coefficients depending on the <code>index_set</code> parameter.</p> <p> TYPE: <code>MiscTree</code> DEFAULT: <code>None</code> </p> <code>derivative</code> <p>whether to compute the first or second derivative (i.e. Jacobian or Hessian)</p> <p> TYPE: <code>Literal['first', 'second']</code> DEFAULT: <code>'first'</code> </p> <code>executor</code> <p>executor for looping over MISC coefficients (optional)</p> <p> TYPE: <code>Executor</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>a <code>dict</code> of the Jacobian or Hessian of the surrogate approximation for each output variable</p> Source code in <code>src/amisc/component.py</code> <pre><code>def gradient(self, inputs: dict | Dataset,\n             index_set: Literal['train', 'test'] | IndexSet = 'test',\n             misc_coeff: MiscTree = None,\n             derivative: Literal['first', 'second'] = 'first',\n             executor: Executor = None) -&gt; Dataset:\n    \"\"\"Evaluate the Jacobian or Hessian of the MISC surrogate approximation at new `inputs`, i.e.\n    the first or second derivatives, respectively.\n\n    :param inputs: `dict` of input arrays for each variable input\n    :param index_set: the active index set, defaults to `self.active_set` if `'train'` or both\n                      `self.active_set + self.candidate_set` if `'test'`\n    :param misc_coeff: the data structure holding the MISC coefficients to use, which defaults to the\n                       training or testing coefficients depending on the `index_set` parameter.\n    :param derivative: whether to compute the first or second derivative (i.e. Jacobian or Hessian)\n    :param executor: executor for looping over MISC coefficients (optional)\n    :returns: a `dict` of the Jacobian or Hessian of the surrogate approximation for each output variable\n    \"\"\"\n    if not self.has_surrogate:\n        self.logger.warning(\"No surrogate model available for gradient computation.\")\n        return None\n\n    index_set, misc_coeff = self._match_index_set(index_set, misc_coeff)\n    inputs, loop_shape = format_inputs(inputs)  # {'x': (N,)}\n    outputs = {}\n\n    if len(index_set) == 0:\n        for var in self.outputs:\n            outputs[var] = np.full(loop_shape, np.nan)\n        return outputs\n    y_vars = self._surrogate_outputs()\n\n    # Combination technique MISC gradient prediction\n    results = []\n    coeffs = []\n    for alpha, beta in index_set:\n        comb_coeff = misc_coeff[alpha, beta]\n        if np.abs(comb_coeff) &gt; 0:\n            coeffs.append(comb_coeff)\n            func = self.interpolator.gradient if derivative == 'first' else self.interpolator.hessian\n            args = (self.misc_states.get((alpha, beta)),\n                    self.get_training_data(alpha, beta, y_vars=y_vars, cached=True))\n\n            results.append(func(inputs, *args) if executor is None else executor.submit(func, inputs, *args))\n\n    if executor is not None:\n        wait(results, timeout=None, return_when=ALL_COMPLETED)\n        results = [future.result() for future in results]\n\n    for coeff, interp_pred in zip(coeffs, results):\n        for var, arr in interp_pred.items():\n            if outputs.get(var) is None:\n                outputs[str(var)] = coeff * arr\n            else:\n                outputs[str(var)] += coeff * arr\n\n    return format_outputs(outputs, loop_shape)\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.hessian","title":"<code>hessian(*args, **kwargs)</code>","text":"<p>Alias for <code>Component.gradient(*args, derivative='second', **kwargs)</code>.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def hessian(self, *args, **kwargs):\n    \"\"\"Alias for `Component.gradient(*args, derivative='second', **kwargs)`.\"\"\"\n    return self.gradient(*args, derivative='second', **kwargs)\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.is_downward_closed","title":"<code>is_downward_closed(indices)</code>  <code>staticmethod</code>","text":"<p>Return if a list of \\((\\alpha, \\beta)\\) multi-indices is downward-closed.</p> <p>MISC approximations require a downward-closed set in order to use the combination-technique formula for the coefficients (as implemented by <code>Component.update_misc_coeff()</code>).</p> <p>Example</p> <p>The list <code>[( (0,), (0,) ), ( (1,), (0,) ), ( (1,), (1,) )]</code> is downward-closed. You can visualize this as building a stack of cubes: in order to place a cube, all adjacent cubes must be present (does the logo make sense now?).</p> PARAMETER DESCRIPTION <code>indices</code> <p><code>IndexSet</code> of (<code>alpha</code>, <code>beta</code>) multi-indices</p> <p> TYPE: <code>IndexSet</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>whether the set of indices is downward-closed</p> Source code in <code>src/amisc/component.py</code> <pre><code>@staticmethod\ndef is_downward_closed(indices: IndexSet) -&gt; bool:\n    \"\"\"Return if a list of $(\\\\alpha, \\\\beta)$ multi-indices is downward-closed.\n\n    MISC approximations require a downward-closed set in order to use the combination-technique formula for the\n    coefficients (as implemented by `Component.update_misc_coeff()`).\n\n    !!! Example\n        The list `[( (0,), (0,) ), ( (1,), (0,) ), ( (1,), (1,) )]` is downward-closed. You can visualize this as\n        building a stack of cubes: in order to place a cube, all adjacent cubes must be present (does the logo\n        make sense now?).\n\n    :param indices: `IndexSet` of (`alpha`, `beta`) multi-indices\n    :returns: whether the set of indices is downward-closed\n    \"\"\"\n    # Iterate over every multi-index\n    for alpha, beta in indices:\n        # Every smaller multi-index must also be included in the indices list\n        sub_sets = [np.arange(tuple(alpha + beta)[i] + 1) for i in range(len(alpha) + len(beta))]\n        for ele in itertools.product(*sub_sets):\n            tup = (MultiIndex(ele[:len(alpha)]), MultiIndex(ele[len(alpha):]))\n            if tup not in indices:\n                return False\n    return True\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.model_kwarg_requested","title":"<code>model_kwarg_requested(kwarg_name)</code>","text":"<p>Return whether the underlying component model requested this <code>kwarg_name</code>. Special kwargs include:</p> <ul> <li><code>output_path</code> \u2014 a save directory created by <code>amisc</code> will be passed to the model for saving model output files.</li> <li><code>alpha</code> \u2014 a tuple or list of model fidelity indices will be passed to the model to adjust fidelity.</li> <li><code>input_vars</code> \u2014 a list of <code>Variable</code> objects will be passed to the model for input variable information.</li> <li><code>output_vars</code> \u2014 a list of <code>Variable</code> objects will be passed to the model for output variable information.</li> </ul> PARAMETER DESCRIPTION <code>kwarg_name</code> <p>the argument to check for in the underlying component model's function signature kwargs</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>whether the component model requests this <code>kwarg</code> argument</p> Source code in <code>src/amisc/component.py</code> <pre><code>def model_kwarg_requested(self, kwarg_name: str) -&gt; bool:\n    \"\"\"Return whether the underlying component model requested this `kwarg_name`. Special kwargs include:\n\n    - `output_path` \u2014 a save directory created by `amisc` will be passed to the model for saving model output files.\n    - `alpha` \u2014 a tuple or list of model fidelity indices will be passed to the model to adjust fidelity.\n    - `input_vars` \u2014 a list of `Variable` objects will be passed to the model for input variable information.\n    - `output_vars` \u2014 a list of `Variable` objects will be passed to the model for output variable information.\n\n    :param kwarg_name: the argument to check for in the underlying component model's function signature kwargs\n    :returns: whether the component model requests this `kwarg` argument\n    \"\"\"\n    signature = inspect.signature(self.model)\n    for param in signature.parameters.values():\n        if param.name == kwarg_name and param.default != param.empty:\n            return True\n    return False\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.predict","title":"<code>predict(inputs, use_model=None, model_dir=None, index_set='test', misc_coeff=None, incremental=False, executor=None, **kwds)</code>","text":"<p>Evaluate the MISC surrogate approximation at new inputs <code>x</code>.</p> <p>Using the underlying model</p> <p>By default this will predict the MISC surrogate approximation; all inputs are assumed to be in a compressed and normalized form. If the component does not have a surrogate (i.e. it is analytical), then the inputs will be converted to model form and the underlying model will be called in place. If you instead want to override the surrogate, passing <code>use_model</code> will call the underlying model directly. In that case, the inputs should be passed in already in model form (i.e. full fields, denormalized).</p> PARAMETER DESCRIPTION <code>inputs</code> <p><code>dict</code> of input arrays for each variable input</p> <p> TYPE: <code>dict | Dataset</code> </p> <code>use_model</code> <p>'best'=high-fidelity, 'worst'=low-fidelity, tuple=a specific <code>alpha</code>, None=surrogate (default)</p> <p> TYPE: <code>Literal['best', 'worst'] | tuple</code> DEFAULT: <code>None</code> </p> <code>model_dir</code> <p>directory to save output files if <code>use_model</code> is specified, ignored otherwise</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>index_set</code> <p>the active index set, defaults to <code>self.active_set</code> if <code>'train'</code> or both <code>self.active_set + self.candidate_set</code> if <code>'test'</code></p> <p> TYPE: <code>Literal['train', 'test'] | IndexSet</code> DEFAULT: <code>'test'</code> </p> <code>misc_coeff</code> <p>the data structure holding the MISC coefficients to use, which defaults to the training or testing coefficients depending on the <code>index_set</code> parameter.</p> <p> TYPE: <code>MiscTree</code> DEFAULT: <code>None</code> </p> <code>incremental</code> <p>a special flag to use if the provided <code>index_set</code> is an incremental update to the active index set. A temporary copy of the internal <code>misc_coeff</code> data structure will be updated and used to incorporate the new indices.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>executor</code> <p>executor for parallel execution if the model is not vectorized (optional), will use the executor for looping over MISC coefficients if evaluating the surrogate rather than the model</p> <p> TYPE: <code>Executor</code> DEFAULT: <code>None</code> </p> <code>kwds</code> <p>additional keyword arguments to pass to the model (if using the underlying model)</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>the surrogate approximation of the model (or the model return itself if <code>use_model</code>)</p> Source code in <code>src/amisc/component.py</code> <pre><code>def predict(self, inputs: dict | Dataset,\n            use_model: Literal['best', 'worst'] | tuple = None,\n            model_dir: str | Path = None,\n            index_set: Literal['train', 'test'] | IndexSet = 'test',\n            misc_coeff: MiscTree = None,\n            incremental: bool = False,\n            executor: Executor = None,\n            **kwds) -&gt; Dataset:\n    \"\"\"Evaluate the MISC surrogate approximation at new inputs `x`.\n\n    !!! Note \"Using the underlying model\"\n        By default this will predict the MISC surrogate approximation; all inputs are assumed to be in a compressed\n        and normalized form. If the component does not have a surrogate (i.e. it is analytical), then the inputs\n        will be converted to model form and the underlying model will be called in place. If you instead want to\n        override the surrogate, passing `use_model` will call the underlying model directly. In that case, the\n        inputs should be passed in already in model form (i.e. full fields, denormalized).\n\n    :param inputs: `dict` of input arrays for each variable input\n    :param use_model: 'best'=high-fidelity, 'worst'=low-fidelity, tuple=a specific `alpha`, None=surrogate (default)\n    :param model_dir: directory to save output files if `use_model` is specified, ignored otherwise\n    :param index_set: the active index set, defaults to `self.active_set` if `'train'` or both\n                      `self.active_set + self.candidate_set` if `'test'`\n    :param misc_coeff: the data structure holding the MISC coefficients to use, which defaults to the\n                       training or testing coefficients depending on the `index_set` parameter.\n    :param incremental: a special flag to use if the provided `index_set` is an incremental update to the active\n                        index set. A temporary copy of the internal `misc_coeff` data structure will be updated\n                        and used to incorporate the new indices.\n    :param executor: executor for parallel execution if the model is not vectorized (optional), will use the\n                     executor for looping over MISC coefficients if evaluating the surrogate rather than the model\n    :param kwds: additional keyword arguments to pass to the model (if using the underlying model)\n    :returns: the surrogate approximation of the model (or the model return itself if `use_model`)\n    \"\"\"\n    # Use raw model inputs/outputs\n    if use_model is not None:\n        outputs = self.call_model(inputs, model_fidelity=use_model, output_path=model_dir, executor=executor,**kwds)\n        return {str(var): outputs[var] for var in outputs}\n\n    # Convert inputs/outputs to/from model if no surrogate (i.e. analytical models)\n    if not self.has_surrogate:\n        field_coords = {f'{var}{COORDS_STR_ID}':\n                        self.model_kwargs.get(f'{var}{COORDS_STR_ID}', kwds.get(f'{var}{COORDS_STR_ID}', None))\n                        for var in self.inputs}\n        inputs, field_coords = to_model_dataset(inputs, self.inputs, del_latent=True, **field_coords)\n        field_coords.update(kwds)\n        outputs = self.call_model(inputs, model_fidelity=use_model or 'best', output_path=model_dir,\n                                  executor=executor, **field_coords)\n        outputs, _ = to_surrogate_dataset(outputs, self.outputs, del_fields=True, **field_coords)\n        return {str(var): outputs[var] for var in outputs}\n\n    # Choose the correct index set and misc_coeff data structures\n    if incremental:\n        misc_coeff = copy.deepcopy(self.misc_coeff_train)\n        self.update_misc_coeff(index_set, self.active_set, misc_coeff)\n        index_set = self.active_set.union(index_set)\n    else:\n        index_set, misc_coeff = self._match_index_set(index_set, misc_coeff)\n\n    # Format inputs for surrogate prediction (all scalars at this point, including latent coeffs)\n    inputs, loop_shape = format_inputs(inputs)  # {'x': (N,)}\n    outputs = {}\n\n    # Handle prediction with empty active set (return nan)\n    if len(index_set) == 0:\n        self.logger.warning(f\"Component '{self.name}' has an empty active set. \"\n                            f\"Has the surrogate been trained yet? Returning NaNs...\")\n        for var in self.outputs:\n            outputs[var.name] = np.full(loop_shape, np.nan)\n        return outputs\n\n    y_vars = self._surrogate_outputs()  # Only request this component's specified outputs (ignore all extras)\n\n    # Combination technique MISC surrogate prediction\n    results = []\n    coeffs = []\n    for alpha, beta in index_set:\n        comb_coeff = misc_coeff[alpha, beta]\n        if np.abs(comb_coeff) &gt; 0:\n            coeffs.append(comb_coeff)\n            args = (self.misc_states.get((alpha, beta)),\n                    self.get_training_data(alpha, beta, y_vars=y_vars, cached=True))\n\n            results.append(self.interpolator.predict(inputs, *args) if executor is None else\n                           executor.submit(self.interpolator.predict, inputs, *args))\n\n    if executor is not None:\n        wait(results, timeout=None, return_when=ALL_COMPLETED)\n        results = [future.result() for future in results]\n\n    for coeff, interp_pred in zip(coeffs, results):\n        for var, arr in interp_pred.items():\n            if outputs.get(var) is None:\n                outputs[str(var)] = coeff * arr\n            else:\n                outputs[str(var)] += coeff * arr\n\n    return format_outputs(outputs, loop_shape)\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.serialize","title":"<code>serialize(keep_yaml_objects=False, serialize_args=None, serialize_kwargs=None)</code>","text":"<p>Convert to a <code>dict</code> with only standard Python types as fields and values.</p> PARAMETER DESCRIPTION <code>keep_yaml_objects</code> <p>whether to keep <code>Variable</code> or other yaml serializable objects instead of also serializing them (default is False)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>serialize_args</code> <p>additional arguments to pass to the <code>serialize</code> method of each <code>Component</code> attribute; specify as a <code>dict</code> of attribute names to tuple of arguments to pass</p> <p> TYPE: <code>dict[str, tuple]</code> DEFAULT: <code>None</code> </p> <code>serialize_kwargs</code> <p>additional keyword arguments to pass to the <code>serialize</code> method of each <code>Component</code> attribute</p> <p> TYPE: <code>dict[str:dict]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a <code>dict</code> representation of the <code>Component</code> object</p> Source code in <code>src/amisc/component.py</code> <pre><code>def serialize(self, keep_yaml_objects: bool = False, serialize_args: dict[str, tuple] = None,\n              serialize_kwargs: dict[str: dict] = None) -&gt; dict:\n    \"\"\"Convert to a `dict` with only standard Python types as fields and values.\n\n    :param keep_yaml_objects: whether to keep `Variable` or other yaml serializable objects instead of\n                              also serializing them (default is False)\n    :param serialize_args: additional arguments to pass to the `serialize` method of each `Component` attribute;\n                           specify as a `dict` of attribute names to tuple of arguments to pass\n    :param serialize_kwargs: additional keyword arguments to pass to the `serialize` method of each\n                             `Component` attribute\n    :returns: a `dict` representation of the `Component` object\n    \"\"\"\n    serialize_args = serialize_args or dict()\n    serialize_kwargs = serialize_kwargs or dict()\n    d = {}\n    for key, value in self.__dict__.items():\n        if value is not None and not key.startswith('_'):\n            if key == 'serializers':\n                # Update the serializers\n                serializers = self._validate_serializers({k: type(getattr(self, k)) for k in value.keys()})\n                d[key] = {k: (v.obj if keep_yaml_objects else v.serialize()) for k, v in serializers.items()}\n            elif key in ['inputs', 'outputs'] and not keep_yaml_objects:\n                d[key] = value.serialize(**serialize_kwargs.get(key, {}))\n            elif key == 'model' and not keep_yaml_objects:\n                d[key] = YamlSerializable(obj=value).serialize()\n            elif key in ['data_fidelity', 'surrogate_fidelity', 'model_fidelity']:\n                if len(value) &gt; 0:\n                    d[key] = str(value)\n            elif key in ['active_set', 'candidate_set']:\n                if len(value) &gt; 0:\n                    d[key] = value.serialize()\n            elif key in ['misc_costs', 'misc_coeff_train', 'misc_coeff_test', 'misc_states']:\n                if len(value) &gt; 0:\n                    d[key] = value.serialize(keep_yaml_objects=keep_yaml_objects)\n            elif key in ['model_costs']:\n                if len(value) &gt; 0:\n                    d[key] = {str(k): float(v) for k, v in value.items()}\n            elif key in ['model_evals']:\n                if len(value) &gt; 0:\n                    d[key] = {str(k): int(v) for k, v in value.items()}\n            elif key in ComponentSerializers.__annotations__.keys():\n                if key in ['training_data', 'interpolator'] and not self.has_surrogate:\n                    continue\n                else:\n                    d[key] = value.serialize(*serialize_args.get(key, ()), **serialize_kwargs.get(key, {}))\n            else:\n                d[key] = value\n    return d\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.set_logger","title":"<code>set_logger(log_file=None, stdout=None, logger=None, level=logging.INFO)</code>","text":"<p>Set a new <code>logging.Logger</code> object.</p> PARAMETER DESCRIPTION <code>log_file</code> <p>log to file (if provided)</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>stdout</code> <p>whether to connect the logger to console (defaults to whatever is currently set or False)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>the logging object to use (if None, then a new logger is created; this will override the <code>log_file</code> and <code>stdout</code> arguments if set)</p> <p> TYPE: <code>Logger</code> DEFAULT: <code>None</code> </p> <code>level</code> <p>the logging level to set (default is <code>logging.INFO</code>)</p> <p> TYPE: <code>int</code> DEFAULT: <code>INFO</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def set_logger(self, log_file: str | Path = None, stdout: bool = None, logger: logging.Logger = None,\n               level: int = logging.INFO):\n    \"\"\"Set a new `logging.Logger` object.\n\n    :param log_file: log to file (if provided)\n    :param stdout: whether to connect the logger to console (defaults to whatever is currently set or False)\n    :param logger: the logging object to use (if None, then a new logger is created; this will override\n                   the `log_file` and `stdout` arguments if set)\n    :param level: the logging level to set (default is `logging.INFO`)\n    \"\"\"\n    if stdout is None:\n        stdout = False\n        if self._logger is not None:\n            for handler in self._logger.handlers:\n                if isinstance(handler, logging.StreamHandler):\n                    stdout = True\n                    break\n    self._logger = logger or get_logger(self.name, log_file=log_file, stdout=stdout, level=level)\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.update_misc_coeff","title":"<code>update_misc_coeff(new_indices, index_set='train', misc_coeff=None)</code>","text":"<p>Update MISC coefficients incrementally resulting from the addition of new indices to an index set.</p> <p>Incremental updates</p> <p>This function is used to update the MISC coefficients stored in <code>misc_coeff</code> after adding new indices to the given <code>index_set</code>. If a custom <code>index_set</code> or <code>misc_coeff</code> are provided, the user is responsible for ensuring the data structures are consistent. Since this is an incremental update, this means all existing coefficients for every index in <code>index_set</code> should be precomputed and stored in <code>misc_coeff</code>.</p> PARAMETER DESCRIPTION <code>new_indices</code> <p>a set of \\((\\alpha, \\beta)\\) tuples that are being added to the <code>index_set</code></p> <p> TYPE: <code>IndexSet</code> </p> <code>index_set</code> <p>the active index set, defaults to <code>self.active_set</code> if <code>'train'</code> or both <code>self.active_set + self.candidate_set</code> if <code>'test'</code></p> <p> TYPE: <code>Literal['test', 'train'] | IndexSet</code> DEFAULT: <code>'train'</code> </p> <code>misc_coeff</code> <p>the data structure holding the MISC coefficients to update, which defaults to the training or testing coefficients depending on the <code>index_set</code> parameter. This data structure is modified in place.</p> <p> TYPE: <code>MiscTree</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def update_misc_coeff(self, new_indices: IndexSet, index_set: Literal['test', 'train'] | IndexSet = 'train',\n                      misc_coeff: MiscTree = None):\n    \"\"\"Update MISC coefficients incrementally resulting from the addition of new indices to an index set.\n\n    !!! Warning \"Incremental updates\"\n        This function is used to update the MISC coefficients stored in `misc_coeff` after adding new indices\n        to the given `index_set`. If a custom `index_set` or `misc_coeff` are provided, the user is responsible\n        for ensuring the data structures are consistent. Since this is an incremental update, this means all\n        existing coefficients for every index in `index_set` should be precomputed and stored in `misc_coeff`.\n\n    :param new_indices: a set of $(\\\\alpha, \\\\beta)$ tuples that are being added to the `index_set`\n    :param index_set: the active index set, defaults to `self.active_set` if `'train'` or both\n                      `self.active_set + self.candidate_set` if `'test'`\n    :param misc_coeff: the data structure holding the MISC coefficients to update, which defaults to the\n                       training or testing coefficients depending on the `index_set` parameter. This data structure\n                       is modified in place.\n    \"\"\"\n    index_set, misc_coeff = self._match_index_set(index_set, misc_coeff)\n\n    for new_alpha, new_beta in new_indices:\n        new_ind = np.array(new_alpha + new_beta)\n\n        # Update all existing/new coefficients if they are a distance of [0, 1] \"below\" the new index\n        # Note that new indices can only be [0, 1] away from themselves -- not any other new indices\n        for old_alpha, old_beta in itertools.chain(index_set, [(new_alpha, new_beta)]):\n            old_ind = np.array(old_alpha + old_beta)\n            diff = new_ind - old_ind\n            if np.all(np.isin(diff, [0, 1])):\n                if misc_coeff.get((old_alpha, old_beta)) is None:\n                    misc_coeff[old_alpha, old_beta] = 0\n                misc_coeff[old_alpha, old_beta] += (-1) ** int(np.sum(np.abs(diff)))\n</code></pre>"},{"location":"reference/component/#amisc.component.Component.update_model","title":"<code>update_model(new_model=None, model_kwargs=None, **kwargs)</code>","text":"<p>Update the underlying component model or its kwargs.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def update_model(self, new_model: callable = None, model_kwargs: dict = None, **kwargs):\n    \"\"\"Update the underlying component model or its kwargs.\"\"\"\n    if new_model is not None:\n        self.model = new_model\n    new_kwargs = self.model_kwargs.data\n    new_kwargs.update(model_kwargs or {})\n    new_kwargs.update(kwargs)\n    self.model_kwargs = new_kwargs\n</code></pre>"},{"location":"reference/component/#amisc.component.ComponentSerializers","title":"<code>ComponentSerializers</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Type hint for the <code>Component</code> class data serializers.</p> ATTRIBUTE DESCRIPTION <code>model_kwargs</code> <p>the model kwarg object class</p> <p> TYPE: <code>str | type[Serializable] | YamlSerializable</code> </p> <code>interpolator</code> <p>the interpolator object class</p> <p> TYPE: <code>str | type[Serializable] | YamlSerializable</code> </p> <code>training_data</code> <p>the training data object class</p> <p> TYPE: <code>str | type[Serializable] | YamlSerializable</code> </p>"},{"location":"reference/component/#amisc.component.IndexSet","title":"<code>IndexSet(s=())</code>","text":"<p>               Bases: <code>set</code>, <code>Serializable</code></p> <p>Dataclass that maintains a list of multi-indices. Overrides basic <code>set</code> functionality to ensure elements are formatted correctly as <code>(alpha, beta)</code>; that is, as a tuple of <code>alpha</code> and <code>beta</code>, which are themselves instances of a <code>MultiIndex</code> tuple.</p> <p>An example index set</p> <p>\\(\\mathcal{I} = [(\\alpha, \\beta)_1 , (\\alpha, \\beta)_2, (\\alpha, \\beta)_3 , ...]\\) would be specified as <code>I = [((0, 0), (0, 0, 0)) , ((0, 1), (0, 1, 0)), ...]</code>.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def __init__(self, s=()):\n    s = [self._validate_element(ele) for ele in s]\n    super().__init__(s)\n</code></pre>"},{"location":"reference/component/#amisc.component.IndexSet.deserialize","title":"<code>deserialize(serialized_data)</code>  <code>classmethod</code>","text":"<p>Deserialize a list of tuples to an <code>IndexSet</code>.</p> Source code in <code>src/amisc/component.py</code> <pre><code>@classmethod\ndef deserialize(cls, serialized_data: list[str]) -&gt; IndexSet:\n    \"\"\"Deserialize a list of tuples to an `IndexSet`.\"\"\"\n    return cls(serialized_data)\n</code></pre>"},{"location":"reference/component/#amisc.component.IndexSet.serialize","title":"<code>serialize()</code>","text":"<p>Return a list of each multi-index in the set serialized to a string.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def serialize(self) -&gt; list[str]:\n    \"\"\"Return a list of each multi-index in the set serialized to a string.\"\"\"\n    return [str(ele) for ele in self]\n</code></pre>"},{"location":"reference/component/#amisc.component.MiscTree","title":"<code>MiscTree(data=None, **kwargs)</code>","text":"<p>               Bases: <code>UserDict</code>, <code>Serializable</code></p> <p>Dataclass that maintains MISC data in a <code>dict</code> tree, indexed by <code>alpha</code> and <code>beta</code>. Overrides basic <code>dict</code> functionality to ensure elements are formatted correctly as <code>(alpha, beta) -&gt; data</code>. Used to store MISC coefficients, model costs, and interpolator states.</p> <p>The underlying data structure is: <code>dict[MultiIndex, dict[MultiIndex, float | InterpolatorState]]</code>.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def __init__(self, data: dict = None, **kwargs):\n    data_dict = data or {}\n    if isinstance(data_dict, MiscTree):\n        data_dict = data_dict.data\n    data_dict.update(kwargs)\n    super().__init__(self._validate_data(data_dict))\n</code></pre>"},{"location":"reference/component/#amisc.component.MiscTree.clear","title":"<code>clear()</code>","text":"<p>Clear the <code>MiscTree</code> data.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def clear(self):\n    \"\"\"Clear the `MiscTree` data.\"\"\"\n    for key in list(self.data.keys()):\n        del self.data[key]\n</code></pre>"},{"location":"reference/component/#amisc.component.MiscTree.deserialize","title":"<code>deserialize(serialized_data)</code>  <code>classmethod</code>","text":"<p>Deserialize a <code>dict</code> to a <code>MiscTree</code>.</p> PARAMETER DESCRIPTION <code>serialized_data</code> <p>the data to deserialize to a <code>MiscTree</code> object</p> <p> TYPE: <code>dict</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>@classmethod\ndef deserialize(cls, serialized_data: dict) -&gt; MiscTree:\n    \"\"\"Deserialize a `dict` to a `MiscTree`.\n\n    :param serialized_data: the data to deserialize to a `MiscTree` object\n    \"\"\"\n    return cls(serialized_data)\n</code></pre>"},{"location":"reference/component/#amisc.component.MiscTree.serialize","title":"<code>serialize(*args, keep_yaml_objects=False, **kwargs)</code>","text":"<p>Serialize <code>alpha, beta</code> indices to string and return a <code>dict</code> of internal data.</p> PARAMETER DESCRIPTION <code>args</code> <p>extra serialization arguments for internal <code>InterpolatorState</code></p> <p> DEFAULT: <code>()</code> </p> <code>keep_yaml_objects</code> <p>whether to keep <code>YamlSerializable</code> instances in the serialization</p> <p> DEFAULT: <code>False</code> </p> <code>kwargs</code> <p>extra serialization keyword arguments for internal <code>InterpolatorState</code></p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>src/amisc/component.py</code> <pre><code>def serialize(self, *args, keep_yaml_objects=False, **kwargs) -&gt; dict:\n    \"\"\"Serialize `alpha, beta` indices to string and return a `dict` of internal data.\n\n    :param args: extra serialization arguments for internal `InterpolatorState`\n    :param keep_yaml_objects: whether to keep `YamlSerializable` instances in the serialization\n    :param kwargs: extra serialization keyword arguments for internal `InterpolatorState`\n    \"\"\"\n    ret_dict = {}\n    if state_serializer := self.state_serializer(self.data):\n        ret_dict[self.SERIALIZER_KEY] = state_serializer.obj if keep_yaml_objects else state_serializer.serialize()\n    for alpha, beta, data in self:\n        ret_dict.setdefault(str(alpha), dict())\n        serialized_data = data.serialize(*args, **kwargs) if isinstance(data, InterpolatorState) else float(data)\n        ret_dict[str(alpha)][str(beta)] = serialized_data\n    return ret_dict\n</code></pre>"},{"location":"reference/component/#amisc.component.MiscTree.state_serializer","title":"<code>state_serializer(data)</code>  <code>classmethod</code>","text":"<p>Infer and return the interpolator state serializer from the <code>MiscTree</code> data (if possible). If no <code>InterpolatorState</code> instance could be found, return <code>None</code>.</p> Source code in <code>src/amisc/component.py</code> <pre><code>@classmethod\ndef state_serializer(cls, data: dict) -&gt; YamlSerializable | None:\n    \"\"\"Infer and return the interpolator state serializer from the `MiscTree` data (if possible). If no\n    `InterpolatorState` instance could be found, return `None`.\n    \"\"\"\n    serializer = data.get(cls.SERIALIZER_KEY, None)  # if `data` is serialized\n    if serializer is None:  # Otherwise search for an InterpolatorState\n        for alpha, beta_dict in data.items():\n            if alpha == cls.SERIALIZER_KEY:\n                continue\n            for beta, value in beta_dict.items():\n                if isinstance(value, InterpolatorState):\n                    serializer = type(value)\n                    break\n            if serializer is not None:\n                break\n    return cls._validate_state_serializer(serializer)\n</code></pre>"},{"location":"reference/component/#amisc.component.MiscTree.update","title":"<code>update(data_dict=None, **kwargs)</code>","text":"<p>Force <code>dict.update()</code> through the validator.</p> Source code in <code>src/amisc/component.py</code> <pre><code>def update(self, data_dict: dict = None, **kwargs):\n    \"\"\"Force `dict.update()` through the validator.\"\"\"\n    data_dict = data_dict or dict()\n    data_dict.update(kwargs)\n    super().update(self._validate_data(data_dict))\n</code></pre>"},{"location":"reference/component/#amisc.component.ModelKwargs","title":"<code>ModelKwargs</code>","text":"<p>               Bases: <code>UserDict</code>, <code>Serializable</code></p> <p>Default dataclass for storing model keyword arguments in a <code>dict</code>. If you have kwargs that require more complicated serialization/specification than a plain <code>dict</code>, then you can subclass from here.</p>"},{"location":"reference/component/#amisc.component.ModelKwargs.from_dict","title":"<code>from_dict(config)</code>  <code>classmethod</code>","text":"<p>Create a <code>ModelKwargs</code> object from a <code>dict</code> configuration.</p> Source code in <code>src/amisc/component.py</code> <pre><code>@classmethod\ndef from_dict(cls, config: dict) -&gt; ModelKwargs:\n    \"\"\"Create a `ModelKwargs` object from a `dict` configuration.\"\"\"\n    method = config.pop('method', 'default_kwargs').lower()\n    match method:\n        case 'default_kwargs':\n            return ModelKwargs(**config)\n        case 'string_kwargs':\n            return StringKwargs(**config)\n        case other:\n            config['method'] = other\n            return ModelKwargs(**config)  # Pass the method through\n</code></pre>"},{"location":"reference/component/#amisc.component.StringKwargs","title":"<code>StringKwargs</code>","text":"<p>               Bases: <code>StringSerializable</code>, <code>ModelKwargs</code></p> <p>Dataclass for storing model keyword arguments as a string.</p>"},{"location":"reference/compression/","title":"compression","text":""},{"location":"reference/compression/#amisc.compression","title":"<code>amisc.compression</code>","text":"<p>Module for compression methods.</p> <p>Especially useful for field quantities with high dimensions.</p> <p>Includes:</p> <ul> <li><code>Compression</code> \u2014 an interface for specifying a compression method for field quantities.</li> <li><code>SVD</code> \u2014 a Singular Value Decomposition (SVD) compression method.</li> </ul>"},{"location":"reference/compression/#amisc.compression.Compression","title":"<code>Compression(fields=list(), method='svd', coords=None, interpolate_method='rbf', interpolate_opts=dict(), _map_computed=False)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>PickleSerializable</code>, <code>ABC</code></p> <p>Base class for compression methods. Compression methods should:</p> <ul> <li><code>compute_map</code> - compute the compression map from provided data</li> <li><code>compress</code> - compress data into a latent space</li> <li><code>reconstruct</code> - reconstruct the compressed data back into the full space</li> <li><code>latent_size</code> - return the size of the latent space</li> <li><code>estimate_latent_ranges</code> - estimate the range of the latent space coefficients</li> </ul> <p>Specifying fields</p> <p>The <code>fields</code> attribute is a list of strings that specify the field quantities to compress. For example, for 3D velocity data, the fields might be <code>['ux', 'uy', 'uz']</code>. The length of the <code>fields</code> attribute is used to determine the number of quantities of interest at each grid point in <code>coords</code>. Note that interpolation to/from the compression grid will assume a shape of <code>(num_pts, num_qoi)</code> for the states on the grid, where <code>num_qoi</code> is the length of <code>fields</code> and <code>num_pts</code> is the length of <code>coords</code>. When constructing the compression map, this important fact should be considered when passing data to <code>compute_map</code>.</p> <p>In order to use a <code>Compression</code> object, you must first call <code>compute_map</code> to compute the compression map, which should set the private value <code>self._map_computed=True</code>. The <code>coords</code> of the compression grid must also be specified. The <code>coords</code> should have the shape <code>(num_pts, dim)</code> where <code>num_pts</code> is the number of points in the compression grid and <code>dim</code> is the number of spatial dimensions. If <code>coords</code> is a 1d array, then the <code>dim</code> is assumed to be 1.</p> ATTRIBUTE DESCRIPTION <code>fields</code> <p>list of field quantities to compress</p> <p> TYPE: <code>list[str]</code> </p> <code>method</code> <p>the compression method to use (only svd is supported for now)</p> <p> TYPE: <code>str</code> </p> <code>coords</code> <p>the coordinates of the compression grid</p> <p> TYPE: <code>ndarray</code> </p> <code>interpolate_method</code> <p>the interpolation method to use to interpolate to/from the compression grid (only <code>rbf</code> (i.e. radial basis function) is supported for now)</p> <p> TYPE: <code>str</code> </p> <code>interpolate_opts</code> <p>additional options to pass to the interpolation method</p> <p> TYPE: <code>dict</code> </p> <code>_map_computed</code> <p>whether the compression map has been computed</p> <p> TYPE: <code>bool</code> </p>"},{"location":"reference/compression/#amisc.compression.Compression.dim","title":"<code>dim</code>  <code>property</code>","text":"<p>Number of physical grid coordinates for the field quantity, (i.e. x,y,z spatial dims)</p>"},{"location":"reference/compression/#amisc.compression.Compression.dof","title":"<code>dof</code>  <code>property</code>","text":"<p>Total degrees of freedom in the compression grid (i.e. <code>num_pts * num_qoi</code>).</p>"},{"location":"reference/compression/#amisc.compression.Compression.map_exists","title":"<code>map_exists</code>  <code>property</code>","text":"<p>All compression methods should have <code>coords</code> when their map has been constructed.</p>"},{"location":"reference/compression/#amisc.compression.Compression.num_pts","title":"<code>num_pts</code>  <code>property</code>","text":"<p>Number of physical points in the compression grid.</p>"},{"location":"reference/compression/#amisc.compression.Compression.num_qoi","title":"<code>num_qoi</code>  <code>property</code>","text":"<p>Number of quantities of interest at each grid point, (i.e. <code>ux, uy, uz</code> for 3d velocity data).</p>"},{"location":"reference/compression/#amisc.compression.Compression.compress","title":"<code>compress(data)</code>  <code>abstractmethod</code>","text":"<p>Compress the data into a latent space.</p> PARAMETER DESCRIPTION <code>data</code> <p><code>(..., dof)</code> - the data to compress from full size of <code>dof</code></p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., rank)</code> - the compressed latent space data with size <code>rank</code></p> Source code in <code>src/amisc/compression.py</code> <pre><code>@abstractmethod\ndef compress(self, data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compress the data into a latent space.\n\n    :param data: `(..., dof)` - the data to compress from full size of `dof`\n    :return: `(..., rank)` - the compressed latent space data with size `rank`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/compression/#amisc.compression.Compression.compute_map","title":"<code>compute_map(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Compute and store the compression map. Must set the value of <code>coords</code> and <code>_is_computed</code>. Should use the same normalization as the parent <code>Variable</code> object.</p> <p>Note</p> <p>You should pass any required data to <code>compute_map</code> with the assumption that the data will be used in the shape <code>(num_pts, num_qoi)</code> where <code>num_qoi</code> is the length of <code>fields</code> and <code>num_pts</code> is the length of <code>coords</code>. This is the shape that the compression map should be constructed in.</p> Source code in <code>src/amisc/compression.py</code> <pre><code>@abstractmethod\ndef compute_map(self, **kwargs):\n    \"\"\"Compute and store the compression map. Must set the value of `coords` and `_is_computed`. Should\n    use the same normalization as the parent `Variable` object.\n\n    !!! Note\n        You should pass any required data to `compute_map` with the assumption that the data will be used in the\n        shape `(num_pts, num_qoi)` where `num_qoi` is the length of `fields` and `num_pts` is the length of\n        `coords`. This is the shape that the compression map should be constructed in.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/compression/#amisc.compression.Compression.estimate_latent_ranges","title":"<code>estimate_latent_ranges()</code>  <code>abstractmethod</code>","text":"<p>Estimate the range of the latent space coefficients.</p> Source code in <code>src/amisc/compression.py</code> <pre><code>@abstractmethod\ndef estimate_latent_ranges(self) -&gt; list[tuple[float, float]]:\n    \"\"\"Estimate the range of the latent space coefficients.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/compression/#amisc.compression.Compression.from_dict","title":"<code>from_dict(spec)</code>  <code>classmethod</code>","text":"<p>Construct a <code>Compression</code> object from a spec dictionary.</p> Source code in <code>src/amisc/compression.py</code> <pre><code>@classmethod\ndef from_dict(cls, spec: dict) -&gt; Compression:\n    \"\"\"Construct a `Compression` object from a spec dictionary.\"\"\"\n    method = spec.pop('method', 'svd').lower()\n    match method:\n        case 'svd':\n            return SVD(**spec)\n        case other:\n            raise NotImplementedError(f\"Compression method '{other}' is not implemented.\")\n</code></pre>"},{"location":"reference/compression/#amisc.compression.Compression.interpolate_from_grid","title":"<code>interpolate_from_grid(states, new_coords)</code>","text":"<p>Interpolate the states on the compression grid to new coordinates.</p> PARAMETER DESCRIPTION <code>states</code> <p><code>(*loop_shape, dof)</code> - the states on the compression grid</p> <p> TYPE: <code>ndarray</code> </p> <code>new_coords</code> <p><code>(*coord_shape, dim)</code> - the new coordinates to interpolate to; if a 1d object array, then each element is assumed to be a unique <code>(*coord_shape, dim)</code> array with assumed same length as loop_shape of the states -- will interpolate each state to the corresponding new coordinates</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <p><code>dict</code> of <code>(*loop_shape, *coord_shape)</code> for each qoi - the interpolated states; will return a single 1d object array for each qoi if new_coords is a 1d object array</p> Source code in <code>src/amisc/compression.py</code> <pre><code>def interpolate_from_grid(self, states: np.ndarray, new_coords: np.ndarray):\n    \"\"\"Interpolate the states on the compression grid to new coordinates.\n\n    :param states: `(*loop_shape, dof)` - the states on the compression grid\n    :param new_coords: `(*coord_shape, dim)` - the new coordinates to interpolate to; if a 1d object array, then\n                        each element is assumed to be a unique `(*coord_shape, dim)` array with assumed\n                        same length as loop_shape of the states -- will interpolate each state to the\n                        corresponding new coordinates\n    :return: `dict` of `(*loop_shape, *coord_shape)` for each qoi - the interpolated states; will return a\n             single 1d object array for each qoi if new_coords is a 1d object array\n    \"\"\"\n    new_coords = self._correct_coords(new_coords)\n    grid_coords = self._correct_coords(self.coords)\n\n    coords_obj_array = np.issubdtype(new_coords.dtype, np.object_)\n\n    # Iterate over one set of coords and states at a time\n    def _iterate_coords_and_states():\n        if coords_obj_array:\n            for index, c in np.ndenumerate(new_coords):  # assumes same number of coords and states\n                yield index, c, states[index]\n        else:\n            yield (0,), new_coords, states  # assumes same coords for all states\n\n    all_qois = np.empty(new_coords.shape if coords_obj_array else (1,), dtype=object)\n\n    # Do interpolation for each set of unique coordinates (if multiple)\n    for j, n_coords, state in _iterate_coords_and_states():\n        if n_coords is None:  # Skip empty coords\n            continue\n\n        skip_interp = (n_coords.shape == grid_coords.shape and np.allclose(n_coords, grid_coords))\n\n        ret_dict = {}\n        loop_shape = state.shape[:-1]\n        coords_shape = n_coords.shape[:-1]\n        state = state.reshape((*loop_shape, self.num_pts, self.num_qoi))\n        n_coords = n_coords.reshape((-1, self.dim))\n        for i, qoi in enumerate(self.fields):\n            if skip_interp:\n                ret_dict[qoi] = state[..., i]\n            else:\n                reshaped_states = state[..., i].reshape(-1, self.num_pts).T  # (num_pts, ...)\n                interp = self.interpolator()(grid_coords, reshaped_states, **self.interpolate_opts)\n                yp = interp(n_coords)\n                ret_dict[qoi] = yp.T.reshape(*loop_shape, *coords_shape)\n\n        all_qois[j] = ret_dict\n\n    if coords_obj_array:\n        # Make an object array for each qoi, where each element is a unique `(*loop_shape, *coord_shape)` array\n        for _, _first_dict in np.ndenumerate(all_qois):\n            if _first_dict is not None:\n                break\n        ret = {qoi: np.empty(all_qois.shape, dtype=object) for qoi in _first_dict}\n        for qoi in ret:\n            for index, qoi_dict in np.ndenumerate(all_qois):\n                if qoi_dict is not None:\n                    ret[qoi][index] = qoi_dict[qoi]\n    else:\n        # Otherwise, all loop dims used the same coords, so just return the single array for each qoi\n        ret = all_qois[0]\n\n    return ret\n</code></pre>"},{"location":"reference/compression/#amisc.compression.Compression.interpolate_to_grid","title":"<code>interpolate_to_grid(field_coords, field_values)</code>","text":"<p>Interpolate the field values at given coordinates to the compression grid. An array of nan is returned for any coordinates or field values that are empty or None.</p> PARAMETER DESCRIPTION <code>field_coords</code> <p><code>(*coord_shape, dim)</code> - the coordinates of the field values; if an object array, then each element is assumed to be a unique <code>(*coord_shape, dim)</code> array</p> <p> TYPE: <code>ndarray</code> </p> <code>field_values</code> <p><code>dict</code> of <code>(*loop_shape, *coord_shape)</code> for each qoi - the field values at the coordinates; if each array is an object array, then each element is assumed to be a unique <code>(*loop_shape, *coord_shape)</code> array corresponding to the <code>field_coords</code></p> <p> </p> RETURNS DESCRIPTION <p><code>(*loop_shape, dof)</code> - the interpolated values on the compression grid</p> Source code in <code>src/amisc/compression.py</code> <pre><code>def interpolate_to_grid(self, field_coords: np.ndarray, field_values):\n    \"\"\"Interpolate the field values at given coordinates to the compression grid. An array of nan is returned\n    for any coordinates or field values that are empty or None.\n\n    :param field_coords: `(*coord_shape, dim)` - the coordinates of the field values; if an object array, then\n                         each element is assumed to be a unique `(*coord_shape, dim)` array\n    :param field_values: `dict` of `(*loop_shape, *coord_shape)` for each qoi - the field values at the coordinates;\n                          if each array is an object array, then each element is assumed to be a unique\n                         `(*loop_shape, *coord_shape)` array corresponding to the `field_coords`\n    :return: `(*loop_shape, dof)` - the interpolated values on the compression grid\n    \"\"\"\n    field_coords = self._correct_coords(field_coords)\n    grid_coords = self._correct_coords(self.coords)\n\n    # Loop over each set of coordinates and field values (multiple if they are object arrays)\n    # If only one set of coords, then they are assumed the same for each set of field values\n    coords_obj_array = np.issubdtype(field_coords.dtype, np.object_)\n    fields_obj_array = np.issubdtype(next(iter(field_values.values())).dtype, np.object_)\n    def _iterate_coords_and_fields():\n        if coords_obj_array:\n            for index, c in np.ndenumerate(field_coords):  # assumes same number of coords and field values\n                yield index, c, {qoi: field_values[qoi][index] for qoi in field_values}\n        elif fields_obj_array:\n            for index in np.ndindex(next(iter(field_values.values())).shape):\n                yield index, field_coords, {qoi: field_values[qoi][index] for qoi in field_values}\n        else:\n            yield (0,), field_coords, field_values  # assumes same coords for all field values\n\n    if coords_obj_array:\n        shape = field_coords.shape\n    elif fields_obj_array:\n        shape = next(iter(field_values.values())).shape\n    else:\n        shape = (1,)\n\n    always_skip_interp = not coords_obj_array and np.array_equal(field_coords, grid_coords)  # must be exact match\n\n    all_states = np.empty(shape, dtype=object)  # are you in good hands?\n\n    for j, f_coords, f_values in _iterate_coords_and_fields():\n        if f_coords is None or any([val is None for val in f_values.values()]):  # Skip empty samples\n            continue\n\n        skip_interp = always_skip_interp or np.array_equal(f_coords, grid_coords)  # exact even for floats\n\n        coords_shape = f_coords.shape[:-1]\n        loop_shape = next(iter(f_values.values())).shape[:-len(coords_shape)]\n        states = np.empty((*loop_shape, self.num_pts, self.num_qoi))\n        f_coords = f_coords.reshape(-1, self.dim)\n        for i, qoi in enumerate(self.fields):\n            field_vals = f_values[qoi].reshape((*loop_shape, -1))  # (..., Q)\n            if skip_interp:\n                states[..., i] = field_vals\n            else:\n                field_vals = field_vals.reshape((-1, field_vals.shape[-1])).T  # (Q, ...)\n                interp = self.interpolator()(f_coords, field_vals, **self.interpolate_opts)\n                yg = interp(grid_coords)\n                states[..., i] = yg.T.reshape(*loop_shape, self.num_pts)\n        all_states[j] = states.reshape((*loop_shape, self.dof))\n\n    # All fields now on the same dof grid, so stack them in same array\n    state_shape = ()\n    for index in np.ndindex(all_states.shape):\n        if all_states[index] is not None:\n            state_shape = all_states[index].shape\n            break\n    ret_states = np.empty(shape + state_shape)\n\n    for index, arr in np.ndenumerate(all_states):\n        ret_states[index] = arr if arr is not None else np.nan\n\n    if not (coords_obj_array or fields_obj_array):\n        ret_states = np.squeeze(ret_states, axis=0)  # artificial leading dim for non-object arrays\n\n    return ret_states\n</code></pre>"},{"location":"reference/compression/#amisc.compression.Compression.interpolator","title":"<code>interpolator()</code>","text":"<p>The interpolator to use during compression and reconstruction. Interpolator expects to be used as:</p> <pre><code>xg = np.ndarray    # (num_pts, dim)  grid coordinates\nyg = np.ndarray    # (num_pts, ...)  scalar values on grid\nxp = np.ndarray    # (Q, dim)        evaluation points\n\ninterp = interpolate_method(xg, yg, **interpolate_opts)\n\nyp = interp(xp)    # (Q, ...)        interpolated values\n</code></pre> Source code in <code>src/amisc/compression.py</code> <pre><code>def interpolator(self):\n    \"\"\"The interpolator to use during compression and reconstruction. Interpolator expects to be used as:\n\n    ```python\n    xg = np.ndarray    # (num_pts, dim)  grid coordinates\n    yg = np.ndarray    # (num_pts, ...)  scalar values on grid\n    xp = np.ndarray    # (Q, dim)        evaluation points\n\n    interp = interpolate_method(xg, yg, **interpolate_opts)\n\n    yp = interp(xp)    # (Q, ...)        interpolated values\n    ```\n    \"\"\"\n    method = self.interpolate_method or 'rbf'\n    match method.lower():\n        case 'rbf':\n            return RBFInterpolator\n        case other:\n            raise NotImplementedError(f\"Interpolation method '{other}' is not implemented.\")\n</code></pre>"},{"location":"reference/compression/#amisc.compression.Compression.latent_size","title":"<code>latent_size()</code>  <code>abstractmethod</code>","text":"<p>Return the size of the latent space.</p> Source code in <code>src/amisc/compression.py</code> <pre><code>@abstractmethod\ndef latent_size(self) -&gt; int:\n    \"\"\"Return the size of the latent space.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/compression/#amisc.compression.Compression.reconstruct","title":"<code>reconstruct(compressed)</code>  <code>abstractmethod</code>","text":"<p>Reconstruct the compressed data back into the full <code>dof</code> space.</p> PARAMETER DESCRIPTION <code>compressed</code> <p><code>(..., rank)</code> - the compressed data to reconstruct</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., dof)</code> - the reconstructed data with full <code>dof</code></p> Source code in <code>src/amisc/compression.py</code> <pre><code>@abstractmethod\ndef reconstruct(self, compressed: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Reconstruct the compressed data back into the full `dof` space.\n\n    :param compressed: `(..., rank)` - the compressed data to reconstruct\n    :return: `(..., dof)` - the reconstructed data with full `dof`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/compression/#amisc.compression.SVD","title":"<code>SVD(fields=list(), method='svd', coords=None, interpolate_method='rbf', interpolate_opts=dict(), _map_computed=False, data_matrix=None, projection_matrix=None, rank=None, energy_tol=None, reconstruction_tol=None)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Compression</code></p> <p>A Singular Value Decomposition (SVD) compression method. The SVD will be computed on initialization if the <code>data_matrix</code> is provided.</p> ATTRIBUTE DESCRIPTION <code>data_matrix</code> <p><code>(dof, num_samples)</code> - the data matrix</p> <p> TYPE: <code>ndarray</code> </p> <code>projection_matrix</code> <p><code>(dof, rank)</code> - the projection matrix</p> <p> TYPE: <code>ndarray</code> </p> <code>rank</code> <p>the rank of the SVD decomposition</p> <p> TYPE: <code>int</code> </p> <code>energy_tol</code> <p>the energy tolerance of the SVD decomposition</p> <p> TYPE: <code>float</code> </p> <code>reconstruction_tol</code> <p>the reconstruction error tolerance of the SVD decomposition</p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/compression/#amisc.compression.SVD.compute_map","title":"<code>compute_map(data_matrix, rank=None, energy_tol=None, reconstruction_tol=None)</code>","text":"<p>Compute the SVD compression map from the data matrix. Recall that <code>dof</code> is the total number of degrees of freedom, equal to the number of grid points <code>num_pts</code> times the number of quantities of interest <code>num_qoi</code> at each grid point.</p> <p>Rank priority: if <code>rank</code> is provided, then it will be used. Otherwise, if <code>reconstruction_tol</code> is provided, then the rank will be chosen to meet this reconstruction error level. Finally, if <code>energy_tol</code> is provided, then the rank will be chosen to meet this energy fraction level (sum of squared singular values).</p> PARAMETER DESCRIPTION <code>data_matrix</code> <p><code>(dof, num_samples)</code> - the data matrix. If passed in as a <code>dict</code>, then the data matrix will be formed by concatenating the values of the <code>dict</code> along the last axis in the order of the <code>fields</code> attribute and flattening the last two axes. This is useful for passing in a dictionary of field values like <code>{field1: (num_samples, num_pts), field2: ...}</code> which ensures consistency of shape with the compression <code>coords</code>.</p> <p> TYPE: <code>ndarray | dict</code> </p> <code>rank</code> <p>the rank of the SVD decomposition</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>energy_tol</code> <p>the energy tolerance of the SVD decomposition (defaults to 0.95)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>reconstruction_tol</code> <p>the reconstruction error tolerance of the SVD decomposition</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/compression.py</code> <pre><code>def compute_map(self, data_matrix: np.ndarray | dict, rank: int = None, energy_tol: float = None,\n                reconstruction_tol: float = None):\n    \"\"\"Compute the SVD compression map from the data matrix. Recall that `dof` is the total number of degrees of\n    freedom, equal to the number of grid points `num_pts` times the number of quantities of interest `num_qoi`\n    at each grid point.\n\n    **Rank priority:** if `rank` is provided, then it will be used. Otherwise, if `reconstruction_tol` is provided,\n    then the rank will be chosen to meet this reconstruction error level. Finally, if `energy_tol` is provided,\n    then the rank will be chosen to meet this energy fraction level (sum of squared singular values).\n\n    :param data_matrix: `(dof, num_samples)` - the data matrix. If passed in as a `dict`, then the data matrix\n                        will be formed by concatenating the values of the `dict` along the last axis in the order\n                        of the `fields` attribute and flattening the last two axes. This is useful for passing\n                        in a dictionary of field values like `{field1: (num_samples, num_pts), field2: ...}`\n                        which ensures consistency of shape with the compression `coords`.\n    :param rank: the rank of the SVD decomposition\n    :param energy_tol: the energy tolerance of the SVD decomposition (defaults to 0.95)\n    :param reconstruction_tol: the reconstruction error tolerance of the SVD decomposition\n    \"\"\"\n    if isinstance(data_matrix, dict):\n        data_matrix = np.concatenate([data_matrix[field][..., np.newaxis] for field in self.fields], axis=-1)\n        data_matrix = data_matrix.reshape(*data_matrix.shape[:-2], -1).T  # (dof, num_samples)\n\n    nan_idx = np.any(np.isnan(data_matrix), axis=0)\n    data_matrix = data_matrix[:, ~nan_idx]\n    u, s, vt = np.linalg.svd(data_matrix)\n    energy_frac = np.cumsum(s ** 2 / np.sum(s ** 2))\n    if rank := (rank or self.rank):\n        energy_tol = energy_frac[rank - 1]\n        reconstruction_tol = relative_error(u[:, :rank] @ u[:, :rank].T @ data_matrix, data_matrix)\n    elif reconstruction_tol := (reconstruction_tol or self.reconstruction_tol):\n        rank = u.shape[1]\n        for r in range(1, u.shape[1] + 1):\n            if relative_error(u[:, :r] @ u[:, :r].T @ data_matrix, data_matrix) &lt;= reconstruction_tol:\n                rank = r\n                break\n        energy_tol = energy_frac[rank - 1]\n    else:\n        energy_tol = energy_tol or self.energy_tol or 0.95\n        idx = int(np.where(energy_frac &gt;= energy_tol)[0][0])\n        rank = idx + 1\n        reconstruction_tol = relative_error(u[:, :rank] @ u[:, :rank].T @ data_matrix, data_matrix)\n\n    self.data_matrix = data_matrix\n    self.rank = rank\n    self.energy_tol = energy_tol\n    self.reconstruction_tol = reconstruction_tol\n    self.projection_matrix = u[:, :rank]  # (dof, rank)\n    self._map_computed = True\n</code></pre>"},{"location":"reference/distribution/","title":"distribution","text":""},{"location":"reference/distribution/#amisc.distribution","title":"<code>amisc.distribution</code>","text":"<p>Module for probability distribution functions (PDFs).</p> <p>Includes:</p> <ul> <li><code>Distribution</code> \u2014 an abstract interface for specifying a PDF.</li> <li><code>Uniform</code> \u2014 a uniform distribution.</li> <li><code>Normal</code> \u2014 a normal distribution.</li> <li><code>Relative</code> \u2014 a relative distribution (i.e. uniform within a percentage of a nominal value).</li> <li><code>Tolerance</code> \u2014 a tolerance distribution (i.e. uniform within a tolerance of a nominal value).</li> <li><code>LogUniform</code> \u2014 a log-uniform distribution.</li> <li><code>LogNormal</code> \u2014 a log-normal distribution.</li> </ul> <p>Distribution objects can be converted easily to/from strings for serialization.</p>"},{"location":"reference/distribution/#amisc.distribution.Distribution","title":"<code>Distribution(dist_args)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for PDF distributions that provide sample and pdf methods. Distributions should:</p> <ul> <li><code>sample</code> - return samples from the distribution</li> <li><code>pdf</code> - return the probability density function of the distribution</li> </ul> ATTRIBUTE DESCRIPTION <code>dist_args</code> <p>the arguments that define the distribution (e.g. mean and std for a normal distribution)</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>src/amisc/distribution.py</code> <pre><code>def __init__(self, dist_args: tuple):\n    self.dist_args = dist_args\n</code></pre>"},{"location":"reference/distribution/#amisc.distribution.Distribution.domain","title":"<code>domain(dist_args=None)</code>","text":"<p>Return the domain of this distribution. Defaults to <code>dist_args</code></p> PARAMETER DESCRIPTION <code>dist_args</code> <p>overrides <code>self.dist_args</code></p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/distribution.py</code> <pre><code>def domain(self, dist_args: tuple = None) -&gt; tuple:\n    \"\"\"Return the domain of this distribution. Defaults to `dist_args`\n\n    :param dist_args: overrides `self.dist_args`\n    \"\"\"\n    return dist_args or self.dist_args\n</code></pre>"},{"location":"reference/distribution/#amisc.distribution.Distribution.from_string","title":"<code>from_string(dist_string)</code>  <code>classmethod</code>","text":"<p>Convert a string to a <code>Distribution</code> object.</p> PARAMETER DESCRIPTION <code>dist_string</code> <p>specifies a PDF or distribution. Can be <code>Normal(mu, std)</code>, <code>Uniform(lb, ub)</code>, <code>LogUniform(lb, ub)</code>, <code>LogNormal(mu, std)</code>, <code>Relative(pct)</code>, or <code>Tolerance(tol)</code>. The shorthands <code>N(0, 1)</code>, <code>U(0, 1)</code>, <code>LU(0, 1)</code>, <code>LN(0, 1)</code>, <code>rel(5)</code>, or <code>tol(1)</code> are also accepted.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Distribution | None</code> <p>the corresponding <code>Distribution</code> object</p> Source code in <code>src/amisc/distribution.py</code> <pre><code>@classmethod\ndef from_string(cls, dist_string: str) -&gt; Distribution | None:\n    \"\"\"Convert a string to a `Distribution` object.\n\n    :param dist_string: specifies a PDF or distribution. Can be `Normal(mu, std)`, `Uniform(lb, ub)`,\n                        `LogUniform(lb, ub)`, `LogNormal(mu, std)`,\n                        `Relative(pct)`, or `Tolerance(tol)`. The shorthands `N(0, 1)`, `U(0, 1)`, `LU(0, 1)`,\n                        `LN(0, 1)`, `rel(5)`, or `tol(1)` are also accepted.\n    :return: the corresponding `Distribution` object\n    \"\"\"\n    if not dist_string:\n        return None\n\n    dist_name, args, kwargs = parse_function_string(dist_string)\n    if dist_name in ['N', 'Normal', 'normal']:\n        # Normal distribution like N(0, 1)\n        try:\n            mu = float(kwargs.get('mu', args[0]))\n            std = float(kwargs.get('std', args[1]))\n            return Normal((mu, std))\n        except Exception as e:\n            raise ValueError(f'Normal distribution string \"{dist_string}\" is not valid: Try N(0, 1).') from e\n    elif dist_name in ['U', 'Uniform', 'uniform']:\n        # Uniform distribution like U(0, 1)\n        try:\n            lb = float(kwargs.get('lb', args[0]))\n            ub = float(kwargs.get('ub', args[1]))\n            return Uniform((lb, ub))\n        except Exception as e:\n            raise ValueError(f'Uniform distribution string \"{dist_string}\" is not valid: Try U(0, 1).') from e\n    elif dist_name in ['R', 'Relative', 'relative', 'rel']:\n        # Relative uniform distribution like rel(+-5%)\n        try:\n            pct = float(kwargs.get('pct', args[0]))\n            return Relative((pct,))\n        except Exception as e:\n            raise ValueError(f'Relative distribution string \"{dist_string}\" is not valid: Try rel(5).') from e\n    elif dist_name in ['T', 'Tolerance', 'tolerance', 'tol']:\n        # Uniform distribution within a tolerance like tol(+-1)\n        try:\n            tol = float(kwargs.get('tol', args[0]))\n            return Tolerance((tol,))\n        except Exception as e:\n            raise ValueError(f'Tolerance distribution string \"{dist_string}\" is not valid: Try tol(1).') from e\n    elif dist_name in ['LogUniform', 'LU']:\n        # LogUniform distribution like LU(1e-3, 1e-1)\n        try:\n            lb = float(kwargs.get('lb', args[0]))\n            ub = float(kwargs.get('ub', args[1]))\n            base = float(kwargs.get('base', args[2] if len(args) &gt; 2 else 10))\n            return LogUniform((lb, ub), base=base)\n        except Exception as e:\n            raise ValueError(f'LogUniform distr string \"{dist_string}\" is not valid: Try LU(1e-3, 1e-1).') from e\n    elif dist_name in ['LogNormal', 'LN']:\n        # LogNniform distribution like LN(-2, 1)\n        try:\n            mu = float(kwargs.get('mu', args[0]))\n            std = float(kwargs.get('std', args[1]))\n            base = float(kwargs.get('base', args[2] if len(args) &gt; 2 else 10))\n            return LogNormal((mu, std), base=base)\n        except Exception as e:\n            raise ValueError(f'LogNormal distr string \"{dist_string}\" is not valid: Try LN(-2, 1).') from e\n    else:\n        raise NotImplementedError(f'The distribution \"{dist_string}\" is not recognized.')\n</code></pre>"},{"location":"reference/distribution/#amisc.distribution.Distribution.nominal","title":"<code>nominal(dist_args=None)</code>","text":"<p>Return the nominal value of this distribution. Defaults to middle of domain.</p> PARAMETER DESCRIPTION <code>dist_args</code> <p>overrides <code>self.dist_args</code></p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/distribution.py</code> <pre><code>def nominal(self, dist_args: tuple = None) -&gt; float:\n    \"\"\"Return the nominal value of this distribution. Defaults to middle of domain.\n\n    :param dist_args: overrides `self.dist_args`\n    \"\"\"\n    lb, ub = self.domain(dist_args=dist_args)\n    return (lb + ub) / 2\n</code></pre>"},{"location":"reference/distribution/#amisc.distribution.Distribution.pdf","title":"<code>pdf(x, dist_args=None)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the pdf of this distribution at the <code>x</code> locations.</p> PARAMETER DESCRIPTION <code>x</code> <p>the locations at which to evaluate the pdf</p> <p> TYPE: <code>ndarray</code> </p> <code>dist_args</code> <p>overrides <code>Distribution.dist_args</code></p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>the pdf evaluations</p> Source code in <code>src/amisc/distribution.py</code> <pre><code>@abstractmethod\ndef pdf(self, x: np.ndarray, dist_args: tuple = None) -&gt; np.ndarray:\n    \"\"\"Evaluate the pdf of this distribution at the `x` locations.\n\n    :param x: the locations at which to evaluate the pdf\n    :param dist_args: overrides `Distribution.dist_args`\n    :return: the pdf evaluations\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/distribution/#amisc.distribution.Distribution.sample","title":"<code>sample(shape, nominal=None, dist_args=None)</code>  <code>abstractmethod</code>","text":"<p>Sample from the distribution.</p> PARAMETER DESCRIPTION <code>shape</code> <p>shape of the samples to return</p> <p> TYPE: <code>int | tuple</code> </p> <code>nominal</code> <p>a nominal value(s) for sampling (e.g. for relative distributions)</p> <p> TYPE: <code>float | ndarray</code> DEFAULT: <code>None</code> </p> <code>dist_args</code> <p>overrides <code>Distribution.dist_args</code></p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>the samples of the given shape</p> Source code in <code>src/amisc/distribution.py</code> <pre><code>@abstractmethod\ndef sample(self, shape: int | tuple, nominal: float | np.ndarray = None, dist_args: tuple = None) -&gt; np.ndarray:\n    \"\"\"Sample from the distribution.\n\n    :param shape: shape of the samples to return\n    :param nominal: a nominal value(s) for sampling (e.g. for relative distributions)\n    :param dist_args: overrides `Distribution.dist_args`\n    :return: the samples of the given shape\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/distribution/#amisc.distribution.LogNormal","title":"<code>LogNormal(dist_args, base=10)</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A LogNormal distribution. Specify by string as <code>LogNormal(mu, sigma)</code> or <code>LN(mu, sigma)</code> in shorthand. Uses base-10 by default.</p> <p>Example</p> <pre><code>x = LogNormal((-2, 1))  # log10(x) ~ N(-2, 1)\n</code></pre> Source code in <code>src/amisc/distribution.py</code> <pre><code>def __init__(self, dist_args: tuple, base=10):\n    self.base = base\n    super().__init__(dist_args)\n</code></pre>"},{"location":"reference/distribution/#amisc.distribution.LogNormal.domain","title":"<code>domain(dist_args=None)</code>","text":"<p>Defaults the domain of the distribution to 3 standard deviations above and below the mean.</p> Source code in <code>src/amisc/distribution.py</code> <pre><code>def domain(self, dist_args=None):\n    \"\"\"Defaults the domain of the distribution to 3 standard deviations above and below the mean.\"\"\"\n    mu, std = dist_args or self.dist_args\n    return self.base ** (mu - 3 * std), self.base ** (mu + 3 * std)\n</code></pre>"},{"location":"reference/distribution/#amisc.distribution.LogUniform","title":"<code>LogUniform(dist_args, base=10)</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A LogUniform distribution. Specify by string as <code>LogUniform(lb, ub)</code> or <code>LU(lb, ub)</code> in shorthand. Uses base-10 by default.</p> <p>Example</p> <pre><code>x = LogUniform((1e-3, 1e-1))  # log10(x) ~ U(-3, -1)\n</code></pre> Source code in <code>src/amisc/distribution.py</code> <pre><code>def __init__(self, dist_args: tuple, base=10):\n    self.base = base\n    super().__init__(dist_args)\n</code></pre>"},{"location":"reference/distribution/#amisc.distribution.Normal","title":"<code>Normal(dist_args)</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A Normal distribution. Specify by string as <code>Normal(mu, std)</code> or <code>N(mu, std)</code> in shorthand.</p> Source code in <code>src/amisc/distribution.py</code> <pre><code>def __init__(self, dist_args: tuple):\n    self.dist_args = dist_args\n</code></pre>"},{"location":"reference/distribution/#amisc.distribution.Normal.domain","title":"<code>domain(dist_args=None)</code>","text":"<p>Defaults the domain of the distribution to 3 standard deviations above and below the mean.</p> Source code in <code>src/amisc/distribution.py</code> <pre><code>def domain(self, dist_args=None):\n    \"\"\"Defaults the domain of the distribution to 3 standard deviations above and below the mean.\"\"\"\n    mu, std = dist_args or self.dist_args\n    return mu - 3 * std, mu + 3 * std\n</code></pre>"},{"location":"reference/distribution/#amisc.distribution.Relative","title":"<code>Relative(dist_args)</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A Relative distribution. Specify by string as <code>Relative(pct)</code> or <code>rel(pct%)</code> in shorthand. Will attempt to sample uniformly within the given percent of a nominal value.</p> Source code in <code>src/amisc/distribution.py</code> <pre><code>def __init__(self, dist_args: tuple):\n    self.dist_args = dist_args\n</code></pre>"},{"location":"reference/distribution/#amisc.distribution.Tolerance","title":"<code>Tolerance(dist_args)</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A Tolerance distribution. Specify by string as <code>Tolerance(tol)</code> or <code>tol(tol)</code> in shorthand. Will attempt to sample uniformly within a given absolute tolerance of a nominal value.</p> Source code in <code>src/amisc/distribution.py</code> <pre><code>def __init__(self, dist_args: tuple):\n    self.dist_args = dist_args\n</code></pre>"},{"location":"reference/distribution/#amisc.distribution.Uniform","title":"<code>Uniform(dist_args)</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A Uniform distribution. Specify by string as \"Uniform(lb, ub)\" or \"U(lb, ub)\" in shorthand.</p> Source code in <code>src/amisc/distribution.py</code> <pre><code>def __init__(self, dist_args: tuple):\n    self.dist_args = dist_args\n</code></pre>"},{"location":"reference/interpolator/","title":"interpolator","text":""},{"location":"reference/interpolator/#amisc.interpolator","title":"<code>amisc.interpolator</code>","text":"<p>Provides interpolator classes. Interpolators approximate the input \u2192 output mapping of a model given a set of training data. The training data consists of input-output pairs, and the interpolator can be refined with new training data.</p> <p>Includes:</p> <ul> <li><code>Interpolator</code>: Abstract class providing basic structure of an interpolator</li> <li><code>Lagrange</code>: Concrete implementation for tensor-product barycentric Lagrange interpolation</li> <li><code>Linear</code>: Concrete implementation for linear regression using <code>sklearn</code></li> <li><code>InterpolatorState</code>: Interface for a dataclass that stores the internal state of an interpolator</li> <li><code>LagrangeState</code>: The internal state for a barycentric Lagrange polynomial interpolator</li> <li><code>LinearState</code>: The internal state for a linear interpolator (using sklearn)</li> </ul>"},{"location":"reference/interpolator/#amisc.interpolator.Interpolator","title":"<code>Interpolator</code>","text":"<p>               Bases: <code>Serializable</code>, <code>ABC</code></p> <p>Interface for an interpolator object that approximates a model. An interpolator should:</p> <ul> <li><code>refine</code> - take an old state and new training data and produce a new \"refined\" state (e.g. new weights/biases)</li> <li><code>predict</code> - interpolate from the training data to a new set of points (i.e. approximate the underlying model)</li> <li><code>gradient</code> - compute the grdient/Jacobian at new points (if you want)</li> <li><code>hessian</code> - compute the 2nd derivative/Hessian at new points (if you want)</li> </ul> <p>Currently, <code>Lagrange</code> and <code>Linear</code> interpolators are supported and can be constructed from a configuration <code>dict</code> via <code>Interpolator.from_dict()</code>.</p>"},{"location":"reference/interpolator/#amisc.interpolator.Interpolator.from_dict","title":"<code>from_dict(config)</code>  <code>classmethod</code>","text":"<p>Create an <code>Interpolator</code> object from a <code>dict</code> config. Only <code>method='lagrange'</code> is supported for now.</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>@classmethod\ndef from_dict(cls, config: dict) -&gt; Interpolator:\n    \"\"\"Create an `Interpolator` object from a `dict` config. Only `method='lagrange'` is supported for now.\"\"\"\n    method = config.pop('method', 'lagrange').lower()\n    match method:\n        case 'lagrange':\n            return Lagrange(**config)\n        case 'linear':\n            return Linear(**config)\n        case other:\n            raise NotImplementedError(f\"Unknown interpolator method: {other}\")\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.Interpolator.gradient","title":"<code>gradient(x, state, training_data)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the gradient/Jacobian at points <code>x</code> using the interpolator.</p> PARAMETER DESCRIPTION <code>x</code> <p>the input Dataset <code>dict</code> mapping input variables to locations at which to evaluate the Jacobian</p> <p> TYPE: <code>Dataset</code> </p> <code>state</code> <p>the current state of the interpolator</p> <p> TYPE: <code>InterpolatorState</code> </p> <code>training_data</code> <p>a tuple of <code>xi, yi</code> Datasets for the input/output training data for the current state</p> <p> TYPE: <code>tuple[Dataset, Dataset]</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>a Dataset <code>dict</code> mapping output variables to Jacobian matrices of shape <code>(ydim, xdim)</code> -- for scalar outputs, the Jacobian is returned as <code>(xdim,)</code></p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>@abstractmethod\ndef gradient(self, x: Dataset, state: InterpolatorState, training_data: tuple[Dataset, Dataset]) -&gt; Dataset:\n    \"\"\"Evaluate the gradient/Jacobian at points `x` using the interpolator.\n\n    :param x: the input Dataset `dict` mapping input variables to locations at which to evaluate the Jacobian\n    :param state: the current state of the interpolator\n    :param training_data: a tuple of `xi, yi` Datasets for the input/output training data for the current state\n    :returns: a Dataset `dict` mapping output variables to Jacobian matrices of shape `(ydim, xdim)` -- for\n              scalar outputs, the Jacobian is returned as `(xdim,)`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.Interpolator.hessian","title":"<code>hessian(x, state, training_data)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the Hessian at points <code>x</code> using the interpolator.</p> PARAMETER DESCRIPTION <code>x</code> <p>the input Dataset <code>dict</code> mapping input variables to locations at which to evaluate the Hessian</p> <p> TYPE: <code>Dataset</code> </p> <code>state</code> <p>the current state of the interpolator</p> <p> TYPE: <code>InterpolatorState</code> </p> <code>training_data</code> <p>a tuple of <code>xi, yi</code> Datasets for the input/output training data for the current state</p> <p> TYPE: <code>tuple[Dataset, Dataset]</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>a Dataset <code>dict</code> mapping output variables to Hessian matrices of shape <code>(xdim, xdim)</code></p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>@abstractmethod\ndef hessian(self, x: Dataset, state: InterpolatorState, training_data: tuple[Dataset, Dataset]) -&gt; Dataset:\n    \"\"\"Evaluate the Hessian at points `x` using the interpolator.\n\n    :param x: the input Dataset `dict` mapping input variables to locations at which to evaluate the Hessian\n    :param state: the current state of the interpolator\n    :param training_data: a tuple of `xi, yi` Datasets for the input/output training data for the current state\n    :returns: a Dataset `dict` mapping output variables to Hessian matrices of shape `(xdim, xdim)`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.Interpolator.predict","title":"<code>predict(x, state, training_data)</code>  <code>abstractmethod</code>","text":"<p>Interpolate the output of the model at points <code>x</code> using the given state and training data</p> PARAMETER DESCRIPTION <code>x</code> <p>the input Dataset <code>dict</code> mapping input variables to locations at which to compute the interpolator</p> <p> TYPE: <code>dict | Dataset</code> </p> <code>state</code> <p>the current state of the interpolator</p> <p> TYPE: <code>InterpolatorState</code> </p> <code>training_data</code> <p>a tuple of <code>xi, yi</code> Datasets for the input/output training data for the current state</p> <p> TYPE: <code>tuple[Dataset, Dataset]</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>a Dataset <code>dict</code> mapping output variables to interpolator outputs</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>@abstractmethod\ndef predict(self, x: dict | Dataset, state: InterpolatorState, training_data: tuple[Dataset, Dataset]) -&gt; Dataset:\n    \"\"\"Interpolate the output of the model at points `x` using the given state and training data\n\n    :param x: the input Dataset `dict` mapping input variables to locations at which to compute the interpolator\n    :param state: the current state of the interpolator\n    :param training_data: a tuple of `xi, yi` Datasets for the input/output training data for the current state\n    :returns: a Dataset `dict` mapping output variables to interpolator outputs\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.Interpolator.refine","title":"<code>refine(beta, training_data, old_state, input_domains)</code>  <code>abstractmethod</code>","text":"<p>Refine the interpolator state with new training data.</p> PARAMETER DESCRIPTION <code>beta</code> <p>a multi-index specifying the fidelity \"levels\" of the new interpolator state (starts at (0,... 0))</p> <p> TYPE: <code>MultiIndex</code> </p> <code>training_data</code> <p>a tuple of <code>xi, yi</code> Datasets for the input/output training data</p> <p> TYPE: <code>tuple[Dataset, Dataset]</code> </p> <code>old_state</code> <p>the previous state of the interpolator (None if initializing the first state)</p> <p> TYPE: <code>InterpolatorState</code> </p> <code>input_domains</code> <p>a <code>dict</code> mapping input variables to their corresponding domains</p> <p> TYPE: <code>dict[str, tuple]</code> </p> RETURNS DESCRIPTION <code>InterpolatorState</code> <p>the new \"refined\" interpolator state</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>@abstractmethod\ndef refine(self, beta: MultiIndex, training_data: tuple[Dataset, Dataset],\n           old_state: InterpolatorState, input_domains: dict[str, tuple]) -&gt; InterpolatorState:\n    \"\"\"Refine the interpolator state with new training data.\n\n    :param beta: a multi-index specifying the fidelity \"levels\" of the new interpolator state (starts at (0,... 0))\n    :param training_data: a tuple of `xi, yi` Datasets for the input/output training data\n    :param old_state: the previous state of the interpolator (None if initializing the first state)\n    :param input_domains: a `dict` mapping input variables to their corresponding domains\n    :returns: the new \"refined\" interpolator state\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.InterpolatorState","title":"<code>InterpolatorState</code>","text":"<p>               Bases: <code>Serializable</code>, <code>ABC</code></p> <p>Interface for a dataclass that stores the internal state of an interpolator (e.g. weights and biases).</p>"},{"location":"reference/interpolator/#amisc.interpolator.Lagrange","title":"<code>Lagrange(interval_capacity=4.0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Interpolator</code>, <code>StringSerializable</code></p> <p>Implementation of a tensor-product barycentric Lagrange polynomial interpolator. A <code>LagrangeState</code> stores the 1d interpolation grids and weights for each input dimension. <code>Lagrange</code> computes the tensor-product of 1d Lagrange polynomials to approximate a multi-variate function.</p> ATTRIBUTE DESCRIPTION <code>interval_capacity</code> <p>tuning knob for Lagrange interpolation (see Berrut and Trefethen 2004)</p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/interpolator/#amisc.interpolator.Lagrange.gradient","title":"<code>gradient(x, state, training_data)</code>","text":"<p>Evaluate the gradient/Jacobian at points <code>x</code> using the interpolator.</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def gradient(self, x: Dataset, state: LagrangeState, training_data: tuple[Dataset, Dataset]):\n    \"\"\"Evaluate the gradient/Jacobian at points `x` using the interpolator.\"\"\"\n    # Convert `x` and `yi` to 2d arrays: (N, xdim) and (N, ydim)\n    xi, yi = training_data\n    x_arr = np.concatenate([x[var][..., np.newaxis] for var in xi], axis=-1)\n    yi_arr = np.concatenate([yi[var][..., np.newaxis] for var in yi], axis=-1)\n\n    xdim = x_arr.shape[-1]\n    ydim = yi_arr.shape[-1]\n    grid_sizes = {var: grid.shape[-1] for var, grid in state.x_grids.items()}\n    max_size = max(grid_sizes.values())\n\n    # Create ragged edge matrix of interpolation pts and weights\n    x_j = np.full((xdim, max_size), np.nan)                             # For example:\n    w_j = np.full((xdim, max_size), np.nan)                             # A= [#####--\n    for n, var in enumerate(state.x_grids):                             #     #######\n        x_j[n, :grid_sizes[var]] = state.x_grids[var]                   #     ###----]\n        w_j[n, :grid_sizes[var]] = state.weights[var]\n\n    # Compute values ahead of time that will be needed for the gradient\n    diff = x_arr[..., np.newaxis] - x_j\n    div_zero_idx = np.isclose(diff, 0, rtol=1e-4, atol=1e-8)\n    check_interp_pts = np.sum(div_zero_idx) &gt; 0\n    diff[div_zero_idx] = 1\n    quotient = w_j / diff                               # (..., xdim, Nx)\n    qsum = np.nansum(quotient, axis=-1)                 # (..., xdim)\n    sqsum = np.nansum(w_j / diff ** 2, axis=-1)         # (..., xdim)\n    jac = np.zeros(x_arr.shape[:-1] + (ydim, xdim))     # (..., ydim, xdim)\n\n    # Loop over multi-indices and compute derivative of tensor-product lagrange polynomials\n    indices = [range(s) for s in grid_sizes.values()]\n    for k, var in enumerate(grid_sizes):\n        dims = [idx for idx in range(xdim) if idx != k]\n        for i, j in enumerate(itertools.product(*indices)):\n            j_dims = [j[idx] for idx in dims]\n            L_j = quotient[..., dims, j_dims] / qsum[..., dims]  # (..., xdim-1)\n\n            # Partial derivative of L_j with respect to x_k\n            dLJ_dx = ((w_j[k, j[k]] / (qsum[..., k] * diff[..., k, j[k]])) *\n                      (sqsum[..., k] / qsum[..., k] - 1 / diff[..., k, j[k]]))\n\n            # Set L_j(x==x_j)=1 for the current j and set L_j(x==x_j)=0 for x_j = x_i, i != j\n            if check_interp_pts:\n                other_pts = np.copy(div_zero_idx)\n                other_pts[div_zero_idx[..., list(range(xdim)), j]] = False\n                L_j[div_zero_idx[..., dims, j_dims]] = 1\n                L_j[np.any(other_pts[..., dims, :], axis=-1)] = 0\n\n                # Set derivatives when x is at the interpolation points (i.e. x==x_j)\n                p_idx = [idx for idx in range(grid_sizes[var]) if idx != j[k]]\n                w_j_large = np.broadcast_to(w_j[k, :], x_arr.shape[:-1] + w_j.shape[-1:]).copy()\n                curr_j_idx = div_zero_idx[..., k, j[k]]\n                other_j_idx = np.any(other_pts[..., k, :], axis=-1)\n                dLJ_dx[curr_j_idx] = -np.nansum((w_j[k, p_idx] / w_j[k, j[k]]) /\n                                                (x_arr[curr_j_idx, k, np.newaxis] - x_j[k, p_idx]), axis=-1)\n                dLJ_dx[other_j_idx] = ((w_j[k, j[k]] / w_j_large[other_pts[..., k, :]]) /\n                                       (x_arr[other_j_idx, k] - x_j[k, j[k]]))\n\n            dLJ_dx = np.expand_dims(dLJ_dx, axis=-1) * np.prod(L_j, axis=-1, keepdims=True)  # (..., 1)\n\n            # Add contribution to the Jacobian\n            jac[..., k] += dLJ_dx * yi_arr[i, :]\n\n    # Unpack the outputs back into a Dataset (array of length xdim for each y_var giving partial derivatives)\n    jac_ret = {}\n    start_idx = 0\n    for var, arr in yi.items():\n        num_vals = arr.shape[-1] if len(arr.shape) &gt; 1 else 1\n        end_idx = start_idx + num_vals\n        jac_ret[var] = jac[..., start_idx:end_idx, :]  # (..., ydim, xdim)\n        if len(arr.shape) == 1:\n            jac_ret[var] = np.squeeze(jac_ret[var], axis=-2)  # for scalars: (..., xdim) partial derivatives\n        start_idx = end_idx\n    return jac_ret\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.Lagrange.hessian","title":"<code>hessian(x, state, training_data)</code>","text":"<p>Evaluate the Hessian at points <code>x</code> using the interpolator.</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def hessian(self, x: Dataset, state: LagrangeState, training_data: tuple[Dataset, Dataset]):\n    \"\"\"Evaluate the Hessian at points `x` using the interpolator.\"\"\"\n    # Convert `x` and `yi` to 2d arrays: (N, xdim) and (N, ydim)\n    xi, yi = training_data\n    x_arr = np.concatenate([x[var][..., np.newaxis] for var in xi], axis=-1)\n    yi_arr = np.concatenate([yi[var][..., np.newaxis] for var in yi], axis=-1)\n\n    xdim = x_arr.shape[-1]\n    ydim = yi_arr.shape[-1]\n    grid_sizes = {var: grid.shape[-1] for var, grid in state.x_grids.items()}\n    grid_size_list = list(grid_sizes.values())\n    max_size = max(grid_size_list)\n\n    # Create ragged edge matrix of interpolation pts and weights\n    x_j = np.full((xdim, max_size), np.nan)                             # For example:\n    w_j = np.full((xdim, max_size), np.nan)                             # A= [#####--\n    for n, var in enumerate(state.x_grids):                             #     #######\n        x_j[n, :grid_sizes[var]] = state.x_grids[var]                   #     ###----]\n        w_j[n, :grid_sizes[var]] = state.weights[var]\n\n    # Compute values ahead of time that will be needed for the gradient\n    diff = x_arr[..., np.newaxis] - x_j\n    div_zero_idx = np.isclose(diff, 0, rtol=1e-4, atol=1e-8)\n    check_interp_pts = np.sum(div_zero_idx) &gt; 0\n    diff[div_zero_idx] = 1\n    quotient = w_j / diff                                       # (..., xdim, Nx)\n    qsum = np.nansum(quotient, axis=-1)                         # (..., xdim)\n    qsum_p = -np.nansum(w_j / diff ** 2, axis=-1)               # (..., xdim)\n    qsum_pp = 2 * np.nansum(w_j / diff ** 3, axis=-1)           # (..., xdim)\n\n    # Loop over multi-indices and compute 2nd derivative of tensor-product lagrange polynomials\n    hess = np.zeros(x_arr.shape[:-1] + (ydim, xdim, xdim))      # (..., ydim, xdim, xdim)\n    indices = [range(s) for s in grid_size_list]\n    for m in range(xdim):\n        for n in range(m, xdim):\n            dims = [idx for idx in range(xdim) if idx not in [m, n]]\n            for i, j in enumerate(itertools.product(*indices)):\n                j_dims = [j[idx] for idx in dims]\n                L_j = quotient[..., dims, j_dims] / qsum[..., dims]  # (..., xdim-2)\n\n                # Set L_j(x==x_j)=1 for the current j and set L_j(x==x_j)=0 for x_j = x_i, i != j\n                if check_interp_pts:\n                    other_pts = np.copy(div_zero_idx)\n                    other_pts[div_zero_idx[..., list(range(xdim)), j]] = False\n                    L_j[div_zero_idx[..., dims, j_dims]] = 1\n                    L_j[np.any(other_pts[..., dims, :], axis=-1)] = 0\n\n                # Cross-terms in Hessian\n                if m != n:\n                    # Partial derivative of L_j with respect to x_m and x_n\n                    d2LJ_dx2 = np.ones(x_arr.shape[:-1])\n                    for k in [m, n]:\n                        dLJ_dx = ((w_j[k, j[k]] / (qsum[..., k] * diff[..., k, j[k]])) *\n                                  (-qsum_p[..., k] / qsum[..., k] - 1 / diff[..., k, j[k]]))\n\n                        # Set derivatives when x is at the interpolation points (i.e. x==x_j)\n                        if check_interp_pts:\n                            p_idx = [idx for idx in range(grid_size_list[k]) if idx != j[k]]\n                            w_j_large = np.broadcast_to(w_j[k, :], x_arr.shape[:-1] + w_j.shape[-1:]).copy()\n                            curr_j_idx = div_zero_idx[..., k, j[k]]\n                            other_j_idx = np.any(other_pts[..., k, :], axis=-1)\n                            dLJ_dx[curr_j_idx] = -np.nansum((w_j[k, p_idx] / w_j[k, j[k]]) /\n                                                            (x_arr[curr_j_idx, k, np.newaxis] - x_j[k, p_idx]),\n                                                            axis=-1)\n                            dLJ_dx[other_j_idx] = ((w_j[k, j[k]] / w_j_large[other_pts[..., k, :]]) /\n                                                   (x_arr[other_j_idx, k] - x_j[k, j[k]]))\n\n                        d2LJ_dx2 *= dLJ_dx\n\n                    d2LJ_dx2 = np.expand_dims(d2LJ_dx2, axis=-1) * np.prod(L_j, axis=-1, keepdims=True)  # (..., 1)\n                    hess[..., m, n] += d2LJ_dx2 * yi_arr[i, :]\n                    hess[..., n, m] += d2LJ_dx2 * yi_arr[i, :]\n\n                # Diagonal terms in Hessian:\n                else:\n                    front_term = w_j[m, j[m]] / (qsum[..., m] * diff[..., m, j[m]])\n                    first_term = (-qsum_pp[..., m] / qsum[..., m]) + 2 * (qsum_p[..., m] / qsum[..., m]) ** 2\n                    second_term = (2 * (qsum_p[..., m] / (qsum[..., m] * diff[..., m, j[m]]))\n                                   + 2 / diff[..., m, j[m]] ** 2)\n                    d2LJ_dx2 = front_term * (first_term + second_term)\n\n                    # Set derivatives when x is at the interpolation points (i.e. x==x_j)\n                    if check_interp_pts:\n                        curr_j_idx = div_zero_idx[..., m, j[m]]\n                        other_j_idx = np.any(other_pts[..., m, :], axis=-1)\n                        if np.any(curr_j_idx) or np.any(other_j_idx):\n                            p_idx = [idx for idx in range(grid_size_list[m]) if idx != j[m]]\n                            w_j_large = np.broadcast_to(w_j[m, :], x_arr.shape[:-1] + w_j.shape[-1:]).copy()\n                            x_j_large = np.broadcast_to(x_j[m, :], x_arr.shape[:-1] + x_j.shape[-1:]).copy()\n\n                            # if these points are at the current j interpolation point\n                            d2LJ_dx2[curr_j_idx] = (2 * np.nansum((w_j[m, p_idx] / w_j[m, j[m]]) /\n                                                                  (x_arr[curr_j_idx, m, np.newaxis] - x_j[m, p_idx]), # noqa: E501\n                                                                  axis=-1) ** 2 +\n                                                    2 * np.nansum((w_j[m, p_idx] / w_j[m, j[m]]) /\n                                                                  (x_arr[curr_j_idx, m, np.newaxis] - x_j[m, p_idx]) ** 2, # noqa: E501\n                                                                  axis=-1))\n\n                            # if these points are at any other interpolation point\n                            other_pts_inv = other_pts.copy()\n                            other_pts_inv[other_j_idx, m, :grid_size_list[m]] = np.invert(\n                                other_pts[other_j_idx, m, :grid_size_list[m]])  # noqa: E501\n                            curr_x_j = x_j_large[other_pts[..., m, :]].reshape((-1, 1))\n                            other_x_j = x_j_large[other_pts_inv[..., m, :]].reshape((-1, len(p_idx)))\n                            curr_w_j = w_j_large[other_pts[..., m, :]].reshape((-1, 1))\n                            other_w_j = w_j_large[other_pts_inv[..., m, :]].reshape((-1, len(p_idx)))\n                            curr_div = w_j[m, j[m]] / np.squeeze(curr_w_j, axis=-1)\n                            curr_diff = np.squeeze(curr_x_j, axis=-1) - x_j[m, j[m]]\n                            d2LJ_dx2[other_j_idx] = ((-2 * curr_div / curr_diff) * (np.nansum(\n                                (other_w_j / curr_w_j) / (curr_x_j - other_x_j), axis=-1) + 1 / curr_diff))\n\n                    d2LJ_dx2 = np.expand_dims(d2LJ_dx2, axis=-1) * np.prod(L_j, axis=-1, keepdims=True)  # (..., 1)\n                    hess[..., m, n] += d2LJ_dx2 * yi_arr[i, :]\n\n    # Unpack the outputs back into a Dataset (matrix (xdim, xdim) for each y_var giving 2nd partial derivatives)\n    hess_ret = {}\n    start_idx = 0\n    for var, arr in yi.items():\n        num_vals = arr.shape[-1] if len(arr.shape) &gt; 1 else 1\n        end_idx = start_idx + num_vals\n        hess_ret[var] = hess[..., start_idx:end_idx, :, :]  # (..., ydim, xdim, xdim)\n        if len(arr.shape) == 1:\n            hess_ret[var] = np.squeeze(hess_ret[var], axis=-3)  # for scalars: (..., xdim, xdim) partial derivatives\n        start_idx = end_idx\n    return hess_ret\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.Lagrange.predict","title":"<code>predict(x, state, training_data)</code>","text":"<p>Predict the output of the model at points <code>x</code> with barycentric Lagrange interpolation.</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def predict(self, x: Dataset, state: LagrangeState, training_data: tuple[Dataset, Dataset]):\n    \"\"\"Predict the output of the model at points `x` with barycentric Lagrange interpolation.\"\"\"\n    # Convert `x` and `yi` to 2d arrays: (N, xdim) and (N, ydim)\n    # Inputs `x` may come in unordered, but they should get realigned with the internal `x_grids` state\n    xi, yi = training_data\n    x_arr = np.concatenate([x[var][..., np.newaxis] for var in xi], axis=-1)\n    yi_arr = np.concatenate([yi[var][..., np.newaxis] for var in yi], axis=-1)\n\n    xdim = x_arr.shape[-1]\n    ydim = yi_arr.shape[-1]\n    grid_sizes = {var: grid.shape[-1] for var, grid in state.x_grids.items()}\n    max_size = max(grid_sizes.values())\n    dims = list(range(xdim))\n\n    # Create ragged edge matrix of interpolation pts and weights\n    x_j = np.full((xdim, max_size), np.nan)                             # For example:\n    w_j = np.full((xdim, max_size), np.nan)                             # A= [#####--\n    for n, var in enumerate(state.x_grids):                             #     #######\n        x_j[n, :grid_sizes[var]] = state.x_grids[var]                   #     ###----]\n        w_j[n, :grid_sizes[var]] = state.weights[var]\n\n    diff = x_arr[..., np.newaxis] - x_j\n    div_zero_idx = np.isclose(diff, 0, rtol=1e-4, atol=1e-8)\n    check_interp_pts = np.sum(div_zero_idx) &gt; 0     # whether we are evaluating directly on some interp pts\n    diff[div_zero_idx] = 1\n    quotient = w_j / diff                           # (..., xdim, Nx)\n    qsum = np.nansum(quotient, axis=-1)             # (..., xdim)\n    y = np.zeros(x_arr.shape[:-1] + (ydim,))        # (..., ydim)\n\n    # Loop over multi-indices and compute tensor-product lagrange polynomials\n    indices = [range(s) for s in grid_sizes.values()]\n    for i, j in enumerate(itertools.product(*indices)):\n        L_j = quotient[..., dims, j] / qsum         # (..., xdim)\n\n        # Set L_j(x==x_j)=1 for the current j and set L_j(x==x_j)=0 for x_j = x_i, i != j\n        if check_interp_pts:\n            other_pts = np.copy(div_zero_idx)\n            other_pts[div_zero_idx[..., dims, j]] = False\n            L_j[div_zero_idx[..., dims, j]] = 1\n            L_j[np.any(other_pts, axis=-1)] = 0\n\n        # Add multivariate basis polynomial contribution to interpolation output\n        y += np.prod(L_j, axis=-1, keepdims=True) * yi_arr[i, :]\n\n    # Unpack the outputs back into a Dataset\n    y_ret = {}\n    start_idx = 0\n    for var, arr in yi.items():\n        num_vals = arr.shape[-1] if len(arr.shape) &gt; 1 else 1\n        end_idx = start_idx + num_vals\n        y_ret[var] = y[..., start_idx:end_idx]\n        if len(arr.shape) == 1:\n            y_ret[var] = np.squeeze(y_ret[var], axis=-1)  # for scalars\n        start_idx = end_idx\n    return y_ret\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.Lagrange.refine","title":"<code>refine(beta, training_data, old_state, input_domains)</code>","text":"<p>Refine the interpolator state with new training data.</p> PARAMETER DESCRIPTION <code>beta</code> <p>the refinement level indices for the interpolator (not used for <code>Lagrange</code>)</p> <p> TYPE: <code>MultiIndex</code> </p> <code>training_data</code> <p>a tuple of dictionaries containing the new training data (<code>xtrain</code>, <code>ytrain</code>)</p> <p> TYPE: <code>tuple[Dataset, Dataset]</code> </p> <code>old_state</code> <p>the old interpolator state to refine (None if initializing)</p> <p> TYPE: <code>LagrangeState</code> </p> <code>input_domains</code> <p>a <code>dict</code> of each input variable's domain; input keys should match <code>xtrain</code> keys</p> <p> TYPE: <code>dict[str, tuple]</code> </p> RETURNS DESCRIPTION <code>LagrangeState</code> <p>the new interpolator state</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def refine(self, beta: MultiIndex, training_data: tuple[Dataset, Dataset],\n           old_state: LagrangeState, input_domains: dict[str, tuple]) -&gt; LagrangeState:\n    \"\"\"Refine the interpolator state with new training data.\n\n    :param beta: the refinement level indices for the interpolator (not used for `Lagrange`)\n    :param training_data: a tuple of dictionaries containing the new training data (`xtrain`, `ytrain`)\n    :param old_state: the old interpolator state to refine (None if initializing)\n    :param input_domains: a `dict` of each input variable's domain; input keys should match `xtrain` keys\n    :returns: the new interpolator state\n    \"\"\"\n    xtrain, ytrain = training_data  # Lagrange only really needs the xtrain data to update barycentric weights/grids\n\n    # Initialize the interpolator state\n    if old_state is None:\n        x_grids = self._extend_grids({}, xtrain)\n        weights = {}\n        for var, grid in x_grids.items():\n            bds = input_domains[var]\n            Nx = grid.shape[0]\n            C = (bds[1] - bds[0]) / self.interval_capacity  # Interval capacity (see Berrut and Trefethen 2004)\n            xj = grid.reshape((Nx, 1))\n            xi = xj.reshape((1, Nx))\n            dist = (xj - xi) / C\n            np.fill_diagonal(dist, 1)  # Ignore product when i==j\n            weights[var] = (1.0 / np.prod(dist, axis=1))  # (Nx,)\n\n    # Otherwise, refine the interpolator state\n    else:\n        x_grids = self._extend_grids(old_state.x_grids, xtrain)\n        weights = copy.deepcopy(old_state.weights)\n        for var, grid in x_grids.items():\n            bds = input_domains[var]\n            Nx_old = old_state.x_grids[var].shape[0]\n            Nx_new = grid.shape[0]\n            if Nx_new &gt; Nx_old:\n                weights[var] = np.pad(weights[var], [(0, Nx_new - Nx_old)], mode='constant', constant_values=np.nan)\n                C = (bds[1] - bds[0]) / self.interval_capacity\n                for j in range(Nx_old, Nx_new):\n                    weights[var][:j] *= (C / (grid[:j] - grid[j]))\n                    weights[var][j] = np.prod(C / (grid[j] - grid[:j]))\n\n    return LagrangeState(weights=weights, x_grids=x_grids)\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.LagrangeState","title":"<code>LagrangeState(weights=dict(), x_grids=dict())</code>  <code>dataclass</code>","text":"<p>               Bases: <code>InterpolatorState</code>, <code>Base64Serializable</code></p> <p>The internal state for a barycentric Lagrange polynomial interpolator.</p> ATTRIBUTE DESCRIPTION <code>weights</code> <p>the 1d interpolation grid weights</p> <p> TYPE: <code>dict[str, ndarray]</code> </p> <code>x_grids</code> <p>the 1d interpolation grids</p> <p> TYPE: <code>dict[str, ndarray]</code> </p>"},{"location":"reference/interpolator/#amisc.interpolator.Linear","title":"<code>Linear(regressor='Ridge', scaler=None, regressor_opts=dict(), scaler_opts=dict(), polynomial_opts=lambda: {'degree': 1, 'include_bias': False}())</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Interpolator</code>, <code>StringSerializable</code></p> <p>Implementation of linear regression using <code>sklearn</code>. The <code>Linear</code> interpolator uses a pipeline of <code>PolynomialFeatures</code> and a linear model from <code>sklearn.linear_model</code> to approximate the input-output mapping with a linear combination of polynomial features. Defaults to Ridge regression (L2 regularization) with polynomials of degree 1 (i.e. normal linear regression).</p> ATTRIBUTE DESCRIPTION <code>regressor</code> <p>the scikit-learn linear model to use (e.g. 'Ridge', 'Lasso', 'ElasticNet', etc.).</p> <p> TYPE: <code>str</code> </p> <code>scaler</code> <p>the scikit-learn preprocessing scaler to use (e.g. 'MinMaxScaler', 'StandardScaler', etc.). If None, no scaling is applied (default).</p> <p> TYPE: <code>str</code> </p> <code>regressor_opts</code> <p>options to pass to the regressor constructor (see scikit-learn documentation).</p> <p> TYPE: <code>dict</code> </p> <code>scaler_opts</code> <p>options to pass to the scaler constructor</p> <p> TYPE: <code>dict</code> </p> <code>polynomial_opts</code> <p>options to pass to the <code>PolynomialFeatures</code> constructor (e.g. 'degree', 'include_bias').</p> <p> TYPE: <code>dict</code> </p>"},{"location":"reference/interpolator/#amisc.interpolator.Linear.predict","title":"<code>predict(x, state, training_data)</code>","text":"<p>Predict the output of the model at points <code>x</code> using the linear regressor provided in <code>state</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p>the input Dataset <code>dict</code> mapping input variables to prediction locations</p> <p> TYPE: <code>Dataset</code> </p> <code>state</code> <p>the state containing the linear regressor to use</p> <p> TYPE: <code>LinearState</code> </p> <code>training_data</code> <p>not used for <code>Linear</code> (since the regressor is already trained in <code>state</code>)</p> <p> TYPE: <code>tuple[Dataset, Dataset]</code> </p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def predict(self, x: Dataset, state: LinearState, training_data: tuple[Dataset, Dataset]):\n    \"\"\"Predict the output of the model at points `x` using the linear regressor provided in `state`.\n\n    :param x: the input Dataset `dict` mapping input variables to prediction locations\n    :param state: the state containing the linear regressor to use\n    :param training_data: not used for `Linear` (since the regressor is already trained in `state`)\n    \"\"\"\n    # Convert to (N, xdim) array for sklearn\n    x_arr = np.concatenate([x[var][..., np.newaxis] for var in state.x_vars], axis=-1)\n    loop_shape = x_arr.shape[:-1]\n    x_arr = x_arr.reshape((-1, x_arr.shape[-1]))\n\n    y_arr = state.regressor.predict(x_arr)\n    y_arr = y_arr.reshape(loop_shape + (len(state.y_vars),))  # (..., ydim)\n\n    # Unpack the outputs back into a Dataset\n    return {var: y_arr[..., i] for i, var in enumerate(state.y_vars)}\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.Linear.refine","title":"<code>refine(beta, training_data, old_state, input_domains)</code>","text":"<p>Train a new linear regression model.</p> PARAMETER DESCRIPTION <code>beta</code> <p>if not empty, then the first element is the number of degrees to add to the polynomial features. For example, if <code>beta=(1,)</code>, then the polynomial degree will be increased by 1. If the degree is already set to 1 in <code>polynomial_opts</code> (default), then the new degree will be 2.</p> <p> TYPE: <code>MultiIndex</code> </p> <code>training_data</code> <p>a tuple of dictionaries containing the new training data (<code>xtrain</code>, <code>ytrain</code>)</p> <p> TYPE: <code>tuple[Dataset, Dataset]</code> </p> <code>old_state</code> <p>the old linear state to refine (only used to get the order of input/output variables)</p> <p> TYPE: <code>LinearState</code> </p> <code>input_domains</code> <p>(not used for <code>Linear</code>)</p> <p> TYPE: <code>dict[str, tuple]</code> </p> RETURNS DESCRIPTION <code>InterpolatorState</code> <p>the new linear state</p> Source code in <code>src/amisc/interpolator.py</code> <pre><code>def refine(self, beta: MultiIndex, training_data: tuple[Dataset, Dataset],\n           old_state: LinearState, input_domains: dict[str, tuple]) -&gt; InterpolatorState:\n    \"\"\"Train a new linear regression model.\n\n    :param beta: if not empty, then the first element is the number of degrees to add to the polynomial features.\n                 For example, if `beta=(1,)`, then the polynomial degree will be increased by 1. If the degree\n                 is already set to 1 in `polynomial_opts` (default), then the new degree will be 2.\n    :param training_data: a tuple of dictionaries containing the new training data (`xtrain`, `ytrain`)\n    :param old_state: the old linear state to refine (only used to get the order of input/output variables)\n    :param input_domains: (not used for `Linear`)\n    :returns: the new linear state\n    \"\"\"\n    polynomial_opts = self.polynomial_opts.copy()\n    degree = polynomial_opts.pop('degree', 1)\n    if beta != ():\n        degree += beta[0]\n\n    pipe = []\n    if self.scaler is not None:\n        pipe.append(('scaler', getattr(preprocessing, self.scaler)(**self.scaler_opts)))\n    pipe.extend([('poly', PolynomialFeatures(degree=degree, **polynomial_opts)),\n                 ('linear', getattr(linear_model, self.regressor)(**self.regressor_opts))])\n    regressor = Pipeline(pipe)\n\n    xtrain, ytrain = training_data\n\n    # Get order of variables for inputs and outputs\n    if old_state is not None:\n        x_vars = old_state.x_vars\n        y_vars = old_state.y_vars\n    else:\n        x_vars = list(xtrain.keys())\n        y_vars = list(ytrain.keys())\n\n    # Convert to (N, xdim) and (N, ydim) arrays\n    x_arr = np.concatenate([xtrain[var][..., np.newaxis] for var in x_vars], axis=-1)\n    y_arr = np.concatenate([ytrain[var][..., np.newaxis] for var in y_vars], axis=-1)\n\n    regressor.fit(x_arr, y_arr)\n\n    return LinearState(regressor=regressor, x_vars=x_vars, y_vars=y_vars)\n</code></pre>"},{"location":"reference/interpolator/#amisc.interpolator.LinearState","title":"<code>LinearState(x_vars=list(), y_vars=list(), regressor=None)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>InterpolatorState</code>, <code>Base64Serializable</code></p> <p>The internal state for a linear interpolator (using sklearn).</p> ATTRIBUTE DESCRIPTION <code>x_vars</code> <p>the input variables in order</p> <p> TYPE: <code>list[str]</code> </p> <code>y_vars</code> <p>the output variables in order</p> <p> TYPE: <code>list[str]</code> </p> <code>regressor</code> <p>the sklearn regressor object, a pipeline that consists of a <code>PolynomialFeatures</code> and a model from <code>sklearn.linear_model</code>, i.e. Ridge, Lasso, etc.</p> <p> TYPE: <code>Pipeline</code> </p>"},{"location":"reference/serialize/","title":"serialize","text":""},{"location":"reference/serialize/#amisc.serialize","title":"<code>amisc.serialize</code>","text":"<p>Provides serialization protocols for objects in the package. Serialization in the context of <code>amisc</code> means converting an object to a built-in Python object (e.g. string, dictionary, float, etc.). The serialized objects are then easy to convert to binary or text forms for storage or transmission using various protocols (i.e. pickle, json, yaml, etc.).</p> <p>Includes:</p> <ul> <li><code>Serializable</code> \u2014 mixin interface for serializing and deserializing objects</li> <li><code>Base64Serializable</code> \u2014 mixin class for serializing objects using base64 encoding</li> <li><code>StringSerializable</code> \u2014 mixin class for serializing objects using string representation</li> <li><code>PickleSerializable</code> \u2014 mixin class for serializing objects using pickle files</li> <li><code>YamlSerializable</code> \u2014 metaclass for serializing an object using Yaml load/dump from string</li> </ul>"},{"location":"reference/serialize/#amisc.serialize.Base64Serializable","title":"<code>Base64Serializable</code>","text":"<p>               Bases: <code>Serializable</code></p> <p>Mixin class for serializing objects using base64 encoding.</p>"},{"location":"reference/serialize/#amisc.serialize.PickleSerializable","title":"<code>PickleSerializable</code>","text":"<p>               Bases: <code>Serializable</code></p> <p>Mixin class for serializing objects using pickle.</p>"},{"location":"reference/serialize/#amisc.serialize.Serializable","title":"<code>Serializable</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Mixin interface for serializing and deserializing objects.</p>"},{"location":"reference/serialize/#amisc.serialize.Serializable.deserialize","title":"<code>deserialize(serialized_data)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Construct a <code>Serializable</code> object from serialized data.</p> <p>Passing arguments to deserialize</p> <p>Subclasses should generally not take arguments for deserialization. The serialized object should contain all the information it needs to reconstruct itself. If you need arguments for deserialization, then serialize them along with the object itself and unpack them during the call to deserialize.</p> Source code in <code>src/amisc/serialize.py</code> <pre><code>@classmethod\n@abstractmethod\ndef deserialize(cls, serialized_data: _builtin) -&gt; Serializable:\n    \"\"\"Construct a `Serializable` object from serialized data.\n\n    !!! Note \"Passing arguments to deserialize\"\n        Subclasses should generally not take arguments for deserialization. The serialized object should contain\n        all the information it needs to reconstruct itself. If you need arguments for deserialization, then\n        serialize them along with the object itself and unpack them during the call to deserialize.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/serialize/#amisc.serialize.Serializable.serialize","title":"<code>serialize()</code>  <code>abstractmethod</code>","text":"<p>Serialize to a builtin Python object.</p> Source code in <code>src/amisc/serialize.py</code> <pre><code>@abstractmethod\ndef serialize(self) -&gt; _builtin:\n    \"\"\"Serialize to a builtin Python object.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/serialize/#amisc.serialize.StringSerializable","title":"<code>StringSerializable</code>","text":"<p>               Bases: <code>Serializable</code></p> <p>Mixin class for serializing objects using string representation.</p>"},{"location":"reference/serialize/#amisc.serialize.StringSerializable.deserialize","title":"<code>deserialize(serialized_data, trust=False)</code>  <code>classmethod</code>","text":"<p>Deserialize a string representation of the object.</p> <p>Security Risk</p> <p>Only use <code>trust=True</code> if you trust the source of the serialized data. This provides a more flexible option for <code>eval</code>-ing the serialized data from string. By default, this will instead try to parse the string as a class signature like <code>MyClass(*args, **kwargs)</code>.</p> PARAMETER DESCRIPTION <code>serialized_data</code> <p>the string representation of the object</p> <p> TYPE: <code>str</code> </p> <code>trust</code> <p>whether to trust the source of the serialized data (i.e. for <code>eval</code>)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amisc/serialize.py</code> <pre><code>@classmethod\ndef deserialize(cls, serialized_data: str, trust: bool = False) -&gt; StringSerializable:\n    \"\"\"Deserialize a string representation of the object.\n\n    !!! Warning \"Security Risk\"\n        Only use `trust=True` if you trust the source of the serialized data. This provides a more flexible\n        option for `eval`-ing the serialized data from string. By default, this will instead try to parse the\n        string as a class signature like `MyClass(*args, **kwargs)`.\n\n    :param serialized_data: the string representation of the object\n    :param trust: whether to trust the source of the serialized data (i.e. for `eval`)\n    \"\"\"\n    if trust:\n        return eval(serialized_data)\n    else:\n        try:\n            name, args, kwargs = parse_function_string(serialized_data)\n            return cls(*args, **kwargs)\n        except Exception as e:\n            raise ValueError(f'String \"{serialized_data}\" is not a valid class signature.') from e\n</code></pre>"},{"location":"reference/serialize/#amisc.serialize.YamlSerializable","title":"<code>YamlSerializable(obj)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Serializable</code></p> <p>Mixin for serializing an object using Yaml load/dump from string.</p>"},{"location":"reference/system/","title":"system","text":""},{"location":"reference/system/#amisc.system","title":"<code>amisc.system</code>","text":"<p>The <code>System</code> object is a framework for multidisciplinary models. It manages multiple single discipline component models and the connections between them. It provides a top-level interface for constructing and evaluating surrogates.</p> <p>Features:</p> <ul> <li>Manages multidisciplinary models in a graph data structure, supports feedforward and feedback connections</li> <li>Feedback connections are solved with a fixed-point iteration (FPI) nonlinear solver with anderson acceleration</li> <li>Top-level interface for training and using surrogates of each component model</li> <li>Adaptive experimental design for choosing training data efficiently</li> <li>Convenient testing, plotting, and performance metrics provided to assess quality of surrogates</li> <li>Detailed logging and traceback information</li> <li>Supports parallel or vectorized execution of component models</li> <li>Abstract and flexible interfacing with component models</li> <li>Easy serialization and deserialization to/from YAML files</li> <li>Supports approximating field quantities via compression</li> </ul> <p>Includes:</p> <ul> <li><code>TrainHistory</code> \u2014 a history of training iterations for the system surrogate</li> <li><code>System</code> \u2014 the top-level object for managing multidisciplinary models</li> </ul>"},{"location":"reference/system/#amisc.system.System","title":"<code>System(*args, components=None, root_dir=None, **kwargs)</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Serializable</code></p> <p>Multidisciplinary (MD) surrogate framework top-level class. Construct a <code>System</code> from a list of <code>Component</code> models.</p> <p>Example</p> <pre><code>def f1(x):\n    y = x ** 2\n    return y\ndef f2(y):\n    z = y + 1\n    return z\n\nsystem = System(f1, f2)\n</code></pre> <p>A <code>System</code> object can saved/loaded from <code>.yml</code> files using the <code>!System</code> yaml tag.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>the name of the system</p> <p> TYPE: <code>Annotated[str, Field(default_factory=lambda: 'System_' + join(choices(digits, k=3)))]</code> </p> <code>components</code> <p>list of <code>Component</code> models that make up the MD system</p> <p> TYPE: <code>Callable | Component | list[Callable | Component]</code> </p> <code>train_history</code> <p>history of training iterations for the system surrogate (filled in during training)</p> <p> TYPE: <code>list[dict] | TrainHistory</code> </p> <code>_root_dir</code> <p>root directory where all surrogate build products are saved to file</p> <p> TYPE: <code>Optional[str]</code> </p> <code>_logger</code> <p>logger object for the system</p> <p> TYPE: <code>Optional[Logger]</code> </p> <p>Construct a <code>System</code> object from a list of <code>Component</code> models in <code>*args</code> or <code>components</code>. If a <code>root_dir</code> is provided, then a new directory will be created under <code>root_dir</code> with the name <code>amisc_{timestamp}</code>. This directory will be used to save all build products and log files.</p> PARAMETER DESCRIPTION <code>components</code> <p>list of <code>Component</code> models that make up the MD system</p> <p> DEFAULT: <code>None</code> </p> <code>root_dir</code> <p>root directory where all surrogate build products are saved to file (optional)</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>def __init__(self, /, *args, components=None, root_dir=None, **kwargs):\n    \"\"\"Construct a `System` object from a list of `Component` models in `*args` or `components`. If\n    a `root_dir` is provided, then a new directory will be created under `root_dir` with the name\n    `amisc_{timestamp}`. This directory will be used to save all build products and log files.\n\n    :param components: list of `Component` models that make up the MD system\n    :param root_dir: root directory where all surrogate build products are saved to file (optional)\n    \"\"\"\n    if components is None:\n        components = []\n        for a in args:\n            if isinstance(a, Component) or callable(a):\n                components.append(a)\n            else:\n                try:\n                    components.extend(a)\n                except TypeError as e:\n                    raise ValueError(f\"Invalid component: {a}\") from e\n\n    import amisc\n    amisc_version = kwargs.pop('amisc_version', amisc.__version__)\n    super().__init__(components=components, amisc_version=amisc_version, **kwargs)\n    self.root_dir = root_dir\n</code></pre>"},{"location":"reference/system/#amisc.system.System.refine_level","title":"<code>refine_level: int</code>  <code>property</code>","text":"<p>The total number of training iterations.</p>"},{"location":"reference/system/#amisc.system.System.root_dir","title":"<code>root_dir</code>  <code>property</code> <code>writable</code>","text":"<p>Return the root directory of the surrogate (if available), otherwise <code>None</code>.</p>"},{"location":"reference/system/#amisc.system.System.add_output","title":"<code>add_output()</code>","text":"<p>Add an output variable retroactively to a component surrogate. User should provide a callable that takes a save path and extracts the model output data for given training point/location.</p> Source code in <code>src/amisc/system.py</code> <pre><code>def add_output(self):\n    \"\"\"Add an output variable retroactively to a component surrogate. User should provide a callable that\n    takes a save path and extracts the model output data for given training point/location.\n    \"\"\"\n    # TODO\n    # Loop back through the surrogate training history\n    # Simulate activate_index and extract the model output from file rather than calling the model\n    # Update all interpolator states\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/system/#amisc.system.System.clear","title":"<code>clear()</code>","text":"<p>Clear all surrogate model data and reset the system.</p> Source code in <code>src/amisc/system.py</code> <pre><code>def clear(self):\n    \"\"\"Clear all surrogate model data and reset the system.\"\"\"\n    for comp in self.components:\n        comp.clear()\n    self.train_history.clear()\n</code></pre>"},{"location":"reference/system/#amisc.system.System.coupling_variables","title":"<code>coupling_variables()</code>","text":"<p>Collect all coupling variables from each component in the <code>System</code> and combine them into a single <code>VariableList</code> object.</p> RETURNS DESCRIPTION <code>VariableList</code> <p>A <code>VariableList</code> containing all coupling variables from the components.</p> Source code in <code>src/amisc/system.py</code> <pre><code>def coupling_variables(self) -&gt; VariableList:\n    \"\"\"Collect all coupling variables from each component in the `System` and combine them into a\n    single [`VariableList`][amisc.variable.VariableList] object.\n\n    :returns: A [`VariableList`][amisc.variable.VariableList] containing all coupling variables from the components.\n    \"\"\"\n    all_outputs = self.outputs()\n    return VariableList({k: all_outputs[k] for k in (all_outputs.keys() &amp;\n                         ChainMap(*[comp.inputs for comp in self.components]).keys())})\n</code></pre>"},{"location":"reference/system/#amisc.system.System.deserialize","title":"<code>deserialize(serialized_data)</code>  <code>classmethod</code>","text":"<p>Construct a <code>System</code> object from serialized data.</p> Source code in <code>src/amisc/system.py</code> <pre><code>@classmethod\ndef deserialize(cls, serialized_data: dict) -&gt; System:\n    \"\"\"Construct a `System` object from serialized data.\"\"\"\n    return cls(**serialized_data)\n</code></pre>"},{"location":"reference/system/#amisc.system.System.fit","title":"<code>fit(targets=None, num_refine=100, max_iter=20, max_tol=0.001, runtime_hr=1.0, estimate_bounds=False, update_bounds=True, test_set=None, start_test_check=None, save_interval=0, plot_interval=1, cache_interval=0, executor=None, weight_fcns='pdf')</code>","text":"<p>Train the system surrogate adaptively by iterative refinement until an end condition is met.</p> PARAMETER DESCRIPTION <code>targets</code> <p>list of system output variables to focus refinement on, use all outputs if not specified</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>num_refine</code> <p>number of input samples to compute error indicators on</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>max_iter</code> <p>the maximum number of refinement steps to take</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>max_tol</code> <p>the max allowable value in relative L2 error to achieve</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>runtime_hr</code> <p>the threshold wall clock time (hr) at which to stop further refinement (will go until all models finish the current iteration)</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>estimate_bounds</code> <p>whether to estimate bounds for the coupling variables; will only try to estimate from the <code>test_set</code> if provided (defaults to <code>True</code>). Otherwise, you should manually provide domains for all coupling variables.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>update_bounds</code> <p>whether to continuously update coupling variable bounds during refinement</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>test_set</code> <p><code>tuple</code> of <code>(xtest, ytest)</code> to show convergence of surrogate to the true model. The test set inputs and outputs are specified as <code>dicts</code> of <code>np.ndarrays</code> with keys corresponding to the variable names. Can also pass a path to a <code>.pkl</code> file that has the test set data as {'test_set': (xtest, ytest)}.</p> <p> TYPE: <code>tuple | str | Path</code> DEFAULT: <code>None</code> </p> <code>start_test_check</code> <p>the iteration to start checking the test set error (defaults to the number of components); surrogate evaluation isn't useful during initialization so you should at least allow one iteration per component before checking test set error</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>save_interval</code> <p>number of refinement steps between each progress save, none if 0; <code>System.root_dir</code> must be specified to save to file</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>plot_interval</code> <p>how often to plot the error indicator and test set error (defaults to every iteration); will only plot and save to file if a root directory is set</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>cache_interval</code> <p>how often to cache component data in order to speed up future training iterations (at the cost of additional memory usage); defaults to 0 (no caching)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>executor</code> <p>a <code>concurrent.futures.Executor</code> object to parallelize model evaluations (optional, but recommended for expensive models)</p> <p> TYPE: <code>Executor</code> DEFAULT: <code>None</code> </p> <code>weight_fcns</code> <p>a <code>dict</code> of weight functions to apply to each input variable for training data selection; defaults to using the pdf of each variable. If None, then no weighting is applied.</p> <p> TYPE: <code>dict[str, callable] | Literal['pdf'] | None</code> DEFAULT: <code>'pdf'</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>@_save_on_error\ndef fit(self, targets: list = None,\n        num_refine: int = 100,\n        max_iter: int = 20,\n        max_tol: float = 1e-3,\n        runtime_hr: float = 1.,\n        estimate_bounds: bool = False,\n        update_bounds: bool = True,\n        test_set: tuple | str | Path = None,\n        start_test_check: int = None,\n        save_interval: int = 0,\n        plot_interval: int = 1,\n        cache_interval: int = 0,\n        executor: Executor = None,\n        weight_fcns: dict[str, callable] | Literal['pdf'] | None = 'pdf'):\n    \"\"\"Train the system surrogate adaptively by iterative refinement until an end condition is met.\n\n    :param targets: list of system output variables to focus refinement on, use all outputs if not specified\n    :param num_refine: number of input samples to compute error indicators on\n    :param max_iter: the maximum number of refinement steps to take\n    :param max_tol: the max allowable value in relative L2 error to achieve\n    :param runtime_hr: the threshold wall clock time (hr) at which to stop further refinement (will go\n                       until all models finish the current iteration)\n    :param estimate_bounds: whether to estimate bounds for the coupling variables; will only try to estimate from\n                            the `test_set` if provided (defaults to `True`). Otherwise, you should manually\n                            provide domains for all coupling variables.\n    :param update_bounds: whether to continuously update coupling variable bounds during refinement\n    :param test_set: `tuple` of `(xtest, ytest)` to show convergence of surrogate to the true model. The test set\n                     inputs and outputs are specified as `dicts` of `np.ndarrays` with keys corresponding to the\n                     variable names. Can also pass a path to a `.pkl` file that has the test set data as\n                     {'test_set': (xtest, ytest)}.\n    :param start_test_check: the iteration to start checking the test set error (defaults to the number\n                             of components); surrogate evaluation isn't useful during initialization so you\n                             should at least allow one iteration per component before checking test set error\n    :param save_interval: number of refinement steps between each progress save, none if 0; `System.root_dir`\n                          must be specified to save to file\n    :param plot_interval: how often to plot the error indicator and test set error (defaults to every iteration);\n                          will only plot and save to file if a root directory is set\n    :param cache_interval: how often to cache component data in order to speed up future training iterations (at\n                           the cost of additional memory usage); defaults to 0 (no caching)\n    :param executor: a `concurrent.futures.Executor` object to parallelize model evaluations (optional, but\n                     recommended for expensive models)\n    :param weight_fcns: a `dict` of weight functions to apply to each input variable for training data selection;\n                        defaults to using the pdf of each variable. If None, then no weighting is applied.\n    \"\"\"\n    start_test_check = start_test_check or sum([1 for _ in self.components if _.has_surrogate])\n    targets = targets or self.outputs()\n    xtest, ytest = self._get_test_set(test_set)\n    max_iter = self.refine_level + max_iter\n\n    # Estimate bounds from test set if provided (override current bounds if they are set)\n    if estimate_bounds:\n        if ytest is not None:\n            y_samples = to_surrogate_dataset(ytest, self.outputs(), del_fields=True)[0]  # normalize/compress\n            _combine_latent_arrays(y_samples)\n            coupling_vars = {k: v for k, v in self.coupling_variables().items() if k in y_samples}\n            y_min, y_max = {}, {}\n            for var in coupling_vars.values():\n                y_min[var] = np.nanmin(y_samples[var], axis=0)\n                y_max[var] = np.nanmax(y_samples[var], axis=0)\n                if var.compression is not None:\n                    new_domain = list(zip(y_min[var].tolist(), y_max[var].tolist()))\n                    var.update_domain(new_domain, override=True)\n                else:\n                    new_domain = (float(y_min[var]), float(y_max[var]))\n                    var.update_domain(var.denormalize(new_domain), override=True)\n            del y_samples\n        else:\n            self.logger.warning('Could not estimate bounds for coupling variables: no test set provided. '\n                                'Make sure you manually provide (good) coupling variable domains.')\n\n    # Track convergence progress on the error indicator and test set (plot to file)\n    if self.root_dir is not None:\n        err_record = [res['added_error'] for res in self.train_history]\n        err_fig, err_ax = plt.subplots(figsize=(6, 5), layout='tight')\n\n        if xtest is not None and ytest is not None:\n            num_plot = min(len(targets), 3)\n            test_record = np.full((self.refine_level, num_plot), np.nan)\n            t_fig, t_ax = plt.subplots(1, num_plot, figsize=(3.5 * num_plot, 4), layout='tight', squeeze=False,\n                                       sharey='row')\n            for j, res in enumerate(self.train_history):\n                for i, var in enumerate(targets[:num_plot]):\n                    if (perf := res.get('test_error')) is not None:\n                        test_record[j, i] = perf[var]\n\n    total_overhead = 0.0\n    total_model_wall_time = 0.0\n    t_start = time.time()\n    while True:\n        # Adaptive refinement step\n        t_iter_start = time.time()\n        train_result = self.refine(targets=targets, num_refine=num_refine, update_bounds=update_bounds,\n                                   executor=executor, weight_fcns=weight_fcns)\n        if train_result['component'] is None:\n            self._print_title_str('Termination criteria reached: No candidates left to refine')\n            break\n\n        # Keep track of algorithmic overhead (before and after call_model for this iteration)\n        m_start, m_end = self[train_result['component']].get_model_timestamps()  # Start and end of call_model\n        if m_start is not None and m_end is not None:\n            train_result['overhead_s'] = (m_start - t_iter_start) + (time.time() - m_end)\n            train_result['model_s'] = m_end - m_start\n        else:\n            train_result['overhead_s'] = time.time() - t_iter_start\n            train_result['model_s'] = 0.0\n        total_overhead += train_result['overhead_s']\n        total_model_wall_time += train_result['model_s']\n\n        curr_error = train_result['added_error']\n\n        # Plot progress of error indicator\n        if self.root_dir is not None:\n            err_record.append(curr_error)\n\n            if plot_interval &gt; 0 and self.refine_level % plot_interval == 0:\n                err_ax.clear(); err_ax.set_yscale('log'); err_ax.grid()\n                err_ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n                err_ax.plot(err_record, '-k')\n                err_ax.set_xlabel('Iteration'); err_ax.set_ylabel('Relative error indicator')\n                err_fig.savefig(str(Path(self.root_dir) / 'error_indicator.pdf'), format='pdf', bbox_inches='tight')\n\n        # Save performance on a test set\n        if xtest is not None and ytest is not None:\n            # don't compute if components are uninitialized\n            perf = self.test_set_performance(xtest, ytest) if self.refine_level + 1 &gt;= start_test_check else (\n                {str(var): np.nan for var in ytest if COORDS_STR_ID not in var})\n            train_result['test_error'] = perf.copy()\n\n            if self.root_dir is not None:\n                test_record = np.vstack((test_record, np.array([perf[var] for var in targets[:num_plot]])))\n\n                if plot_interval &gt; 0 and self.refine_level % plot_interval == 0:\n                    for i in range(num_plot):\n                        with warnings.catch_warnings():\n                            warnings.simplefilter(\"ignore\", UserWarning)\n                            t_ax[0, i].clear(); t_ax[0, i].set_yscale('log'); t_ax[0, i].grid()\n                        t_ax[0, i].xaxis.set_major_locator(MaxNLocator(integer=True))\n                        t_ax[0, i].plot(test_record[:, i], '-k')\n                        t_ax[0, i].set_title(self.outputs()[targets[i]].get_tex(units=True))\n                        t_ax[0, i].set_xlabel('Iteration')\n                        t_ax[0, i].set_ylabel('Test set relative error' if i==0 else '')\n                    t_fig.savefig(str(Path(self.root_dir) / 'test_set_error.pdf'),format='pdf',bbox_inches='tight')\n\n        self.train_history.append(train_result)\n\n        if self.root_dir is not None and save_interval &gt; 0 and self.refine_level % save_interval == 0:\n            iter_name = f'{self.name}_iter{self.refine_level}'\n            if not (pth := self.root_dir / 'surrogates' / iter_name).is_dir():\n                os.mkdir(pth)\n            self.save_to_file(f'{iter_name}.yml', save_dir=pth)  # Save to an iteration-specific directory\n\n        if cache_interval &gt; 0 and self.refine_level % cache_interval == 0:\n            for comp in self.components:\n                comp.cache()\n\n        # Check all end conditions\n        if self.refine_level &gt;= max_iter:\n            self._print_title_str(f'Termination criteria reached: Max iteration {self.refine_level}/{max_iter}')\n            break\n        if curr_error &lt; max_tol:\n            self._print_title_str(f'Termination criteria reached: relative error {curr_error} &lt; tol {max_tol}')\n            break\n        if ((time.time() - t_start) / 3600.0) &gt;= runtime_hr:\n            t_end = time.time()\n            actual = datetime.timedelta(seconds=t_end - t_start)\n            target = datetime.timedelta(seconds=runtime_hr * 3600)\n            train_surplus = ((t_end - t_start) - runtime_hr * 3600) / 3600\n            self._print_title_str(f'Termination criteria reached: runtime {str(actual)} &gt; {str(target)}')\n            self.logger.info(f'Surplus wall time: {train_surplus:.3f}/{runtime_hr:.3f} hours '\n                             f'(+{100 * train_surplus / runtime_hr:.2f}%)')\n            break\n\n    self.logger.info(f'Model evaluation algorithm efficiency: '\n                     f'{100 * total_model_wall_time / (total_model_wall_time + total_overhead):.2f}%')\n\n    if self.root_dir is not None:\n        iter_name = f'{self.name}_iter{self.refine_level}'\n        if not (pth := self.root_dir / 'surrogates' / iter_name).is_dir():\n            os.mkdir(pth)\n        self.save_to_file(f'{iter_name}.yml', save_dir=pth)\n\n        if xtest is not None and ytest is not None:\n            self._save_test_set((xtest, ytest))\n\n    self.logger.info(f'Final system surrogate: \\n {self}')\n</code></pre>"},{"location":"reference/system/#amisc.system.System.get_allocation","title":"<code>get_allocation()</code>","text":"<p>Get a breakdown of cost allocation during training.</p> RETURNS DESCRIPTION <p><code>cost_alloc, model_cost, overhead_cost, model_evals</code> - the cost allocation per model/fidelity, the model evaluation cost per iteration (in s of CPU time), the algorithmic overhead cost per iteration, and the total number of model evaluations at each training iteration</p> Source code in <code>src/amisc/system.py</code> <pre><code>def get_allocation(self):\n    \"\"\"Get a breakdown of cost allocation during training.\n\n    :returns: `cost_alloc, model_cost, overhead_cost, model_evals` - the cost allocation per model/fidelity,\n              the model evaluation cost per iteration (in s of CPU time), the algorithmic overhead cost per\n              iteration, and the total number of model evaluations at each training iteration\n    \"\"\"\n    cost_alloc = dict()     # Cost allocation (cpu time in s) per node and model fidelity\n    model_cost = []         # Cost of model evaluations (CPU time in s) per iteration\n    overhead_cost = []      # Algorithm overhead costs (CPU time in s) per iteration\n    model_evals = []        # Number of model evaluations at each training iteration\n\n    prev_cands = {comp.name: IndexSet() for comp in self.components}  # empty candidate sets\n\n    # Add cumulative training costs\n    for train_res, active_sets, cand_sets, misc_coeff_train, misc_coeff_test in self.simulate_fit():\n        comp = train_res['component']\n        alpha = train_res['alpha']\n        beta = train_res['beta']\n        overhead = train_res['overhead_s']\n\n        cost_alloc.setdefault(comp, dict())\n\n        new_cands = cand_sets[comp].union({(alpha, beta)}) - prev_cands[comp]  # newly computed candidates\n\n        iter_cost = 0.\n        iter_eval = 0\n        for alpha_new, beta_new in new_cands:\n            cost_alloc[comp].setdefault(alpha_new, 0.)\n\n            added_eval = self[comp].get_cost(alpha_new, beta_new)\n            single_cost = self[comp].model_costs.get(alpha_new, 1.)\n\n            iter_cost += added_eval * single_cost\n            iter_eval += added_eval\n\n            cost_alloc[comp][alpha_new] += added_eval * single_cost\n\n        overhead_cost.append(overhead)\n        model_cost.append(iter_cost)\n        model_evals.append(iter_eval)\n        prev_cands[comp] = cand_sets[comp].union({(alpha, beta)})\n\n    return cost_alloc, np.atleast_1d(model_cost), np.atleast_1d(overhead_cost), np.atleast_1d(model_evals)\n</code></pre>"},{"location":"reference/system/#amisc.system.System.get_component","title":"<code>get_component(comp_name)</code>","text":"<p>Return the <code>Component</code> object for this component.</p> PARAMETER DESCRIPTION <code>comp_name</code> <p>name of the component to return</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Component</code> <p>the <code>Component</code> object</p> RAISES DESCRIPTION <code>KeyError</code> <p>if the component does not exist</p> Source code in <code>src/amisc/system.py</code> <pre><code>def get_component(self, comp_name: str) -&gt; Component:\n    \"\"\"Return the `Component` object for this component.\n\n    :param comp_name: name of the component to return\n    :raises KeyError: if the component does not exist\n    :returns: the `Component` object\n    \"\"\"\n    if comp_name.lower() == 'system':\n        return self\n    else:\n        for comp in self.components:\n            if comp.name == comp_name:\n                return comp\n        raise KeyError(f\"Component '{comp_name}' not found in system.\")\n</code></pre>"},{"location":"reference/system/#amisc.system.System.graph","title":"<code>graph()</code>","text":"<p>Build a directed graph of the system components based on their input-output relationships.</p> Source code in <code>src/amisc/system.py</code> <pre><code>def graph(self) -&gt; nx.DiGraph:\n    \"\"\"Build a directed graph of the system components based on their input-output relationships.\"\"\"\n    graph = nx.DiGraph()\n    model_deps = {}\n    for comp in self.components:\n        graph.add_node(comp.name)\n        for output in comp.outputs:\n            model_deps[output] = comp.name\n    for comp in self.components:\n        for in_var in comp.inputs:\n            if in_var in model_deps:\n                graph.add_edge(model_deps[in_var], comp.name)\n\n    return graph\n</code></pre>"},{"location":"reference/system/#amisc.system.System.inputs","title":"<code>inputs()</code>","text":"<p>Collect all inputs from each component in the <code>System</code> and combine them into a single <code>VariableList</code> object, excluding variables that are also outputs of any component.</p> RETURNS DESCRIPTION <code>VariableList</code> <p>A <code>VariableList</code> containing all inputs from the components.</p> Source code in <code>src/amisc/system.py</code> <pre><code>def inputs(self) -&gt; VariableList:\n    \"\"\"Collect all inputs from each component in the `System` and combine them into a\n    single [`VariableList`][amisc.variable.VariableList] object, excluding variables that are also outputs of\n    any component.\n\n    :returns: A [`VariableList`][amisc.variable.VariableList] containing all inputs from the components.\n    \"\"\"\n    all_inputs = ChainMap(*[comp.inputs for comp in self.components])\n    return VariableList({k: all_inputs[k] for k in all_inputs.keys() - self.outputs().keys()})\n</code></pre>"},{"location":"reference/system/#amisc.system.System.insert_components","title":"<code>insert_components(components)</code>","text":"<p>Insert new components into the system.</p> Source code in <code>src/amisc/system.py</code> <pre><code>def insert_components(self, components: list | Callable | Component):\n    \"\"\"Insert new components into the system.\"\"\"\n    components = components if isinstance(components, list) else [components]\n    self.components = self.components + components\n</code></pre>"},{"location":"reference/system/#amisc.system.System.load_from_file","title":"<code>load_from_file(filename, root_dir=None, loader=None)</code>  <code>staticmethod</code>","text":"<p>Load surrogate from file. Defaults to yaml loading. Tries to infer <code>amisc</code> directory structure.</p> PARAMETER DESCRIPTION <code>filename</code> <p>the name of the load file</p> <p> TYPE: <code>str | Path</code> </p> <code>root_dir</code> <p>set this as the surrogate's root directory (will try to load from <code>amisc_</code> fmt by default)</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>loader</code> <p>the encoder to use (defaults to the <code>amisc</code> yaml encoder)</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>@staticmethod\ndef load_from_file(filename: str | Path, root_dir: str | Path = None, loader=None):\n    \"\"\"Load surrogate from file. Defaults to yaml loading. Tries to infer `amisc` directory structure.\n\n    :param filename: the name of the load file\n    :param root_dir: set this as the surrogate's root directory (will try to load from `amisc_` fmt by default)\n    :param loader: the encoder to use (defaults to the `amisc` yaml encoder)\n    \"\"\"\n    from amisc import YamlLoader\n    encoder = loader or YamlLoader\n    system = encoder.load(filename)\n    root_dir = root_dir or system.root_dir\n\n    # Try to infer amisc_root/surrogates/iter/filename structure\n    if root_dir is None:\n        parts = Path(filename).resolve().parts\n        if len(parts) &gt; 1 and parts[-2].startswith('amisc_'):\n            root_dir = Path(filename).resolve().parent\n        elif len(parts) &gt; 2 and parts[-3].startswith('amisc_'):\n            root_dir = Path(filename).resolve().parent.parent\n        elif len(parts) &gt; 3 and parts[-4].startswith('amisc_'):\n            root_dir = Path(filename).resolve().parent.parent.parent\n\n    system.root_dir = root_dir\n    return system\n</code></pre>"},{"location":"reference/system/#amisc.system.System.outputs","title":"<code>outputs()</code>","text":"<p>Collect all outputs from each component in the <code>System</code> and combine them into a single <code>VariableList</code> object.</p> RETURNS DESCRIPTION <code>VariableList</code> <p>A <code>VariableList</code> containing all outputs from the components.</p> Source code in <code>src/amisc/system.py</code> <pre><code>def outputs(self) -&gt; VariableList:\n    \"\"\"Collect all outputs from each component in the `System` and combine them into a\n    single [`VariableList`][amisc.variable.VariableList] object.\n\n    :returns: A [`VariableList`][amisc.variable.VariableList] containing all outputs from the components.\n    \"\"\"\n    return VariableList({k: v for k, v in ChainMap(*[comp.outputs for comp in self.components]).items()})\n</code></pre>"},{"location":"reference/system/#amisc.system.System.plot_allocation","title":"<code>plot_allocation(cmap='Blues', text_bar_width=0.06, arrow_bar_width=0.02)</code>","text":"<p>Plot bar charts showing cost allocation during training.</p> <p>Beta feature</p> <p>This has pretty good default settings, but it might look terrible for your use. Mostly provided here as a template for making cost allocation bar charts. Please feel free to copy and edit in your own code.</p> PARAMETER DESCRIPTION <code>cmap</code> <p>the colormap string identifier for <code>plt</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'Blues'</code> </p> <code>text_bar_width</code> <p>the minimum total cost fraction above which a bar will print centered model fidelity text</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.06</code> </p> <code>arrow_bar_width</code> <p>the minimum total cost fraction above which a bar will try to print text with an arrow; below this amount, the bar is too skinny and won't print any text</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.02</code> </p> RETURNS DESCRIPTION <p><code>fig, ax</code>, Figure and Axes objects</p> Source code in <code>src/amisc/system.py</code> <pre><code>def plot_allocation(self, cmap: str = 'Blues', text_bar_width: float = 0.06, arrow_bar_width: float = 0.02):\n    \"\"\"Plot bar charts showing cost allocation during training.\n\n    !!! Warning \"Beta feature\"\n        This has pretty good default settings, but it might look terrible for your use. Mostly provided here as\n        a template for making cost allocation bar charts. Please feel free to copy and edit in your own code.\n\n    :param cmap: the colormap string identifier for `plt`\n    :param text_bar_width: the minimum total cost fraction above which a bar will print centered model fidelity text\n    :param arrow_bar_width: the minimum total cost fraction above which a bar will try to print text with an arrow;\n                            below this amount, the bar is too skinny and won't print any text\n    :returns: `fig, ax`, Figure and Axes objects\n    \"\"\"\n    # Get total cost\n    cost_alloc, model_cost, _, _ = self.get_allocation()\n    total_cost = np.cumsum(model_cost)[-1]\n\n    # Remove nodes with cost=0 from alloc dicts (i.e. analytical models)\n    remove_nodes = []\n    for node, alpha_dict in cost_alloc.items():\n        if len(alpha_dict) == 0:\n            remove_nodes.append(node)\n    for node in remove_nodes:\n        del cost_alloc[node]\n\n    # Bar chart showing cost allocation breakdown for MF system at final iteration\n    fig, ax = plt.subplots(figsize=(6, 5), layout='tight')\n    width = 0.7\n    x = np.arange(len(cost_alloc))\n    xlabels = list(cost_alloc.keys())  # One bar for each component\n    cmap = plt.get_cmap(cmap)\n\n    for j, (node, alpha_dict) in enumerate(cost_alloc.items()):\n        bottom = 0\n        c_intervals = np.linspace(0, 1, len(alpha_dict))\n        bars = [(alpha, cost, cost / total_cost) for alpha, cost in alpha_dict.items()]\n        bars = sorted(bars, key=lambda ele: ele[2], reverse=True)\n        for i, (alpha, cost, frac) in enumerate(bars):\n            p = ax.bar(x[j], frac, width, color=cmap(c_intervals[i]), linewidth=1,\n                       edgecolor=[0, 0, 0], bottom=bottom)\n            bottom += frac\n            num_evals = round(cost / self[node].model_costs.get(alpha, 1.))\n            if frac &gt; text_bar_width:\n                ax.bar_label(p, labels=[f'{alpha}, {num_evals}'], label_type='center')\n            elif frac &gt; arrow_bar_width:\n                xy = (x[j] + width / 2, bottom - frac / 2)  # Label smaller bars with a text off to the side\n                ax.annotate(f'{alpha}, {num_evals}', xy, xytext=(xy[0] + 0.2, xy[1]),\n                            arrowprops={'arrowstyle': '-&gt;', 'linewidth': 1})\n            else:\n                pass  # Don't label really small bars\n    ax.set_xlabel('')\n    ax.set_ylabel('Fraction of total cost')\n    ax.set_xticks(x, xlabels)\n    ax.set_xlim(left=-1, right=x[-1] + 1)\n\n    if self.root_dir is not None:\n        fig.savefig(Path(self.root_dir) / 'mf_allocation.pdf', bbox_inches='tight', format='pdf')\n\n    return fig, ax\n</code></pre>"},{"location":"reference/system/#amisc.system.System.plot_slice","title":"<code>plot_slice(inputs=None, outputs=None, num_steps=20, show_surr=True, show_model=None, save_dir=None, executor=None, nominal=None, random_walk=False, from_file=None, subplot_size_in=3.0)</code>","text":"<p>Helper function to plot 1d slices of the surrogate and/or model outputs over the inputs. A single \"slice\" works by smoothly stepping from the lower bound of an input to its upper bound, while holding all other inputs constant at their nominal values (or smoothly varying them if <code>random_walk=True</code>). This function is useful for visualizing the behavior of the system surrogate and/or model(s) over a single input variable at a time.</p> PARAMETER DESCRIPTION <code>inputs</code> <p>list of input variables to take 1d slices of (defaults to first 3 in <code>System.inputs</code>)</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> <code>outputs</code> <p>list of model output variables to plot 1d slices of (defaults to first 3 in <code>System.outputs</code>)</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> <code>num_steps</code> <p>the number of points to take in the 1d slice for each input variable; this amounts to a total of <code>num_steps*len(inputs)</code> model/surrogate evaluations</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>show_surr</code> <p>whether to show the surrogate prediction</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_model</code> <p>also compute and plot model predictions, <code>list</code> of ['best', 'worst', tuple(alpha), etc.]</p> <p> TYPE: <code>str | tuple | list</code> DEFAULT: <code>None</code> </p> <code>save_dir</code> <p>base directory to save model outputs and plots (if specified)</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>executor</code> <p>a <code>concurrent.futures.Executor</code> object to parallelize model or surrogate evaluations</p> <p> TYPE: <code>Executor</code> DEFAULT: <code>None</code> </p> <code>nominal</code> <p><code>dict</code> of <code>var-&gt;nominal</code> to use as constant values for all non-sliced variables (use unnormalized values only; use <code>var_LATENT0</code> to specify nominal latent values)</p> <p> TYPE: <code>dict[str:float]</code> DEFAULT: <code>None</code> </p> <code>random_walk</code> <p>whether to slice in a random d-dimensional direction instead of holding all non-slice variables const at <code>nominal</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>from_file</code> <p>path to a <code>.pkl</code> file to load a saved slice from disk</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>subplot_size_in</code> <p>side length size of each square subplot in inches</p> <p> TYPE: <code>float</code> DEFAULT: <code>3.0</code> </p> RETURNS DESCRIPTION <p><code>fig, ax</code> with <code>len(inputs)</code> by <code>len(outputs)</code> subplots</p> Source code in <code>src/amisc/system.py</code> <pre><code>def plot_slice(self, inputs: list[str] = None,\n               outputs: list[str] = None,\n               num_steps: int = 20,\n               show_surr: bool = True,\n               show_model: str | tuple | list = None,\n               save_dir: str | Path = None,\n               executor: Executor = None,\n               nominal: dict[str: float] = None,\n               random_walk: bool = False,\n               from_file: str | Path = None,\n               subplot_size_in: float = 3.):\n    \"\"\"Helper function to plot 1d slices of the surrogate and/or model outputs over the inputs. A single\n    \"slice\" works by smoothly stepping from the lower bound of an input to its upper bound, while holding all other\n    inputs constant at their nominal values (or smoothly varying them if `random_walk=True`).\n    This function is useful for visualizing the behavior of the system surrogate and/or model(s) over a\n    single input variable at a time.\n\n    :param inputs: list of input variables to take 1d slices of (defaults to first 3 in `System.inputs`)\n    :param outputs: list of model output variables to plot 1d slices of (defaults to first 3 in `System.outputs`)\n    :param num_steps: the number of points to take in the 1d slice for each input variable; this amounts to a total\n                      of `num_steps*len(inputs)` model/surrogate evaluations\n    :param show_surr: whether to show the surrogate prediction\n    :param show_model: also compute and plot model predictions, `list` of ['best', 'worst', tuple(alpha), etc.]\n    :param save_dir: base directory to save model outputs and plots (if specified)\n    :param executor: a `concurrent.futures.Executor` object to parallelize model or surrogate evaluations\n    :param nominal: `dict` of `var-&gt;nominal` to use as constant values for all non-sliced variables (use\n                    unnormalized values only; use `var_LATENT0` to specify nominal latent values)\n    :param random_walk: whether to slice in a random d-dimensional direction instead of holding all non-slice\n                        variables const at `nominal`\n    :param from_file: path to a `.pkl` file to load a saved slice from disk\n    :param subplot_size_in: side length size of each square subplot in inches\n    :returns: `fig, ax` with `len(inputs)` by `len(outputs)` subplots\n    \"\"\"\n    # Manage loading important quantities from file (if provided)\n    input_slices, output_slices_model, output_slices_surr = None, None, None\n    if from_file is not None:\n        with open(Path(from_file), 'rb') as fd:\n            slice_data = pickle.load(fd)\n            inputs = slice_data['inputs']           # Must use same input slices as save file\n            show_model = slice_data['show_model']   # Must use same model data as save file\n            outputs = slice_data.get('outputs') if outputs is None else outputs\n            input_slices = slice_data['input_slices']\n            save_dir = None  # Don't run or save any models if loading from file\n\n    # Set default values (take up to the first 3 inputs by default)\n    all_inputs = self.inputs()\n    all_outputs = self.outputs()\n    rand_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))\n    if save_dir is not None:\n        os.mkdir(Path(save_dir) / f'slice_{rand_id}')\n    if nominal is None:\n        nominal = dict()\n    inputs = all_inputs[:3] if inputs is None else inputs\n    outputs = all_outputs[:3] if outputs is None else outputs\n\n    if show_model is not None and not isinstance(show_model, list):\n        show_model = [show_model]\n\n    # Handle field quantities (directly use latent variables or only the first one)\n    for i, var in enumerate(list(inputs)):\n        if LATENT_STR_ID not in str(var) and all_inputs[var].compression is not None:\n            inputs[i] = f'{var}{LATENT_STR_ID}0'\n    for i, var in enumerate(list(outputs)):\n        if LATENT_STR_ID not in str(var) and all_outputs[var].compression is not None:\n            outputs[i] = f'{var}{LATENT_STR_ID}0'\n\n    bds = all_inputs.get_domains()\n    xlabels = [all_inputs[var].get_tex(units=False) if LATENT_STR_ID not in str(var) else\n               all_inputs[str(var).split(LATENT_STR_ID)[0]].get_tex(units=False) +\n               f' (latent {str(var).split(LATENT_STR_ID)[1]})' for var in inputs]\n\n    ylabels = [all_outputs[var].get_tex(units=False) if LATENT_STR_ID not in str(var) else\n               all_outputs[str(var).split(LATENT_STR_ID)[0]].get_tex(units=False) +\n               f' (latent {str(var).split(LATENT_STR_ID)[1]})' for var in outputs]\n\n    # Construct slices of model inputs (if not provided)\n    if input_slices is None:\n        input_slices = {}  # Each input variable with shape (num_steps, num_slice)\n        for i in range(len(inputs)):\n            if random_walk:\n                # Make a random straight-line walk across d-cube\n                r0 = self.sample_inputs((1,), use_pdf=False)\n                rf = self.sample_inputs((1,), use_pdf=False)\n\n                for var, bd in bds.items():\n                    if var == inputs[i]:\n                        r0[var] = np.atleast_1d(bd[0])          # Start slice at this lower bound\n                        rf[var] = np.atleast_1d(bd[1])          # Slice up to this upper bound\n\n                    step_size = (rf[var] - r0[var]) / (num_steps - 1)\n                    arr = r0[var] + step_size * np.arange(num_steps)\n\n                    input_slices[var] = arr[..., np.newaxis] if input_slices.get(var) is None else (\n                        np.concatenate((input_slices[var], arr[..., np.newaxis]), axis=-1))\n            else:\n                # Otherwise, only slice one variable\n                for var, bd in bds.items():\n                    nom = nominal.get(var, np.mean(bd)) if LATENT_STR_ID in str(var) else (\n                        all_inputs[var].normalize(nominal.get(var, all_inputs[var].get_nominal())))\n                    arr = np.linspace(bd[0], bd[1], num_steps) if var == inputs[i] else np.full(num_steps, nom)\n\n                    input_slices[var] = arr[..., np.newaxis] if input_slices.get(var) is None else (\n                        np.concatenate((input_slices[var], arr[..., np.newaxis]), axis=-1))\n\n    # Walk through each model that is requested by show_model\n    if show_model is not None:\n        if from_file is not None:\n            output_slices_model = slice_data['output_slices_model']\n        else:\n            output_slices_model = list()\n            for model in show_model:\n                output_dir = None\n                if save_dir is not None:\n                    output_dir = (Path(save_dir) / f'slice_{rand_id}' /\n                                  str(model).replace('{', '').replace('}', '').replace(':', '=').replace(\"'\", ''))\n                    os.mkdir(output_dir)\n                output_slices_model.append(self.predict(input_slices, use_model=model, model_dir=output_dir,\n                                                        executor=executor))\n    if show_surr:\n        output_slices_surr = self.predict(input_slices, executor=executor) \\\n            if from_file is None else slice_data['output_slices_surr']\n\n    # Make len(outputs) by len(inputs) grid of subplots\n    fig, axs = plt.subplots(len(outputs), len(inputs), sharex='col', sharey='row', squeeze=False)\n    for i, output_var in enumerate(outputs):\n        for j, input_var in enumerate(inputs):\n            ax = axs[i, j]\n            x = input_slices[input_var][:, j]\n\n            if show_model is not None:\n                c = np.array([[0, 0, 0, 1], [0.5, 0.5, 0.5, 1]]) if len(show_model) &lt;= 2 else (\n                    plt.get_cmap('jet')(np.linspace(0, 1, len(show_model))))\n                for k in range(len(show_model)):\n                    model_str = (str(show_model[k]).replace('{', '').replace('}', '')\n                                 .replace(':', '=').replace(\"'\", ''))\n                    model_ret = to_surrogate_dataset(output_slices_model[k], all_outputs)[0]\n                    y_model = model_ret[output_var][:, j]\n                    label = {'best': 'High-fidelity' if len(show_model) &gt; 1 else 'Model',\n                             'worst': 'Low-fidelity'}.get(model_str, model_str)\n                    ax.plot(x, y_model, ls='-', c=c[k, :], label=label)\n\n            if show_surr:\n                y_surr = output_slices_surr[output_var][:, j]\n                ax.plot(x, y_surr, '--r', label='Surrogate')\n\n            ax.set_xlabel(xlabels[j] if i == len(outputs) - 1 else '')\n            ax.set_ylabel(ylabels[i] if j == 0 else '')\n            if i == 0 and j == len(inputs) - 1:\n                ax.legend()\n    fig.set_size_inches(subplot_size_in * len(inputs), subplot_size_in * len(outputs))\n    fig.tight_layout()\n\n    # Save results (unless we were already loading from a save file)\n    if from_file is None and save_dir is not None:\n        fname = f'in={\",\".join([str(v) for v in inputs])}_out={\",\".join([str(v) for v in outputs])}'\n        fname = f'slice_rand{rand_id}_' + fname if random_walk else f'slice_nom{rand_id}_' + fname\n        fdir = Path(save_dir) / f'slice_{rand_id}'\n        fig.savefig(fdir / f'{fname}.pdf', bbox_inches='tight', format='pdf')\n        save_dict = {'inputs': inputs, 'outputs': outputs, 'show_model': show_model, 'show_surr': show_surr,\n                     'nominal': nominal, 'random_walk': random_walk, 'input_slices': input_slices,\n                     'output_slices_model': output_slices_model, 'output_slices_surr': output_slices_surr}\n        with open(fdir / f'{fname}.pkl', 'wb') as fd:\n            pickle.dump(save_dict, fd)\n\n    return fig, axs\n</code></pre>"},{"location":"reference/system/#amisc.system.System.predict","title":"<code>predict(x, max_fpi_iter=100, anderson_mem=10, fpi_tol=1e-10, use_model=None, model_dir=None, verbose=False, index_set='test', misc_coeff=None, normalized_inputs=True, incremental=False, targets=None, executor=None, var_shape=None)</code>","text":"<p>Evaluate the system surrogate at inputs <code>x</code>. Return <code>y = system(x)</code>.</p> <p>Computing the true model with feedback loops</p> <p>You can use this function to predict outputs for your MD system using the full-order models rather than the surrogate, by specifying <code>use_model</code>. This is convenient because the <code>System</code> manages all the coupled information flow between models automatically. However, it is highly recommended to not use the full model if your system contains feedback loops. The FPI nonlinear solver would be infeasible using anything more computationally demanding than the surrogate.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>dict</code> of input samples for each variable in the system</p> <p> TYPE: <code>dict | Dataset</code> </p> <code>max_fpi_iter</code> <p>the limit on convergence for the fixed-point iteration routine</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>anderson_mem</code> <p>hyperparameter for tuning the convergence of FPI with anderson acceleration</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>fpi_tol</code> <p>tolerance limit for convergence of fixed-point iteration</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-10</code> </p> <code>use_model</code> <p>'best'=highest-fidelity, 'worst'=lowest-fidelity, tuple=specific fidelity, None=surrogate, specify a <code>dict</code> of the above to assign different model fidelities for diff components</p> <p> TYPE: <code>str | tuple | dict</code> DEFAULT: <code>None</code> </p> <code>model_dir</code> <p>directory to save model outputs if <code>use_model</code> is specified</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>verbose</code> <p>whether to print out iteration progress during execution</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>index_set</code> <p><code>dict(comp=[indices])</code> to override the active set for a component, defaults to using the <code>test</code> set for every component. Can also specify <code>train</code> for any component or a valid <code>IndexSet</code> object. If <code>incremental</code> is specified, will be overwritten with <code>train</code>.</p> <p> TYPE: <code>dict[str:IndexSet | Literal['train', 'test']]</code> DEFAULT: <code>'test'</code> </p> <code>misc_coeff</code> <p><code>dict(comp=MiscTree)</code> to override the default coefficients for a component, passes through along with <code>index_set</code> and <code>incremental</code> to <code>comp.predict()</code>.</p> <p> TYPE: <code>dict[str:MiscTree]</code> DEFAULT: <code>None</code> </p> <code>normalized_inputs</code> <p>true if the passed inputs are compressed/normalized for surrogate evaluation (default), such as inputs returned by <code>sample_inputs</code>. Set to <code>False</code> if you are passing inputs as the true models would expect them instead (i.e. not normalized).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>incremental</code> <p>whether to add <code>index_set</code> to the current active set for each component (temporarily); this will set <code>index_set='train'</code> for all other components (since incremental will augment the \"training\" active sets, not the \"testing\" candidate sets)</p> <p> TYPE: <code>dict[str, bool]</code> DEFAULT: <code>False</code> </p> <code>targets</code> <p>list of output variables to return, defaults to returning all system outputs</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> <code>executor</code> <p>a <code>concurrent.futures.Executor</code> object to parallelize model evaluations</p> <p> TYPE: <code>Executor</code> DEFAULT: <code>None</code> </p> <code>var_shape</code> <p>(Optional) <code>dict</code> of shapes for field quantity inputs in <code>x</code> -- you would only specify this if passing field qtys directly to the models (i.e. not using <code>sample_inputs</code>)</p> <p> TYPE: <code>dict[str, tuple]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p><code>dict</code> of output variables - the surrogate approximation of the system outputs (or the true model)</p> Source code in <code>src/amisc/system.py</code> <pre><code>def predict(self, x: dict | Dataset,\n            max_fpi_iter: int = 100,\n            anderson_mem: int = 10,\n            fpi_tol: float = 1e-10,\n            use_model: str | tuple | dict = None,\n            model_dir: str | Path = None,\n            verbose: bool = False,\n            index_set: dict[str: IndexSet | Literal['train', 'test']] = 'test',\n            misc_coeff: dict[str: MiscTree] = None,\n            normalized_inputs: bool = True,\n            incremental: dict[str, bool] = False,\n            targets: list[str] = None,\n            executor: Executor = None,\n            var_shape: dict[str, tuple] = None) -&gt; Dataset:\n    \"\"\"Evaluate the system surrogate at inputs `x`. Return `y = system(x)`.\n\n    !!! Warning \"Computing the true model with feedback loops\"\n        You can use this function to predict outputs for your MD system using the full-order models rather than the\n        surrogate, by specifying `use_model`. This is convenient because the `System` manages all the\n        coupled information flow between models automatically. However, it is *highly* recommended to not use\n        the full model if your system contains feedback loops. The FPI nonlinear solver would be infeasible using\n        anything more computationally demanding than the surrogate.\n\n    :param x: `dict` of input samples for each variable in the system\n    :param max_fpi_iter: the limit on convergence for the fixed-point iteration routine\n    :param anderson_mem: hyperparameter for tuning the convergence of FPI with anderson acceleration\n    :param fpi_tol: tolerance limit for convergence of fixed-point iteration\n    :param use_model: 'best'=highest-fidelity, 'worst'=lowest-fidelity, tuple=specific fidelity, None=surrogate,\n                       specify a `dict` of the above to assign different model fidelities for diff components\n    :param model_dir: directory to save model outputs if `use_model` is specified\n    :param verbose: whether to print out iteration progress during execution\n    :param index_set: `dict(comp=[indices])` to override the active set for a component, defaults to using the\n                      `test` set for every component. Can also specify `train` for any component or a valid\n                      `IndexSet` object. If `incremental` is specified, will be overwritten with `train`.\n    :param misc_coeff: `dict(comp=MiscTree)` to override the default coefficients for a component, passes through\n                       along with `index_set` and `incremental` to `comp.predict()`.\n    :param normalized_inputs: true if the passed inputs are compressed/normalized for surrogate evaluation\n                              (default), such as inputs returned by `sample_inputs`. Set to `False` if you are\n                              passing inputs as the true models would expect them instead (i.e. not normalized).\n    :param incremental: whether to add `index_set` to the current active set for each component (temporarily);\n                        this will set `index_set='train'` for all other components (since incremental will\n                        augment the \"training\" active sets, not the \"testing\" candidate sets)\n    :param targets: list of output variables to return, defaults to returning all system outputs\n    :param executor: a `concurrent.futures.Executor` object to parallelize model evaluations\n    :param var_shape: (Optional) `dict` of shapes for field quantity inputs in `x` -- you would only specify this\n                      if passing field qtys directly to the models (i.e. not using `sample_inputs`)\n    :returns: `dict` of output variables - the surrogate approximation of the system outputs (or the true model)\n    \"\"\"\n    # Format inputs and allocate space\n    var_shape = var_shape or {}\n    x, loop_shape = format_inputs(x, var_shape=var_shape)  # {'x': (N, *var_shape)}\n    y = {}\n    all_inputs = ChainMap(x, y)   # track all inputs (including coupling vars in y)\n    N = int(np.prod(loop_shape))\n    t1 = 0\n    output_dir = None\n    norm_status = {var: normalized_inputs for var in x}  # keep track of whether inputs are normalized or not\n    graph = self.graph()\n\n    # Keep track of what outputs are computed\n    is_computed = {}\n    for var in (targets or self.outputs()):\n        if (v := self.outputs().get(var, None)) is not None:\n            if v.compression is not None:\n                for field in v.compression.fields:\n                    is_computed[field] = False\n            else:\n                is_computed[var] = False\n\n    def _set_default(struct: dict, default=None):\n        \"\"\"Helper to set a default value for each component key in a `dict`. Ensures all components have a value.\"\"\"\n        if struct is not None:\n            if not isinstance(struct, dict):\n                struct = {node: struct for node in graph.nodes}  # use same for each component\n        else:\n            struct = {node: default for node in graph.nodes}\n        return {node: struct.get(node, default) for node in graph.nodes}\n\n    # Ensure use_model, index_set, and incremental are specified for each component model\n    use_model = _set_default(use_model, None)\n    incremental = _set_default(incremental, False)  # default to train if incremental anywhere\n    index_set = _set_default(index_set, 'train' if any([incremental[node] for node in graph.nodes]) else 'test')\n    misc_coeff = _set_default(misc_coeff, None)\n\n    samples = _Converged(N)  # track convergence of samples\n\n    def _gather_comp_inputs(comp, coupling=None):\n        \"\"\"Helper to gather inputs for a component, making sure they are normalized correctly. Any coupling\n        variables passed in will be used in preference over `all_inputs`.\n        \"\"\"\n        # Will access but not modify: all_inputs, use_model, norm_status\n        field_coords = {}\n        comp_input = {}\n        coupling = coupling or {}\n\n        # Take coupling variables as a priority\n        comp_input.update({var: np.copy(arr[samples.curr_idx, ...]) for var, arr in\n                           coupling.items() if str(var).split(LATENT_STR_ID)[0] in comp.inputs})\n        # Gather all other inputs\n        for var, arr in all_inputs.items():\n            var_id = str(var).split(LATENT_STR_ID)[0]\n            if var_id in comp.inputs and var not in coupling:\n                comp_input[var] = np.copy(arr[samples.curr_idx, ...])\n\n        # Gather field coordinates\n        for var in comp.inputs:\n            coords_str = f'{var}{COORDS_STR_ID}'\n            if (coords := all_inputs.get(coords_str)) is not None:\n                field_coords[coords_str] = coords[samples.curr_idx, ...]\n            elif (coords := comp.model_kwargs.get(coords_str)) is not None:\n                field_coords[coords_str] = coords\n\n        # Gather extra fields (will never be in coupling since field couplings should always be latent coeff)\n        for var in comp.inputs:\n            if var not in comp_input and var.compression is not None:\n                for field in var.compression.fields:\n                    if field in all_inputs:\n                        comp_input[field] = np.copy(all_inputs[field][samples.curr_idx, ...])\n\n        call_model = use_model.get(comp.name, None) is not None\n\n        # Make sure we format all inputs for model evaluation (i.e. denormalize)\n        if call_model:\n            norm_inputs = {var: arr for var, arr in comp_input.items() if norm_status[var]}\n            if len(norm_inputs) &gt; 0:\n                denorm_inputs, fc = to_model_dataset(norm_inputs, comp.inputs, del_latent=True, **field_coords)\n                for var in norm_inputs:\n                    del comp_input[var]\n                field_coords.update(fc)\n                comp_input.update(denorm_inputs)\n\n        # Otherwise, make sure we format inputs for surrogate evaluation (i.e. normalize)\n        else:\n            denorm_inputs = {var: arr for var, arr in comp_input.items() if not norm_status[var]}\n            if len(denorm_inputs) &gt; 0:\n                norm_inputs, _ = to_surrogate_dataset(denorm_inputs, comp.inputs, del_fields=True, **field_coords)\n\n                for var in denorm_inputs:\n                    del comp_input[var]\n                comp_input.update(norm_inputs)\n\n        return comp_input, field_coords, call_model\n\n    # Convert system into DAG by grouping strongly-connected-components\n    dag = nx.condensation(graph)\n\n    # Compute component models in topological order\n    for supernode in nx.topological_sort(dag):\n        if np.all(list(is_computed.values())):\n            break  # Exit early if all selected return qois are computed\n\n        scc = [n for n in dag.nodes[supernode]['members']]\n        samples.reset_convergence()\n\n        # Compute single component feedforward output (no FPI needed)\n        if len(scc) == 1:\n            if verbose:\n                self.logger.info(f\"Running component '{scc[0]}'...\")\n                t1 = time.time()\n\n            # Gather inputs\n            comp = self[scc[0]]\n            comp_input, field_coords, call_model = _gather_comp_inputs(comp)\n\n            # Compute outputs\n            if model_dir is not None:\n                output_dir = Path(model_dir) / scc[0]\n                if not output_dir.exists():\n                    os.mkdir(output_dir)\n            comp_output = comp.predict(comp_input, use_model=use_model.get(scc[0]), model_dir=output_dir,\n                                       index_set=index_set.get(scc[0]), incremental=incremental.get(scc[0]),\n                                       misc_coeff=misc_coeff.get(scc[0]), executor=executor, **field_coords)\n\n            for var, arr in comp_output.items():\n                if var == 'errors':\n                    if y.get(var) is None:\n                        y.setdefault(var, np.full((N,), None, dtype=object))\n                    global_indices = np.arange(N)[samples.curr_idx]\n\n                    for local_idx, err_info in arr.items():\n                        global_idx = int(global_indices[local_idx])\n                        err_info['index'] = global_idx\n                        y[var][global_idx] = err_info\n                    continue\n\n                is_numeric = np.issubdtype(arr.dtype, np.number)\n                if is_numeric:  # for scalars or vectorized field quantities\n                    output_shape = arr.shape[1:]\n                    if y.get(var) is None:\n                        y.setdefault(var, np.full((N, *output_shape), np.nan))\n                    y[var][samples.curr_idx, ...] = arr\n\n                else:  # for fields returned as object arrays\n                    if y.get(var) is None:\n                        y.setdefault(var, np.full((N,), None, dtype=object))\n                    y[var][samples.curr_idx] = arr\n\n            # Update valid indices and status for component outputs\n            for var in comp_output:\n                if str(var).split(LATENT_STR_ID)[0] in comp.outputs:\n                    is_numeric = np.issubdtype(y[var].dtype, np.number)\n                    new_valid = ~np.any(np.isnan(y[var]), axis=tuple(range(1, y[var].ndim))) if is_numeric else (\n                        [False if arr is None else ~np.any(np.isnan(arr)) for i, arr in enumerate(y[var])]\n                    )\n                    samples.valid_idx = np.logical_and(samples.valid_idx, new_valid)\n\n                    is_computed[str(var).split(LATENT_STR_ID)[0]] = True\n                    norm_status[var] = not call_model\n\n            if verbose:\n                self.logger.info(f\"Component '{scc[0]}' completed. Runtime: {time.time() - t1} s\")\n\n        # Handle FPI for SCCs with more than one component\n        else:\n            # Set the initial guess for all coupling vars (middle of domain)\n            scc_inputs = ChainMap(*[self[comp].inputs for comp in scc])\n            scc_outputs = ChainMap(*[self[comp].outputs for comp in scc])\n            coupling_vars = [scc_inputs.get(var) for var in (scc_inputs.keys() - x.keys()) if var in scc_outputs]\n            coupling_prev = {}\n            for var in coupling_vars:\n                if (domain := var.get_domain()) is None:\n                    raise RuntimeError(f\"Coupling variable '{var}' has an empty domain. All coupling variables \"\n                                       f\"require a domain for the fixed-point iteration (FPI) solver.\")\n\n                if isinstance(domain, list):  # Latent coefficients are the coupling variables\n                    for i, d in enumerate(domain):\n                        lb, ub = d\n                        coupling_prev[f'{var.name}{LATENT_STR_ID}{i}'] = np.broadcast_to((lb + ub) / 2, (N,)).copy()\n                        norm_status[f'{var.name}{LATENT_STR_ID}{i}'] = True\n                else:\n                    lb, ub = var.normalize(domain)\n                    shape = (N,) + (1,) * len(var_shape.get(var, ()))\n                    coupling_prev[var] = np.broadcast_to((lb + ub) / 2, shape).copy()\n                    norm_status[var] = True\n\n            residual_hist = deque(maxlen=anderson_mem)\n            coupling_hist = deque(maxlen=anderson_mem)\n\n            def _end_conditions_met():\n                \"\"\"Helper to compute residual, update history, and check end conditions.\"\"\"\n                residual = {}\n                converged_idx = np.full(N, True)\n                for var in coupling_prev:\n                    residual[var] = y[var] - coupling_prev[var]\n                    var_conv = np.all(np.abs(residual[var]) &lt;= fpi_tol, axis=tuple(range(1, residual[var].ndim)))\n                    converged_idx = np.logical_and(converged_idx, var_conv)\n                    samples.valid_idx = np.logical_and(samples.valid_idx, ~np.isnan(coupling_prev[var]))\n                samples.converged_idx = np.logical_or(samples.converged_idx, converged_idx)\n\n                for var in coupling_prev:\n                    coupling_prev[var][samples.curr_idx, ...] = y[var][samples.curr_idx, ...]\n                residual_hist.append(copy.deepcopy(residual))\n                coupling_hist.append(copy.deepcopy(coupling_prev))\n\n                if int(np.sum(samples.curr_idx)) == 0:\n                    if verbose:\n                        self.logger.info(f'FPI converged for SCC {scc} in {k} iterations with tol '\n                                         f'{fpi_tol}. Final time: {time.time() - t1} s')\n                    return True\n\n                max_error = np.max([np.max(np.abs(res[samples.curr_idx, ...])) for res in residual.values()])\n                if verbose:\n                    self.logger.info(f'FPI iter: {k}. Max residual: {max_error}. Time: {time.time() - t1} s')\n\n                if k &gt;= max_fpi_iter:\n                    self.logger.warning(f'FPI did not converge in {max_fpi_iter} iterations for SCC {scc}: '\n                                        f'{max_error} &gt; tol {fpi_tol}. Some samples will be returned as NaN.')\n                    for var in coupling_prev:\n                        y[var][~samples.converged_idx, ...] = np.nan\n                    samples.valid_idx = np.logical_and(samples.valid_idx, samples.converged_idx)\n                    return True\n                else:\n                    return False\n\n            # Main FPI loop\n            if verbose:\n                self.logger.info(f\"Initializing FPI for SCC {scc} ...\")\n                t1 = time.time()\n            k = 0\n            while True:\n                for node in scc:\n                    # Gather inputs from exogenous and coupling sources\n                    comp = self[node]\n                    comp_input, kwds, call_model = _gather_comp_inputs(comp, coupling=coupling_prev)\n\n                    # Compute outputs (just don't do this FPI with expensive real models, please..)\n                    comp_output = comp.predict(comp_input, use_model=use_model.get(node), model_dir=None,\n                                               index_set=index_set.get(node), incremental=incremental.get(node),\n                                               misc_coeff=misc_coeff.get(node), executor=executor, **kwds)\n\n                    for var, arr in comp_output.items():\n                        if var == 'errors':\n                            if y.get(var) is None:\n                                y.setdefault(var, np.full((N,), None, dtype=object))\n                            global_indices = np.arange(N)[samples.curr_idx]\n\n                            for local_idx, err_info in arr.items():\n                                global_idx = int(global_indices[local_idx])\n                                err_info['index'] = global_idx\n                                y[var][global_idx] = err_info\n                            continue\n\n                        if np.issubdtype(arr.dtype, np.number):  # scalars and vectorized field quantities\n                            output_shape = arr.shape[1:]\n                            if y.get(var) is not None:\n                                if output_shape != y.get(var).shape[1:]:\n                                    y[var] = _merge_shapes((N, *output_shape), y[var])\n                            else:\n                                y.setdefault(var, np.full((N, *output_shape), np.nan))\n                            y[var][samples.curr_idx, ...] = arr\n                        else:  # fields returned as object arrays\n                            if y.get(var) is None:\n                                y.setdefault(var, np.full((N,), None, dtype=object))\n                            y[var][samples.curr_idx] = arr\n\n                        if str(var).split(LATENT_STR_ID)[0] in comp.outputs:\n                            norm_status[var] = not call_model\n                            is_computed[str(var).split(LATENT_STR_ID)[0]] = True\n\n                # Compute residual and check end conditions\n                if _end_conditions_met():\n                    break\n\n                # Skip anderson acceleration on first iteration\n                if k == 0:\n                    k += 1\n                    continue\n\n                # Iterate with anderson acceleration (only iterate on samples that are not yet converged)\n                N_curr = int(np.sum(samples.curr_idx))\n                mk = len(residual_hist)  # Max of anderson mem\n                var_shapes = []\n                xdims = []\n                for var in coupling_prev:\n                    shape = coupling_prev[var].shape[1:]\n                    var_shapes.append(shape)\n                    xdims.append(int(np.prod(shape)))\n                N_couple = int(np.sum(xdims))\n                res_snap = np.empty((N_curr, N_couple, mk))       # Shortened snapshot of residual history\n                coupling_snap = np.empty((N_curr, N_couple, mk))  # Shortened snapshot of coupling history\n                for i, (coupling_iter, residual_iter) in enumerate(zip(coupling_hist, residual_hist)):\n                    start_idx = 0\n                    for j, var in enumerate(coupling_prev):\n                        end_idx = start_idx + xdims[j]\n                        coupling_snap[:, start_idx:end_idx, i] = coupling_iter[var][samples.curr_idx, ...].reshape((N_curr, -1))  # noqa: E501\n                        res_snap[:, start_idx:end_idx, i] = residual_iter[var][samples.curr_idx, ...].reshape((N_curr, -1))  # noqa: E501\n                        start_idx = end_idx\n                C = np.ones((N_curr, 1, mk))\n                b = np.zeros((N_curr, N_couple, 1))\n                d = np.ones((N_curr, 1, 1))\n                alpha = np.expand_dims(constrained_lls(res_snap, b, C, d), axis=-3)   # (..., 1, mk, 1)\n                coupling_new = np.squeeze(coupling_snap[:, :, np.newaxis, :] @ alpha, axis=(-1, -2))\n                start_idx = 0\n                for j, var in enumerate(coupling_prev):\n                    end_idx = start_idx + xdims[j]\n                    coupling_prev[var][samples.curr_idx, ...] = coupling_new[:, start_idx:end_idx].reshape((N_curr, *var_shapes[j]))  # noqa: E501\n                    start_idx = end_idx\n                k += 1\n\n    # Return all component outputs; samples that didn't converge during FPI are left as np.nan\n    return format_outputs(y, loop_shape)\n</code></pre>"},{"location":"reference/system/#amisc.system.System.refine","title":"<code>refine(targets=None, num_refine=100, update_bounds=True, executor=None, weight_fcns='pdf')</code>","text":"<p>Perform a single adaptive refinement step on the system surrogate.</p> PARAMETER DESCRIPTION <code>targets</code> <p>list of system output variables to focus refinement on, use all outputs if not specified</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>num_refine</code> <p>number of input samples to compute error indicators on</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>update_bounds</code> <p>whether to continuously update coupling variable bounds during refinement</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>executor</code> <p>a <code>concurrent.futures.Executor</code> object to parallelize model evaluations</p> <p> TYPE: <code>Executor</code> DEFAULT: <code>None</code> </p> <code>weight_fcns</code> <p>weight functions for choosing new training data for each input variable; defaults to the PDFs of each variable. If None, then no weighting is applied.</p> <p> TYPE: <code>dict[str, callable] | Literal['pdf'] | None</code> DEFAULT: <code>'pdf'</code> </p> RETURNS DESCRIPTION <code>TrainIteration</code> <p><code>dict</code> of the refinement results indicating the chosen component and candidate index</p> Source code in <code>src/amisc/system.py</code> <pre><code>def refine(self, targets: list = None, num_refine: int = 100, update_bounds: bool = True, executor: Executor = None,\n           weight_fcns: dict[str, callable] | Literal['pdf'] | None = 'pdf') -&gt; TrainIteration:\n    \"\"\"Perform a single adaptive refinement step on the system surrogate.\n\n    :param targets: list of system output variables to focus refinement on, use all outputs if not specified\n    :param num_refine: number of input samples to compute error indicators on\n    :param update_bounds: whether to continuously update coupling variable bounds during refinement\n    :param executor: a `concurrent.futures.Executor` object to parallelize model evaluations\n    :param weight_fcns: weight functions for choosing new training data for each input variable; defaults to\n                        the PDFs of each variable. If None, then no weighting is applied.\n    :returns: `dict` of the refinement results indicating the chosen component and candidate index\n    \"\"\"\n    self._print_title_str(f'Refining system surrogate: iteration {self.refine_level + 1}')\n    targets = targets or self.outputs()\n\n    # Check for uninitialized components and refine those first\n    for comp in self.components:\n        if len(comp.active_set) == 0 and comp.has_surrogate:\n            alpha_star = (0,) * len(comp.model_fidelity)\n            beta_star = (0,) * len(comp.max_beta)\n            self.logger.info(f\"Initializing component {comp.name}: adding {(alpha_star, beta_star)} to active set\")\n            model_dir = (pth / 'components' / comp.name) if (pth := self.root_dir) is not None else None\n            comp.activate_index(alpha_star, beta_star, model_dir=model_dir, executor=executor,\n                                weight_fcns=weight_fcns)\n            num_evals = comp.get_cost(alpha_star, beta_star)\n            cost_star = max(1., comp.model_costs.get(alpha_star, 1.) * num_evals)  # Cpu time (s)\n            err_star = np.nan\n            return {'component': comp.name, 'alpha': alpha_star, 'beta': beta_star, 'num_evals': int(num_evals),\n                    'added_cost': float(cost_star), 'added_error': float(err_star)}\n\n    # Compute entire integrated-surrogate on a random test set for global system QoI error estimation\n    x_samples = self.sample_inputs(num_refine)\n    y_curr = self.predict(x_samples, index_set='train', targets=targets)\n    _combine_latent_arrays(y_curr)\n    coupling_vars = {k: v for k, v in self.coupling_variables().items() if k in y_curr}\n\n    y_min, y_max = None, None\n    if update_bounds:\n        y_min = {var: np.nanmin(y_curr[var], axis=0, keepdims=True) for var in coupling_vars}  # (1, ydim)\n        y_max = {var: np.nanmax(y_curr[var], axis=0, keepdims=True) for var in coupling_vars}  # (1, ydim)\n\n    # Find the candidate surrogate with the largest error indicator\n    error_max, error_indicator = -np.inf, -np.inf\n    comp_star, alpha_star, beta_star, err_star, cost_star = None, None, None, -np.inf, 0\n    for comp in self.components:\n        if not comp.has_surrogate:  # Skip analytic models that don't need a surrogate\n            continue\n\n        self.logger.info(f\"Estimating error for component '{comp.name}'...\")\n\n        if len(comp.candidate_set) &gt; 0:\n            candidates = list(comp.candidate_set)\n            if executor is None:\n                ret = [self.predict(x_samples, targets=targets, index_set={comp.name: {(alpha, beta)}},\n                                    incremental={comp.name: True})\n                       for alpha, beta in candidates]\n            else:\n                temp_buffer = self._remove_unpickleable()\n                futures = [executor.submit(self.predict, x_samples, targets=targets,\n                                           index_set={comp.name: {(alpha, beta)}}, incremental={comp.name: True})\n                           for alpha, beta in candidates]\n                wait(futures, timeout=None, return_when=ALL_COMPLETED)\n                ret = [f.result() for f in futures]\n                self._restore_unpickleable(temp_buffer)\n\n            for i, y_cand in enumerate(ret):\n                alpha, beta = candidates[i]\n                _combine_latent_arrays(y_cand)\n                error = {}\n                for var, arr in y_cand.items():\n                    if var in targets:\n                        error[var] = relative_error(arr, y_curr[var], skip_nan=True)\n\n                    if update_bounds and var in coupling_vars:\n                        y_min[var] = np.nanmin(np.concatenate((y_min[var], arr), axis=0), axis=0, keepdims=True)\n                        y_max[var] = np.nanmax(np.concatenate((y_max[var], arr), axis=0), axis=0, keepdims=True)\n\n                delta_error = np.nanmax([np.nanmax(error[var]) for var in error])  # Max error over all target QoIs\n                num_evals = comp.get_cost(alpha, beta)\n                delta_work = max(1., comp.model_costs.get(alpha, 1.) * num_evals)  # Cpu time (s)\n                error_indicator = delta_error / delta_work\n\n                self.logger.info(f\"Candidate multi-index: {(alpha, beta)}. Relative error: {delta_error}. \"\n                                 f\"Error indicator: {error_indicator}.\")\n\n                if error_indicator &gt; error_max:\n                    error_max = error_indicator\n                    comp_star, alpha_star, beta_star, err_star, cost_star = (\n                        comp.name, alpha, beta, delta_error, delta_work)\n        else:\n            self.logger.info(f\"Component '{comp.name}' has no available candidates left!\")\n\n    # Update all coupling variable ranges\n    if update_bounds:\n        for var in coupling_vars.values():\n            if np.all(~np.isnan(y_min[var])) and np.all(~np.isnan(y_max[var])):\n                if var.compression is not None:\n                    new_domain = list(zip(np.squeeze(y_min[var], axis=0).tolist(),\n                                          np.squeeze(y_max[var], axis=0).tolist()))\n                    var.update_domain(new_domain)\n                else:\n                    new_domain = (y_min[var][0], y_max[var][0])\n                    var.update_domain(var.denormalize(new_domain))  # bds will be in norm space from predict() call\n\n    # Add the chosen multi-index to the chosen component\n    if comp_star is not None:\n        self.logger.info(f\"Candidate multi-index {(alpha_star, beta_star)} chosen for component '{comp_star}'.\")\n        model_dir = (pth / 'components' / comp_star) if (pth := self.root_dir) is not None else None\n        self[comp_star].activate_index(alpha_star, beta_star, model_dir=model_dir, executor=executor,\n                                       weight_fcns=weight_fcns)\n        num_evals = self[comp_star].get_cost(alpha_star, beta_star)\n    else:\n        self.logger.info(f\"No candidates left for refinement, iteration: {self.refine_level}\")\n        num_evals = 0\n\n    # Return the results of the refinement step\n    return {'component': comp_star, 'alpha': alpha_star, 'beta': beta_star, 'num_evals': int(num_evals),\n            'added_cost': float(cost_star), 'added_error': float(err_star)}\n</code></pre>"},{"location":"reference/system/#amisc.system.System.remove_component","title":"<code>remove_component(component)</code>","text":"<p>Remove a component from the system.</p> Source code in <code>src/amisc/system.py</code> <pre><code>def remove_component(self, component: str | Component):\n    \"\"\"Remove a component from the system.\"\"\"\n    comp_name = component if isinstance(component, str) else component.name\n    self.components = [comp for comp in self.components if comp.name != comp_name]\n</code></pre>"},{"location":"reference/system/#amisc.system.System.sample_inputs","title":"<code>sample_inputs(size, component='System', normalize=True, use_pdf=False, include=None, exclude=None, nominal=None)</code>","text":"<p>Return samples of the inputs according to provided options. Will return samples in the normalized/compressed space of the surrogate by default. See <code>to_model_dataset</code> to convert the samples to be usable by the true model directly.</p> PARAMETER DESCRIPTION <code>size</code> <p>tuple or integer specifying shape or number of samples to obtain</p> <p> TYPE: <code>tuple | int</code> </p> <code>component</code> <p>which component to sample inputs for (defaults to full system exogenous inputs)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'System'</code> </p> <code>normalize</code> <p>whether to normalize the samples (defaults to True)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_pdf</code> <p>whether to sample from variable pdfs (defaults to False, which will instead sample from the variable domain bounds). If a string or list of strings is provided, then only those variables or variable categories will be sampled using their pdfs.</p> <p> TYPE: <code>bool | str | list[str]</code> DEFAULT: <code>False</code> </p> <code>include</code> <p>a list of variable or variable categories to include in the sampling. Defaults to using all input variables.</p> <p> TYPE: <code>str | list[str]</code> DEFAULT: <code>None</code> </p> <code>exclude</code> <p>a list of variable or variable categories to exclude from the sampling. Empty by default.</p> <p> TYPE: <code>str | list[str]</code> DEFAULT: <code>None</code> </p> <code>nominal</code> <p><code>dict(var_id=value)</code> of nominal values for params with relative uncertainty. Specify nominal values as unnormalized (will be normalized if <code>normalize=True</code>)</p> <p> TYPE: <code>dict[str, float]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p><code>dict</code> of <code>(*size,)</code> samples for each selected input variable</p> Source code in <code>src/amisc/system.py</code> <pre><code>def sample_inputs(self, size: tuple | int,\n                  component: str = 'System',\n                  normalize: bool = True,\n                  use_pdf: bool | str | list[str] = False,\n                  include: str | list[str] = None,\n                  exclude: str | list[str] = None,\n                  nominal: dict[str, float] = None) -&gt; Dataset:\n    \"\"\"Return samples of the inputs according to provided options. Will return samples in the\n    normalized/compressed space of the surrogate by default. See [`to_model_dataset`][amisc.utils.to_model_dataset]\n    to convert the samples to be usable by the true model directly.\n\n    :param size: tuple or integer specifying shape or number of samples to obtain\n    :param component: which component to sample inputs for (defaults to full system exogenous inputs)\n    :param normalize: whether to normalize the samples (defaults to True)\n    :param use_pdf: whether to sample from variable pdfs (defaults to False, which will instead sample from the\n                    variable domain bounds). If a string or list of strings is provided, then only those variables\n                    or variable categories will be sampled using their pdfs.\n    :param include: a list of variable or variable categories to include in the sampling. Defaults to using all\n                    input variables.\n    :param exclude: a list of variable or variable categories to exclude from the sampling. Empty by default.\n    :param nominal: `dict(var_id=value)` of nominal values for params with relative uncertainty. Specify nominal\n                    values as unnormalized (will be normalized if `normalize=True`)\n    :returns: `dict` of `(*size,)` samples for each selected input variable\n    \"\"\"\n    size = (size, ) if isinstance(size, int) else size\n    nominal = nominal or dict()\n    inputs = self.inputs() if component == 'System' else self[component].inputs\n    if include is None:\n        include = []\n    if not isinstance(include, list):\n        include = [include]\n    if exclude is None:\n        exclude = []\n    if not isinstance(exclude, list):\n        exclude = [exclude]\n    if isinstance(use_pdf, str):\n        use_pdf = [use_pdf]\n\n    selected_inputs = []\n    for var in inputs:\n        if len(include) == 0 or var.name in include or var.category in include:\n            if var.name not in exclude and var.category not in exclude:\n                selected_inputs.append(var)\n\n    samples = {}\n    for var in selected_inputs:\n        # Sample from latent variable domains for field quantities\n        if var.compression is not None:\n            latent = var.sample_domain(size)\n            for i in range(latent.shape[-1]):\n                samples[f'{var.name}{LATENT_STR_ID}{i}'] = latent[..., i]\n\n        # Sample scalars normally\n        else:\n            if (domain := var.get_domain()) is None:\n                raise RuntimeError(f\"Trying to sample variable '{var}' with empty domain. Please set a domain \"\n                                   f\"for this variable. Samples outside the provided domain will be rejected.\")\n            lb, ub = domain\n            pdf = (var.name in use_pdf or var.category in use_pdf) if isinstance(use_pdf, list) else use_pdf\n            nom = nominal.get(var.name, None)\n\n            x_sample = var.sample(size, nominal=nom) if pdf else var.sample_domain(size)\n            good_idx = (x_sample &lt; ub) &amp; (x_sample &gt; lb)\n            num_reject = np.sum(~good_idx)\n\n            while num_reject &gt; 0:\n                new_sample = var.sample((num_reject,), nominal=nom) if pdf else var.sample_domain((num_reject,))\n                x_sample[~good_idx] = new_sample\n                good_idx = (x_sample &lt; ub) &amp; (x_sample &gt; lb)\n                num_reject = np.sum(~good_idx)\n\n            samples[var.name] = var.normalize(x_sample) if normalize else x_sample\n\n    return samples\n</code></pre>"},{"location":"reference/system/#amisc.system.System.save_to_file","title":"<code>save_to_file(filename, save_dir=None, dumper=None)</code>","text":"<p>Save surrogate to file. Defaults to <code>root/surrogates/filename.yml</code> with the default yaml encoder.</p> PARAMETER DESCRIPTION <code>filename</code> <p>the name of the save file</p> <p> TYPE: <code>str</code> </p> <code>save_dir</code> <p>the directory to save the file to (defaults to <code>root/surrogates</code> or <code>cwd()</code>)</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>dumper</code> <p>the encoder to use (defaults to the <code>amisc</code> yaml encoder)</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>def save_to_file(self, filename: str, save_dir: str | Path = None, dumper=None):\n    \"\"\"Save surrogate to file. Defaults to `root/surrogates/filename.yml` with the default yaml encoder.\n\n    :param filename: the name of the save file\n    :param save_dir: the directory to save the file to (defaults to `root/surrogates` or `cwd()`)\n    :param dumper: the encoder to use (defaults to the `amisc` yaml encoder)\n    \"\"\"\n    from amisc import YamlLoader\n    encoder = dumper or YamlLoader\n    save_dir = save_dir or self.root_dir or Path.cwd()\n    if Path(save_dir) == self.root_dir:\n        save_dir = self.root_dir / 'surrogates'\n    encoder.dump(self, Path(save_dir) / filename)\n</code></pre>"},{"location":"reference/system/#amisc.system.System.serialize","title":"<code>serialize(keep_components=False, serialize_args=None, serialize_kwargs=None)</code>","text":"<p>Convert to a <code>dict</code> with only standard Python types for fields.</p> PARAMETER DESCRIPTION <code>keep_components</code> <p>whether to serialize the components as well (defaults to False)</p> <p> DEFAULT: <code>False</code> </p> <code>serialize_args</code> <p><code>dict</code> of arguments to pass to each component's serialize method</p> <p> DEFAULT: <code>None</code> </p> <code>serialize_kwargs</code> <p><code>dict</code> of keyword arguments to pass to each component's serialize method</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a <code>dict</code> representation of the <code>System</code> object</p> Source code in <code>src/amisc/system.py</code> <pre><code>def serialize(self, keep_components=False, serialize_args=None, serialize_kwargs=None) -&gt; dict:\n    \"\"\"Convert to a `dict` with only standard Python types for fields.\n\n    :param keep_components: whether to serialize the components as well (defaults to False)\n    :param serialize_args: `dict` of arguments to pass to each component's serialize method\n    :param serialize_kwargs: `dict` of keyword arguments to pass to each component's serialize method\n    :returns: a `dict` representation of the `System` object\n    \"\"\"\n    serialize_args = serialize_args or dict()\n    serialize_kwargs = serialize_kwargs or dict()\n    d = {}\n    for key, value in self.__dict__.items():\n        if value is not None and not key.startswith('_'):\n            if key == 'components' and not keep_components:\n                d[key] = [comp.serialize(keep_yaml_objects=False,\n                                         serialize_args=serialize_args.get(comp.name),\n                                         serialize_kwargs=serialize_kwargs.get(comp.name))\n                          for comp in value]\n            elif key == 'train_history':\n                if len(value) &gt; 0:\n                    d[key] = value.serialize()\n            else:\n                if not isinstance(value, _builtin):\n                    self.logger.warning(f\"Attribute '{key}' of type '{type(value)}' may not be a builtin \"\n                                        f\"Python type. This may cause issues when saving/loading from file.\")\n                d[key] = value\n\n    for key, value in self.model_extra.items():\n        if isinstance(value, _builtin):\n            d[key] = value\n\n    return d\n</code></pre>"},{"location":"reference/system/#amisc.system.System.set_logger","title":"<code>set_logger(log_file=None, stdout=None, logger=None, level=logging.INFO)</code>","text":"<p>Set a new <code>logging.Logger</code> object.</p> PARAMETER DESCRIPTION <code>log_file</code> <p>log to this file if str or Path (defaults to whatever is currently set or empty); set <code>False</code> to remove file logging or set <code>True</code> to create a default log file in the root dir</p> <p> TYPE: <code>str | Path | bool</code> DEFAULT: <code>None</code> </p> <code>stdout</code> <p>whether to connect the logger to console (defaults to whatever is currently set or <code>False</code>)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>the logging object to use (this will override the <code>log_file</code> and <code>stdout</code> arguments if set); if <code>None</code>, then a new logger is created according to <code>log_file</code> and <code>stdout</code></p> <p> TYPE: <code>Logger</code> DEFAULT: <code>None</code> </p> <code>level</code> <p>the logging level to set the logger to (defaults to <code>logging.INFO</code>)</p> <p> TYPE: <code>int</code> DEFAULT: <code>INFO</code> </p> Source code in <code>src/amisc/system.py</code> <pre><code>def set_logger(self, log_file: str | Path | bool = None, stdout: bool = None, logger: logging.Logger = None,\n               level: int = logging.INFO):\n    \"\"\"Set a new `logging.Logger` object.\n\n    :param log_file: log to this file if str or Path (defaults to whatever is currently set or empty);\n                     set `False` to remove file logging or set `True` to create a default log file in the root dir\n    :param stdout: whether to connect the logger to console (defaults to whatever is currently set or `False`)\n    :param logger: the logging object to use (this will override the `log_file` and `stdout` arguments if set);\n                   if `None`, then a new logger is created according to `log_file` and `stdout`\n    :param level: the logging level to set the logger to (defaults to `logging.INFO`)\n    \"\"\"\n    # Decide whether to use stdout\n    if stdout is None:\n        stdout = False\n        if self._logger is not None:\n            for handler in self._logger.handlers:\n                if isinstance(handler, logging.StreamHandler):\n                    stdout = True\n                    break\n\n    # Decide what log_file to use (if any)\n    if log_file is True:\n        log_file = pth / f'amisc_{self.timestamp()}.log' if (pth := self.root_dir) is not None else (\n            f'amisc_{self.timestamp()}.log')\n    elif log_file is None:\n        if self._logger is not None:\n            for handler in self._logger.handlers:\n                if isinstance(handler, logging.FileHandler):\n                    log_file = handler.baseFilename\n                    break\n    elif log_file is False:\n        log_file = None\n\n    self._logger = logger or get_logger(self.name, log_file=log_file, stdout=stdout, level=level)\n\n    for comp in self.components:\n        comp.set_logger(log_file=log_file, stdout=stdout, logger=logger, level=level)\n</code></pre>"},{"location":"reference/system/#amisc.system.System.simulate_fit","title":"<code>simulate_fit()</code>","text":"<p>Loop back through training history and simulate each iteration. Will yield the internal data structures of each <code>Component</code> surrogate after each iteration of training (without needing to call <code>fit()</code> or any of the underlying models). This might be useful, for example, for computing the surrogate predictions on a new test set or viewing cumulative training costs.</p> <p>Example</p> <p>Say you have a new test set: <code>(new_xtest, new_ytest)</code>, and you want to compute the accuracy of the surrogate fit at each iteration of the training history:</p> <pre><code>for train_iter, active_sets, candidate_sets, misc_coeff_train, misc_coeff_test in system.simulate_fit():\n    # Do something with the surrogate data structures\n    new_ysurr = system.predict(new_xtest, index_set=active_sets, misc_coeff=misc_coeff_train)\n    train_error = relative_error(new_ysurr, new_ytest)\n</code></pre> RETURNS DESCRIPTION <p>a generator of the active index sets, candidate index sets, and MISC coefficients of each component model at each iteration of the training history</p> Source code in <code>src/amisc/system.py</code> <pre><code>def simulate_fit(self):\n    \"\"\"Loop back through training history and simulate each iteration. Will yield the internal data structures\n    of each `Component` surrogate after each iteration of training (without needing to call `fit()` or any\n    of the underlying models). This might be useful, for example, for computing the surrogate predictions on\n    a new test set or viewing cumulative training costs.\n\n    !!! Example\n        Say you have a new test set: `(new_xtest, new_ytest)`, and you want to compute the accuracy of the\n        surrogate fit at each iteration of the training history:\n\n        ```python\n        for train_iter, active_sets, candidate_sets, misc_coeff_train, misc_coeff_test in system.simulate_fit():\n            # Do something with the surrogate data structures\n            new_ysurr = system.predict(new_xtest, index_set=active_sets, misc_coeff=misc_coeff_train)\n            train_error = relative_error(new_ysurr, new_ytest)\n        ```\n\n    :return: a generator of the active index sets, candidate index sets, and MISC coefficients\n             of each component model at each iteration of the training history\n    \"\"\"\n    # \"Simulated\" data structures for each component\n    active_sets = {comp.name: IndexSet() for comp in self.components}       # active index sets for each component\n    candidate_sets = {comp.name: IndexSet() for comp in self.components}    # candidate sets for each component\n    misc_coeff_train = {comp.name: MiscTree() for comp in self.components}  # MISC coeff for active sets\n    misc_coeff_test = {comp.name: MiscTree() for comp in self.components}   # MISC coeff for active + candidate sets\n\n    for train_result in self.train_history:\n        # The selected refinement component and indices\n        comp_star = train_result['component']\n        alpha_star = train_result['alpha']\n        beta_star = train_result['beta']\n        comp = self[comp_star]\n\n        # Get forward neighbors for the selected index\n        neighbors = comp._neighbors(alpha_star, beta_star, active_set=active_sets[comp_star], forward=True)\n\n        # \"Activate\" the index in the simulated data structure\n        s = set()\n        s.add((alpha_star, beta_star))\n        comp.update_misc_coeff(IndexSet(s), index_set=active_sets[comp_star],\n                               misc_coeff=misc_coeff_train[comp_star])\n\n        if (alpha_star, beta_star) in candidate_sets[comp_star]:\n            candidate_sets[comp_star].remove((alpha_star, beta_star))\n        else:\n            # Only for initial index which didn't come from the candidate set\n            comp.update_misc_coeff(IndexSet(s), index_set=active_sets[comp_star].union(candidate_sets[comp_star]),\n                                   misc_coeff=misc_coeff_test[comp_star])\n        active_sets[comp_star].update(s)\n\n        comp.update_misc_coeff(neighbors, index_set=active_sets[comp_star].union(candidate_sets[comp_star]),\n                               misc_coeff=misc_coeff_test[comp_star])  # neighbors will only ever pass here once\n        candidate_sets[comp_star].update(neighbors)\n\n        # Caller can now do whatever they want as if the system surrogate were at this training iteration\n        # See the \"index_set\" and \"misc_coeff\" overrides for `System.predict()` for example\n        yield train_result, active_sets, candidate_sets, misc_coeff_train, misc_coeff_test\n</code></pre>"},{"location":"reference/system/#amisc.system.System.swap_component","title":"<code>swap_component(old_component, new_component)</code>","text":"<p>Replace an old component with a new component.</p> Source code in <code>src/amisc/system.py</code> <pre><code>def swap_component(self, old_component: str | Component, new_component: Callable | Component):\n    \"\"\"Replace an old component with a new component.\"\"\"\n    old_name = old_component if isinstance(old_component, str) else old_component.name\n    comps = [comp if comp.name != old_name else new_component for comp in self.components]\n    self.components = comps\n</code></pre>"},{"location":"reference/system/#amisc.system.System.test_set_performance","title":"<code>test_set_performance(xtest, ytest, index_set='test')</code>","text":"<p>Compute the relative L2 error on a test set for the given target output variables.</p> PARAMETER DESCRIPTION <code>xtest</code> <p><code>dict</code> of test set input samples   (unnormalized)</p> <p> TYPE: <code>Dataset</code> </p> <code>ytest</code> <p><code>dict</code> of test set output samples  (unnormalized)</p> <p> TYPE: <code>Dataset</code> </p> <code>index_set</code> <p>index set to use for prediction (defaults to 'train')</p> <p> DEFAULT: <code>'test'</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p><code>dict</code> of relative L2 errors for each target output variable</p> Source code in <code>src/amisc/system.py</code> <pre><code>def test_set_performance(self, xtest: Dataset, ytest: Dataset, index_set='test') -&gt; Dataset:\n    \"\"\"Compute the relative L2 error on a test set for the given target output variables.\n\n    :param xtest: `dict` of test set input samples   (unnormalized)\n    :param ytest: `dict` of test set output samples  (unnormalized)\n    :param index_set: index set to use for prediction (defaults to 'train')\n    :returns: `dict` of relative L2 errors for each target output variable\n    \"\"\"\n    targets = [var for var in ytest.keys() if COORDS_STR_ID not in var and var in self.outputs()]\n    coords = {var: ytest[var] for var in ytest if COORDS_STR_ID in var}\n    xtest = to_surrogate_dataset(xtest, self.inputs(), del_fields=True)[0]\n    ysurr = self.predict(xtest, index_set=index_set, targets=targets)\n    ysurr = to_model_dataset(ysurr, self.outputs(), del_latent=True, **coords)[0]\n    perf = {}\n    for var in targets:\n        # Handle relative error for object arrays (field qtys)\n        ytest_obj = np.issubdtype(ytest[var].dtype, np.object_)\n        ysurr_obj = np.issubdtype(ysurr[var].dtype, np.object_)\n        if ytest_obj or ysurr_obj:\n            _iterable = np.ndindex(ytest[var].shape) if ytest_obj else np.ndindex(ysurr[var].shape)\n            num, den = [], []\n            for index in _iterable:\n                pred, targ = ysurr[var][index], ytest[var][index]\n                num.append((pred - targ)**2)\n                den.append(targ ** 2)\n            perf[var] = float(np.sqrt(sum([np.sum(n) for n in num]) / sum([np.sum(d) for d in den])))\n        else:\n            perf[var] = float(relative_error(ysurr[var], ytest[var]))\n\n    return perf\n</code></pre>"},{"location":"reference/system/#amisc.system.System.timestamp","title":"<code>timestamp()</code>  <code>staticmethod</code>","text":"<p>Return a UTC timestamp string in the isoformat <code>YYYY-MM-DDTHH.MM.SS</code>.</p> Source code in <code>src/amisc/system.py</code> <pre><code>@staticmethod\ndef timestamp() -&gt; str:\n    \"\"\"Return a UTC timestamp string in the isoformat `YYYY-MM-DDTHH.MM.SS`.\"\"\"\n    return datetime.datetime.now(tz=timezone.utc).isoformat().split('.')[0].replace(':', '.')\n</code></pre>"},{"location":"reference/system/#amisc.system.System.variables","title":"<code>variables()</code>","text":"<p>Iterator over all variables in the system (inputs and outputs).</p> Source code in <code>src/amisc/system.py</code> <pre><code>def variables(self):\n    \"\"\"Iterator over all variables in the system (inputs and outputs).\"\"\"\n    yield from ChainMap(self.inputs(), self.outputs()).values()\n</code></pre>"},{"location":"reference/system/#amisc.system.TrainHistory","title":"<code>TrainHistory(data=None)</code>","text":"<p>               Bases: <code>UserList</code>, <code>Serializable</code></p> <p>Stores the training history of a system surrogate as a list of <code>TrainIteration</code> objects.</p> Source code in <code>src/amisc/system.py</code> <pre><code>def __init__(self, data: list = None):\n    data = data or []\n    super().__init__(self._validate_data(data))\n</code></pre>"},{"location":"reference/system/#amisc.system.TrainHistory.deserialize","title":"<code>deserialize(serialized_data)</code>  <code>classmethod</code>","text":"<p>Deserialize a list of <code>dict</code> objects into a <code>TrainHistory</code> object.</p> Source code in <code>src/amisc/system.py</code> <pre><code>@classmethod\ndef deserialize(cls, serialized_data: list[dict]) -&gt; TrainHistory:\n    \"\"\"Deserialize a list of `dict` objects into a `TrainHistory` object.\"\"\"\n    return TrainHistory(serialized_data)\n</code></pre>"},{"location":"reference/system/#amisc.system.TrainHistory.serialize","title":"<code>serialize()</code>","text":"<p>Return a list of each result in the history serialized to a <code>dict</code>.</p> Source code in <code>src/amisc/system.py</code> <pre><code>def serialize(self) -&gt; list[dict]:\n    \"\"\"Return a list of each result in the history serialized to a `dict`.\"\"\"\n    ret_list = []\n    for res in self:\n        new_res = res.copy()\n        new_res['alpha'] = str(res['alpha'])\n        new_res['beta'] = str(res['beta'])\n        ret_list.append(new_res)\n    return ret_list\n</code></pre>"},{"location":"reference/training/","title":"training","text":""},{"location":"reference/training/#amisc.training","title":"<code>amisc.training</code>","text":"<p>Classes for storing and managing training data for surrogate models. The <code>TrainingData</code> interface also specifies how new training data should be sampled over the input space (i.e. experimental design).</p> <p>Includes:</p> <ul> <li><code>TrainingData</code> \u2014 an interface for storing surrogate training data.</li> <li><code>SparseGrid</code> \u2014 a class for storing training data in a sparse grid format.</li> </ul>"},{"location":"reference/training/#amisc.training.SparseGrid","title":"<code>SparseGrid(collocation_rule='leja', knots_per_level=2, expand_latent_method='round-robin', opt_args=lambda: {'locally_biased': False, 'maxfun': 300}(), betas=set(), x_grids=dict(), yi_map=dict(), yi_nan_map=dict(), error_map=dict(), latent_size=dict())</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingData</code>, <code>PickleSerializable</code></p> <p>A class for storing training data in a sparse grid format. The <code>SparseGrid</code> class stores training points by their coordinate location in a larger tensor-product grid, and obtains new training data by refining a single 1d grid at a time.</p> <p>MISC and sparse grids</p> <p>MISC itself can be thought of as an extension to the well-known sparse grid technique, so this class readily integrates with the MISC implementation in <code>Component</code>. Sparse grids limit the curse of dimensionality up to about <code>dim = 10-15</code> for the input space (which would otherwise be infeasible with a normal full tensor-product grid of the same size).</p> <p>About points in a sparse grid</p> <p>A sparse grid approximates a full tensor-product grid \\((N_1, N_2, ..., N_d)\\), where \\(N_i\\) is the number of grid points along dimension \\(i\\), for a \\(d\\)-dimensional space. Each point is uniquely identified in the sparse grid by a list of indices \\((j_1, j_2, ..., j_d)\\), where \\(j_i = 0 ... N_i\\). We refer to this unique identifier as a \"grid coordinate\". In the <code>SparseGrid</code> data structure, these coordinates are used along with the <code>alpha</code> fidelity index to uniquely locate the training data for a given multi-index pair.</p> ATTRIBUTE DESCRIPTION <code>collocation_rule</code> <p>the collocation rule to use for generating new grid points (only 'leja' is supported)</p> <p> TYPE: <code>str</code> </p> <code>knots_per_level</code> <p>the number of grid knots/points per level in the <code>beta</code> fidelity multi-index</p> <p> TYPE: <code>int</code> </p> <code>expand_latent_method</code> <p>method for expanding latent grids, either 'round-robin' or 'tensor-product'</p> <p> TYPE: <code>str</code> </p> <code>opt_args</code> <p>extra arguments for the global 1d <code>direct</code> optimizer</p> <p> TYPE: <code>dict</code> </p> <code>betas</code> <p>a set of all <code>beta</code> multi-indices that have been seen so far</p> <p> TYPE: <code>set[MultiIndex]</code> </p> <code>x_grids</code> <p>a <code>dict</code> of grid points for each 1d input dimension</p> <p> TYPE: <code>dict[str, ArrayLike]</code> </p> <code>yi_map</code> <p>a <code>dict</code> of model outputs for each grid coordinate</p> <p> TYPE: <code>dict[MultiIndex, dict[tuple[int, ...], dict[str, ArrayLike]]]</code> </p> <code>yi_nan_map</code> <p>a <code>dict</code> of imputed model outputs for each grid coordinate where the model failed (or gave nan)</p> <p> TYPE: <code>dict[MultiIndex, dict[tuple[int, ...], dict[str, ArrayLike]]]</code> </p> <code>error_map</code> <p>a <code>dict</code> of error information for each grid coordinate where the model failed</p> <p> TYPE: <code>dict[MultiIndex, dict[tuple[int, ...], dict[str, Any]]]</code> </p> <code>latent_size</code> <p>the number of latent coefficients for each variable (0 if scalar)</p> <p> TYPE: <code>dict[str, int]</code> </p>"},{"location":"reference/training/#amisc.training.SparseGrid.beta_to_knots","title":"<code>beta_to_knots(beta, knots_per_level=None, latent_size=None, expand_latent_method=None)</code>","text":"<p>Convert a <code>beta</code> multi-index to the number of knots per dimension in the sparse grid.</p> PARAMETER DESCRIPTION <code>beta</code> <p>refinement level indices</p> <p> TYPE: <code>MultiIndex</code> </p> <code>knots_per_level</code> <p>level-to-grid-size multiplier, i.e. number of new points (or knots) for each beta level</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>latent_size</code> <p>the number of latent coefficients for each variable (0 if scalar); number of variables and order should match the <code>beta</code> multi-index</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>expand_latent_method</code> <p>method for expanding latent grids, either 'round-robin' or 'tensor-product'</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>the number of knots/points per dimension for the sparse grid</p> Source code in <code>src/amisc/training.py</code> <pre><code>def beta_to_knots(self, beta: MultiIndex, knots_per_level: int = None, latent_size: dict = None,\n                  expand_latent_method: str = None) -&gt; tuple:\n    \"\"\"Convert a `beta` multi-index to the number of knots per dimension in the sparse grid.\n\n    :param beta: refinement level indices\n    :param knots_per_level: level-to-grid-size multiplier, i.e. number of new points (or knots) for each beta level\n    :param latent_size: the number of latent coefficients for each variable (0 if scalar); number of variables and\n                        order should match the `beta` multi-index\n    :param expand_latent_method: method for expanding latent grids, either 'round-robin' or 'tensor-product'\n    :returns: the number of knots/points per dimension for the sparse grid\n    \"\"\"\n    knots_per_level = knots_per_level or self.knots_per_level\n    latent_size = latent_size or self.latent_size\n    expand_latent_method = expand_latent_method or self.expand_latent_method\n\n    grid_size = []\n    for i, (var, num_latent) in enumerate(latent_size.items()):\n        if num_latent &gt; 0:\n            match expand_latent_method:\n                case 'round-robin':\n                    if beta[i] == 0:\n                        grid_size.append((1,) * num_latent)  # initializes all latent grids to 1\n                    else:\n                        latent_refine_idx = (beta[i] - 1) % num_latent\n                        latent_refine_num = ((beta[i] - 1) // num_latent) + 1\n                        latent_beta = tuple([latent_refine_num] * (latent_refine_idx + 1) +\n                                            [latent_refine_num - 1] * (num_latent - latent_refine_idx - 1))\n                        latent_grid = [knots_per_level * latent_beta[j] + 1 for j in range(num_latent)]\n                        grid_size.append(tuple(latent_grid))\n                case 'tensor-product':\n                    grid_size.append((knots_per_level * beta[i] + 1,) * num_latent)\n                case other:\n                    raise NotImplementedError(f\"Unknown method for expanding latent grids: {other}\")\n        else:\n            grid_size.append(knots_per_level * beta[i] + 1)\n\n    return tuple(grid_size)\n</code></pre>"},{"location":"reference/training/#amisc.training.SparseGrid.clear","title":"<code>clear()</code>","text":"<p>Clear all training data.</p> Source code in <code>src/amisc/training.py</code> <pre><code>def clear(self):\n    \"\"\"Clear all training data.\"\"\"\n    self.betas.clear()\n    self.x_grids.clear()\n    self.yi_map.clear()\n    self.yi_nan_map.clear()\n    self.error_map.clear()\n    self.latent_size.clear()\n</code></pre>"},{"location":"reference/training/#amisc.training.SparseGrid.collocation_1d","title":"<code>collocation_1d(N, z_bds, z_pts=None, wt_fcn=None, method='leja', opt_args=None)</code>  <code>staticmethod</code>","text":"<p>Find the next <code>N</code> points in the 1d sequence of <code>z_pts</code> using the provided collocation method.</p> PARAMETER DESCRIPTION <code>N</code> <p>number of new points to add to the sequence</p> <p> TYPE: <code>int</code> </p> <code>z_bds</code> <p>bounds on the 1d domain</p> <p> TYPE: <code>tuple</code> </p> <code>z_pts</code> <p>current univariate sequence <code>(Nz,)</code>, start at middle of <code>z_bds</code> if <code>None</code></p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> <code>wt_fcn</code> <p>weighting function, uses a constant weight if <code>None</code>, callable as <code>wt_fcn(z)</code></p> <p> TYPE: <code>callable</code> DEFAULT: <code>None</code> </p> <code>method</code> <p>collocation method to use, currently only 'leja' is supported</p> <p> DEFAULT: <code>'leja'</code> </p> <code>opt_args</code> <p>extra arguments for the global 1d <code>direct</code> optimizer</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>the univariate sequence <code>z_pts</code> augmented by <code>N</code> new points</p> Source code in <code>src/amisc/training.py</code> <pre><code>@staticmethod\ndef collocation_1d(N: int, z_bds: tuple, z_pts: np.ndarray = None,\n                   wt_fcn: callable = None, method='leja', opt_args=None) -&gt; np.ndarray:\n    \"\"\"Find the next `N` points in the 1d sequence of `z_pts` using the provided collocation method.\n\n    :param N: number of new points to add to the sequence\n    :param z_bds: bounds on the 1d domain\n    :param z_pts: current univariate sequence `(Nz,)`, start at middle of `z_bds` if `None`\n    :param wt_fcn: weighting function, uses a constant weight if `None`, callable as `wt_fcn(z)`\n    :param method: collocation method to use, currently only 'leja' is supported\n    :param opt_args: extra arguments for the global 1d `direct` optimizer\n    :returns: the univariate sequence `z_pts` augmented by `N` new points\n    \"\"\"\n    opt_args = opt_args or {}\n    if wt_fcn is None:\n        wt_fcn = lambda z: 1\n    if z_pts is None:\n        z_pts = (z_bds[1] + z_bds[0]) / 2\n        N = N - 1\n    z_pts = np.atleast_1d(z_pts)\n\n    match method:\n        case 'leja':\n            # Construct Leja sequence by maximizing the Leja objective sequentially\n            for i in range(N):\n                obj_fun = lambda z: -wt_fcn(np.array(z)) * np.prod(np.abs(z - z_pts))\n                res = direct(obj_fun, [z_bds], **opt_args)  # Use global DIRECT optimization over 1d domain\n                z_star = res.x\n                z_pts = np.concatenate((z_pts, z_star))\n        case other:\n            raise NotImplementedError(f\"Unknown collocation method: {other}\")\n\n    return z_pts\n</code></pre>"},{"location":"reference/training/#amisc.training.SparseGrid.get","title":"<code>get(alpha, beta, y_vars=None, skip_nan=False)</code>","text":"<p>Get the training data from the sparse grid for a given <code>alpha</code> and <code>beta</code> pair.</p> Source code in <code>src/amisc/training.py</code> <pre><code>def get(self, alpha: MultiIndex, beta: MultiIndex, y_vars: list[str] = None, skip_nan: bool = False):\n    \"\"\"Get the training data from the sparse grid for a given `alpha` and `beta` pair.\"\"\"\n    return self.get_by_coord(alpha, list(self._expand_grid_coords(beta)), y_vars=y_vars, skip_nan=skip_nan)\n</code></pre>"},{"location":"reference/training/#amisc.training.SparseGrid.get_by_coord","title":"<code>get_by_coord(alpha, coords, y_vars=None, skip_nan=False)</code>","text":"<p>Get training data from the sparse grid for a given <code>alpha</code> and list of grid coordinates. Try to replace <code>nan</code> values with imputed values. Skip any data points with remaining <code>nan</code> values if <code>skip_nan=True</code>.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>the model fidelity indices</p> <p> TYPE: <code>MultiIndex</code> </p> <code>coords</code> <p>a list of grid coordinates to locate the <code>yi</code> values in the sparse grid data structure</p> <p> TYPE: <code>list</code> </p> <code>y_vars</code> <p>the keys of the outputs to return (if <code>None</code>, return all outputs)</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>skip_nan</code> <p>skip any data points with remaining <code>nan</code> values if <code>skip_nan=True</code> (only for numeric outputs)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <p><code>dicts</code> of model inputs <code>xi_dict</code> and outputs <code>yi_dict</code></p> Source code in <code>src/amisc/training.py</code> <pre><code>def get_by_coord(self, alpha: MultiIndex, coords: list, y_vars: list = None, skip_nan: bool = False):\n    \"\"\"Get training data from the sparse grid for a given `alpha` and list of grid coordinates. Try to replace\n    `nan` values with imputed values. Skip any data points with remaining `nan` values if `skip_nan=True`.\n\n    :param alpha: the model fidelity indices\n    :param coords: a list of grid coordinates to locate the `yi` values in the sparse grid data structure\n    :param y_vars: the keys of the outputs to return (if `None`, return all outputs)\n    :param skip_nan: skip any data points with remaining `nan` values if `skip_nan=True` (only for numeric outputs)\n    :returns: `dicts` of model inputs `xi_dict` and outputs `yi_dict`\n    \"\"\"\n    N = len(coords)\n    is_numeric = {}\n    is_singleton = {}\n    xi_dict = self._extract_grid_points(coords)\n    yi_dict = {}\n\n    first_yi = next(iter(self.yi_map[alpha].values()))\n    if y_vars is None:\n        y_vars = first_yi.keys()\n\n    for var in y_vars:\n        yi = np.atleast_1d(first_yi[var])\n        is_numeric[var] = self._is_numeric(yi)\n        is_singleton[var] = self._is_singleton(yi)\n        yi_dict[var] = np.empty(N, dtype=np.float64 if is_numeric[var] and is_singleton[var] else object)\n\n    for i, coord in enumerate(coords):\n        try:\n            yi_curr = self.yi_map[alpha][coord]\n            for var in y_vars:\n                yi = arr if (arr := self.yi_nan_map[alpha].get(coord, {}).get(var)) is not None else yi_curr[var]\n                yi_dict[var][i] = yi if is_singleton[var] else np.atleast_1d(yi)\n\n        except KeyError as e:\n            raise KeyError(f\"Can't access sparse grid data for alpha={alpha}, coord={coord}. \"\n                           f\"Make sure the data has been set first.\") from e\n\n    # Delete nans if requested (only for numeric singleton outputs)\n    if skip_nan:\n        nan_idx = np.full(N, False)\n        for var in y_vars:\n            if is_numeric[var] and is_singleton[var]:\n                nan_idx |= np.isnan(yi_dict[var])\n\n        xi_dict = {k: v[~nan_idx] for k, v in xi_dict.items()}\n        yi_dict = {k: v[~nan_idx] for k, v in yi_dict.items()}\n\n    return xi_dict, yi_dict  # Both with elements of shape (N, ...) for N grid points\n</code></pre>"},{"location":"reference/training/#amisc.training.SparseGrid.impute_missing_data","title":"<code>impute_missing_data(alpha, beta)</code>","text":"<p>Impute missing values in the sparse grid for a given multi-index pair by linear regression imputation.</p> Source code in <code>src/amisc/training.py</code> <pre><code>def impute_missing_data(self, alpha: MultiIndex, beta: MultiIndex):\n    \"\"\"Impute missing values in the sparse grid for a given multi-index pair by linear regression imputation.\"\"\"\n    imputer, xi_all, yi_all = None, None, None\n\n    # only impute (small-length) numeric quantities\n    yi_dict = next(iter(self.yi_map[alpha].values()))\n    output_vars = [var for var in self._numeric_outputs(yi_dict)\n                   if len(np.ravel(yi_dict[var])) &lt;= self.MAX_IMPUTE_SIZE]\n\n    for coord, yi_dict in self.yi_map[alpha].items():\n        if any([np.any(np.isnan(yi_dict[var])) for var in output_vars]):\n            if imputer is None:\n                # Grab all 'good' interpolation points and train a simple linear regression fit\n                xi_all, yi_all = self.get(alpha, beta, y_vars=output_vars, skip_nan=True)\n                if len(xi_all) == 0 or len(next(iter(xi_all.values()))) == 0:\n                    continue  # possible if no good data has been set yet\n\n                N = next(iter(xi_all.values())).shape[0]  # number of grid points\n                xi_mat = np.concatenate([xi_all[var][:, np.newaxis] if len(xi_all[var].shape) == 1 else\n                                         xi_all[var] for var in xi_all.keys()], axis=-1)\n                yi_mat = np.concatenate([yi_all[var][:, np.newaxis] if len(yi_all[var].shape) == 1 else\n                                         yi_all[var].reshape((N, -1)) for var in output_vars], axis=-1)\n\n                imputer = _RidgeRegression(alpha=1.0)\n                imputer.fit(xi_mat, yi_mat)\n\n            # Run the imputer for this coordinate\n            x_interp = self._extract_grid_points(coord)\n            x_interp = np.concatenate([x_interp[var][:, np.newaxis] if len(x_interp[var].shape) == 1 else\n                                       x_interp[var] for var in x_interp.keys()], axis=-1)\n            y_interp = imputer.predict(x_interp)\n\n            # Unpack the imputed value\n            y_impute = {}\n            start_idx = 0\n            for var in output_vars:\n                var_shape = yi_all[var].shape[1:] or (1,)\n                end_idx = start_idx + int(np.prod(var_shape))\n                yi = np.atleast_1d(y_interp[0, start_idx:end_idx]).reshape(var_shape)\n                nan_idx = np.isnan(np.atleast_1d(yi_dict[var]))\n                yi[~nan_idx] = np.atleast_1d(yi_dict[var])[~nan_idx]  # Only keep imputed values where yi is nan\n                y_impute[var] = float(yi[0]) if self._is_singleton(yi) else yi.tolist()\n                start_idx = end_idx\n\n            self.yi_nan_map[alpha][coord] = copy.deepcopy(y_impute)\n</code></pre>"},{"location":"reference/training/#amisc.training.SparseGrid.is_one_level_refinement","title":"<code>is_one_level_refinement(beta_old, beta_new)</code>  <code>staticmethod</code>","text":"<p>Check if a new <code>beta</code> multi-index is a one-level refinement from a previous <code>beta</code>.</p> <p>Example</p> <p>Refining from <code>(0, 1, 2)</code> to the new multi-index <code>(1, 1, 2)</code> is a one-level refinement. But refining to either <code>(2, 1, 2)</code> or <code>(1, 2, 2)</code> are not, since more than one refinement occurs at the same time.</p> PARAMETER DESCRIPTION <code>beta_old</code> <p>the starting multi-index</p> <p> TYPE: <code>tuple</code> </p> <code>beta_new</code> <p>the new refined multi-index</p> <p> TYPE: <code>tuple</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>whether <code>beta_new</code> is a one-level refinement from <code>beta_old</code></p> Source code in <code>src/amisc/training.py</code> <pre><code>@staticmethod\ndef is_one_level_refinement(beta_old: tuple, beta_new: tuple) -&gt; bool:\n    \"\"\"Check if a new `beta` multi-index is a one-level refinement from a previous `beta`.\n\n    !!! Example\n        Refining from `(0, 1, 2)` to the new multi-index `(1, 1, 2)` is a one-level refinement. But refining to\n        either `(2, 1, 2)` or `(1, 2, 2)` are not, since more than one refinement occurs at the same time.\n\n    :param beta_old: the starting multi-index\n    :param beta_new: the new refined multi-index\n    :returns: whether `beta_new` is a one-level refinement from `beta_old`\n    \"\"\"\n    level_diff = np.array(beta_new, dtype=int) - np.array(beta_old, dtype=int)\n    ind = np.nonzero(level_diff)[0]\n    return ind.shape[0] == 1 and level_diff[ind] == 1\n</code></pre>"},{"location":"reference/training/#amisc.training.SparseGrid.refine","title":"<code>refine(alpha, beta, input_domains, weight_fcns=None)</code>","text":"<p>Refine the sparse grid for a given <code>alpha</code> and <code>beta</code> pair and given collocation rules. Return any new grid points that do not have model evaluations saved yet.</p> <p>Note</p> <p>The <code>beta</code> multi-index is used to determine the number of collocation points in each input dimension. The length of <code>beta</code> should therefore match the number of variables in <code>x_vars</code>.</p> Source code in <code>src/amisc/training.py</code> <pre><code>def refine(self, alpha: MultiIndex, beta: MultiIndex, input_domains: dict, weight_fcns: dict = None):\n    \"\"\"Refine the sparse grid for a given `alpha` and `beta` pair and given collocation rules. Return any new\n    grid points that do not have model evaluations saved yet.\n\n    !!! Note\n        The `beta` multi-index is used to determine the number of collocation points in each input dimension. The\n        length of `beta` should therefore match the number of variables in `x_vars`.\n    \"\"\"\n    weight_fcns = weight_fcns or {}\n\n    # Initialize a sparse grid for beta=(0, 0, ..., 0)\n    if np.sum(beta) == 0:\n        if len(self.x_grids) == 0:\n            num_latent = {}\n            for var in input_domains:\n                if LATENT_STR_ID in var:\n                    base_id = var.split(LATENT_STR_ID)[0]\n                    num_latent[base_id] = 1 if base_id not in num_latent else num_latent[base_id] + 1\n                else:\n                    num_latent[var] = 0\n            self.latent_size = num_latent\n\n            new_pt = {}\n            domains = iter(input_domains.items())\n            for grid_size in self.beta_to_knots(beta):\n                if isinstance(grid_size, int):  # scalars\n                    var, domain = next(domains)\n                    new_pt[var] = self.collocation_1d(grid_size, domain, method=self.collocation_rule,\n                                                      wt_fcn=weight_fcns.get(var, None),\n                                                      opt_args=self.opt_args).tolist()\n                else:                           # latent coeffs\n                    for s in grid_size:\n                        var, domain = next(domains)\n                        new_pt[var] = self.collocation_1d(s, domain, method=self.collocation_rule,\n                                                          wt_fcn=weight_fcns.get(var, None),\n                                                          opt_args=self.opt_args).tolist()\n            self.x_grids = new_pt\n        self.betas.add(beta)\n        self.yi_map.setdefault(alpha, dict())\n        self.yi_nan_map.setdefault(alpha, dict())\n        self.error_map.setdefault(alpha, dict())\n        new_coords = list(self._expand_grid_coords(beta))\n        return new_coords, self._extract_grid_points(new_coords)\n\n    # Otherwise, refine the sparse grid\n    for beta_old in self.betas:\n        # Get the first lower neighbor in the sparse grid and refine the 1d grid if necessary\n        if self.is_one_level_refinement(beta_old, beta):\n            new_grid_size = self.beta_to_knots(beta)\n            inputs = zip(self.x_grids.keys(), self.x_grids.values(), input_domains.values())\n\n            for new_size in new_grid_size:\n                if isinstance(new_size, int):       # scalar grid\n                    var, grid, domain = next(inputs)\n                    if len(grid) &lt; new_size:\n                        num_new_pts = new_size - len(grid)\n                        self.x_grids[var] = self.collocation_1d(num_new_pts, domain, grid, opt_args=self.opt_args,\n                                                                wt_fcn=weight_fcns.get(var, None),\n                                                                method=self.collocation_rule).tolist()\n                else:                               # latent grid\n                    for s_new in new_size:\n                        var, grid, domain = next(inputs)\n                        if len(grid) &lt; s_new:\n                            num_new_pts = s_new - len(grid)\n                            self.x_grids[var] = self.collocation_1d(num_new_pts, domain, grid,\n                                                                    opt_args=self.opt_args,\n                                                                    wt_fcn=weight_fcns.get(var, None),\n                                                                    method=self.collocation_rule).tolist()\n            break\n\n    new_coords = []\n    for coord in self._expand_grid_coords(beta):\n        if coord not in self.yi_map[alpha]:\n            # If we have not computed this grid coordinate yet\n            new_coords.append(coord)\n\n    new_pts = self._extract_grid_points(new_coords)\n\n    self.betas.add(beta)\n    return new_coords, new_pts\n</code></pre>"},{"location":"reference/training/#amisc.training.SparseGrid.set","title":"<code>set(alpha, beta, coords, yi_dict)</code>","text":"<p>Store model output <code>yi_dict</code> values.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>the model fidelity indices</p> <p> TYPE: <code>MultiIndex</code> </p> <code>beta</code> <p>the surrogate fidelity indices</p> <p> TYPE: <code>MultiIndex</code> </p> <code>coords</code> <p>a list of grid coordinates to locate the <code>yi</code> values in the sparse grid data structure</p> <p> TYPE: <code>list</code> </p> <code>yi_dict</code> <p>a <code>dict</code> of model output <code>yi</code> values</p> <p> TYPE: <code>dict[str, ArrayLike]</code> </p> Source code in <code>src/amisc/training.py</code> <pre><code>def set(self, alpha: MultiIndex, beta: MultiIndex, coords: list, yi_dict: dict[str, ArrayLike]):\n    \"\"\"Store model output `yi_dict` values.\n\n    :param alpha: the model fidelity indices\n    :param beta: the surrogate fidelity indices\n    :param coords: a list of grid coordinates to locate the `yi` values in the sparse grid data structure\n    :param yi_dict: a `dict` of model output `yi` values\n    \"\"\"\n    for i, coord in enumerate(coords):  # First dim of yi is loop dim aligning with coords\n        new_yi = {}\n        for var, yi in yi_dict.items():\n            yi = np.atleast_1d(yi[i])\n            new_yi[var] = (float(yi[0]) if self._is_numeric(yi) else yi[0]) if self._is_singleton(yi) else yi.tolist()  # noqa: E501\n        self.yi_map[alpha][coord] = copy.deepcopy(new_yi)\n</code></pre>"},{"location":"reference/training/#amisc.training.SparseGrid.set_errors","title":"<code>set_errors(alpha, beta, coords, errors)</code>","text":"<p>Store error information in the sparse-grid for a given multi-index pair.</p> Source code in <code>src/amisc/training.py</code> <pre><code>def set_errors(self, alpha: MultiIndex, beta: MultiIndex, coords: list, errors: list[dict]):\n    \"\"\"Store error information in the sparse-grid for a given multi-index pair.\"\"\"\n    for coord, error in zip(coords, errors):\n        self.error_map[alpha][coord] = copy.deepcopy(error)\n</code></pre>"},{"location":"reference/training/#amisc.training.TrainingData","title":"<code>TrainingData</code>","text":"<p>               Bases: <code>Serializable</code>, <code>ABC</code></p> <p>Interface for storing and collecting surrogate training data. <code>TrainingData</code> objects should:</p> <ul> <li><code>get</code> - retrieve the training data</li> <li><code>set</code> - store the training data</li> <li><code>refine</code> - generate new design points for the parent <code>Component</code> model</li> <li><code>clear</code> - clear all training data</li> <li><code>set_errors</code> - store error information (if desired)</li> <li><code>impute_missing_data</code> - fill in missing values in the training data (if desired)</li> </ul>"},{"location":"reference/training/#amisc.training.TrainingData.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>Clear all training data.</p> Source code in <code>src/amisc/training.py</code> <pre><code>@abstractmethod\ndef clear(self):\n    \"\"\"Clear all training data.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/training/#amisc.training.TrainingData.from_dict","title":"<code>from_dict(config)</code>  <code>classmethod</code>","text":"<p>Create a <code>TrainingData</code> object from a <code>dict</code> configuration. Currently, only <code>method='sparse-grid'</code> is supported for the <code>SparseGrid</code> class.</p> Source code in <code>src/amisc/training.py</code> <pre><code>@classmethod\ndef from_dict(cls, config: dict) -&gt; TrainingData:\n    \"\"\"Create a `TrainingData` object from a `dict` configuration. Currently, only `method='sparse-grid'` is\n    supported for the `SparseGrid` class.\n    \"\"\"\n    method = config.pop('method', 'sparse-grid').lower()\n    match method:\n        case 'sparse-grid':\n            return SparseGrid(**config)\n        case other:\n            raise NotImplementedError(f\"Unknown training data method: {other}\")\n</code></pre>"},{"location":"reference/training/#amisc.training.TrainingData.get","title":"<code>get(alpha, beta, y_vars=None, skip_nan=False)</code>  <code>abstractmethod</code>","text":"<p>Return the training data for a given multi-index pair.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>the model fidelity indices</p> <p> TYPE: <code>MultiIndex</code> </p> <code>beta</code> <p>the surrogate fidelity indices</p> <p> TYPE: <code>MultiIndex</code> </p> <code>y_vars</code> <p>the keys of the outputs to return (if <code>None</code>, return all outputs)</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> <code>skip_nan</code> <p>skip any data points with remaining <code>nan</code> values if <code>skip_nan=True</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple[Dataset, Dataset]</code> <p><code>dicts</code> of model inputs <code>x_train</code> and outputs <code>y_train</code></p> Source code in <code>src/amisc/training.py</code> <pre><code>@abstractmethod\ndef get(self, alpha: MultiIndex, beta: MultiIndex, y_vars: list[str] = None,\n        skip_nan: bool = False) -&gt; tuple[Dataset, Dataset]:\n    \"\"\"Return the training data for a given multi-index pair.\n\n    :param alpha: the model fidelity indices\n    :param beta: the surrogate fidelity indices\n    :param y_vars: the keys of the outputs to return (if `None`, return all outputs)\n    :param skip_nan: skip any data points with remaining `nan` values if `skip_nan=True`\n    :returns: `dicts` of model inputs `x_train` and outputs `y_train`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/training/#amisc.training.TrainingData.impute_missing_data","title":"<code>impute_missing_data(alpha, beta)</code>  <code>abstractmethod</code>","text":"<p>Impute missing values in the training data for a given multi-index pair (just pass if you don't care).</p> PARAMETER DESCRIPTION <code>alpha</code> <p>the model fidelity indices</p> <p> TYPE: <code>MultiIndex</code> </p> <code>beta</code> <p>the surrogate fidelity indices</p> <p> TYPE: <code>MultiIndex</code> </p> Source code in <code>src/amisc/training.py</code> <pre><code>@abstractmethod\ndef impute_missing_data(self, alpha: MultiIndex, beta: MultiIndex):\n    \"\"\"Impute missing values in the training data for a given multi-index pair (just pass if you don't care).\n\n    :param alpha: the model fidelity indices\n    :param beta: the surrogate fidelity indices\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/training/#amisc.training.TrainingData.refine","title":"<code>refine(alpha, beta, input_domains, weight_fcns=None)</code>  <code>abstractmethod</code>","text":"<p>Return new design/training points for a given multi-index pair and their coordinates/locations in the <code>TrainingData</code> storage structure.</p> <p>Example</p> <pre><code>domains = {'x1': (0, 1), 'x2': (0, 1)}\nalpha, beta = (0, 1), (1, 1)\ncoords, x_train = training_data.refine(alpha, beta, domains)\ny_train = my_model(x_train)\ntraining_data.set(alpha, beta, coords, y_train)\n</code></pre> <p>The returned data coordinates <code>coords</code> should be any object that can be used to locate the corresponding <code>x_train</code> training points in the <code>TrainingData</code> storage structure. These <code>coords</code> will be passed back to the <code>set</code> function to store the training data at a later time (i.e. after model evaluation).</p> PARAMETER DESCRIPTION <code>alpha</code> <p>the model fidelity indices</p> <p> TYPE: <code>MultiIndex</code> </p> <code>beta</code> <p>the surrogate fidelity indices</p> <p> TYPE: <code>MultiIndex</code> </p> <code>input_domains</code> <p>a <code>dict</code> specifying domain bounds for each input variable</p> <p> TYPE: <code>dict[str, tuple]</code> </p> <code>weight_fcns</code> <p>a <code>dict</code> of weighting functions for each input variable</p> <p> TYPE: <code>dict[str, callable]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[list[Any], Dataset]</code> <p>a list of new data coordinates <code>coords</code> and the corresponding training points <code>x_train</code></p> Source code in <code>src/amisc/training.py</code> <pre><code>@abstractmethod\ndef refine(self, alpha: MultiIndex, beta: MultiIndex, input_domains: dict[str, tuple],\n           weight_fcns: dict[str, callable] = None) -&gt; tuple[list[Any], Dataset]:\n    \"\"\"Return new design/training points for a given multi-index pair and their coordinates/locations in the\n    `TrainingData` storage structure.\n\n    !!! Example\n        ```python\n        domains = {'x1': (0, 1), 'x2': (0, 1)}\n        alpha, beta = (0, 1), (1, 1)\n        coords, x_train = training_data.refine(alpha, beta, domains)\n        y_train = my_model(x_train)\n        training_data.set(alpha, beta, coords, y_train)\n        ```\n\n    The returned data coordinates `coords` should be any object that can be used to locate the corresponding\n    `x_train` training points in the `TrainingData` storage structure. These `coords` will be passed back to the\n    `set` function to store the training data at a later time (i.e. after model evaluation).\n\n    :param alpha: the model fidelity indices\n    :param beta: the surrogate fidelity indices\n    :param input_domains: a `dict` specifying domain bounds for each input variable\n    :param weight_fcns: a `dict` of weighting functions for each input variable\n    :returns: a list of new data coordinates `coords` and the corresponding training points `x_train`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/training/#amisc.training.TrainingData.set","title":"<code>set(alpha, beta, coords, yi_dict)</code>  <code>abstractmethod</code>","text":"<p>Store training data for a given multi-index pair.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>the model fidelity indices</p> <p> TYPE: <code>MultiIndex</code> </p> <code>beta</code> <p>the surrogate fidelity indices</p> <p> TYPE: <code>MultiIndex</code> </p> <code>coords</code> <p>locations for storing the <code>yi</code> values in the underlying data structure</p> <p> TYPE: <code>list[Any]</code> </p> <code>yi_dict</code> <p>a <code>dict</code> of model output <code>yi</code> values, each entry should be the same length as <code>coords</code></p> <p> TYPE: <code>Dataset</code> </p> Source code in <code>src/amisc/training.py</code> <pre><code>@abstractmethod\ndef set(self, alpha: MultiIndex, beta: MultiIndex, coords: list[Any], yi_dict: Dataset):\n    \"\"\"Store training data for a given multi-index pair.\n\n    :param alpha: the model fidelity indices\n    :param beta: the surrogate fidelity indices\n    :param coords: locations for storing the `yi` values in the underlying data structure\n    :param yi_dict: a `dict` of model output `yi` values, each entry should be the same length as `coords`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/training/#amisc.training.TrainingData.set_errors","title":"<code>set_errors(alpha, beta, coords, errors)</code>  <code>abstractmethod</code>","text":"<p>Store error information for a given multi-index pair (just pass if you don't care).</p> PARAMETER DESCRIPTION <code>alpha</code> <p>the model fidelity indices</p> <p> TYPE: <code>MultiIndex</code> </p> <code>beta</code> <p>the surrogate fidelity indices</p> <p> TYPE: <code>MultiIndex</code> </p> <code>coords</code> <p>locations for storing the error information in the underlying data structure</p> <p> TYPE: <code>list[Any]</code> </p> <code>errors</code> <p>a list of error dictionaries, should be the same length as <code>coords</code></p> <p> TYPE: <code>list[dict]</code> </p> Source code in <code>src/amisc/training.py</code> <pre><code>@abstractmethod\ndef set_errors(self, alpha: MultiIndex, beta: MultiIndex, coords: list[Any], errors: list[dict]):\n    \"\"\"Store error information for a given multi-index pair (just pass if you don't care).\n\n    :param alpha: the model fidelity indices\n    :param beta: the surrogate fidelity indices\n    :param coords: locations for storing the error information in the underlying data structure\n    :param errors: a list of error dictionaries, should be the same length as `coords`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/transform/","title":"transform","text":""},{"location":"reference/transform/#amisc.transform","title":"<code>amisc.transform</code>","text":"<p>Module for data transformation methods.</p> <p>Includes:</p> <ul> <li><code>Transform</code> \u2014 an abstract interface for specifying a transformation.</li> <li><code>Linear</code> \u2014 a linear transformation \\(y=mx+b\\).</li> <li><code>Log</code> \u2014 a logarithmic transformation \\(y=\\log_b(x + \\mathrm{offset})\\).</li> <li><code>Minmax</code> \u2014 a min-max scaling transformation \\(x: (lb, ub) \\mapsto (lb_{norm}, ub_{norm})\\).</li> <li><code>Zscore</code> \u2014 a z-score normalization transformation \\(y=(x-\\mu)/\\sigma\\).</li> </ul> <p>Transform objects can be converted easily to/from strings for serialization.</p>"},{"location":"reference/transform/#amisc.transform.Linear","title":"<code>Linear(transform_args)</code>","text":"<p>               Bases: <code>Transform</code></p> <p>A Linear transform: \\(y=mx+b\\).</p> ATTRIBUTE DESCRIPTION <code>transform_args</code> <p><code>(m, b)</code> the slope and offset</p> <p> </p> Source code in <code>src/amisc/transform.py</code> <pre><code>def __init__(self, transform_args: tuple):\n    self.transform_args = transform_args\n</code></pre>"},{"location":"reference/transform/#amisc.transform.Log","title":"<code>Log(transform_args)</code>","text":"<p>               Bases: <code>Transform</code></p> <p>A Log transform: \\(y=\\log_b(x + \\mathrm{offset})\\).</p> ATTRIBUTE DESCRIPTION <code>transform_args</code> <p><code>(base, offset)</code> the log base and offset</p> <p> </p> Source code in <code>src/amisc/transform.py</code> <pre><code>def __init__(self, transform_args: tuple):\n    self.transform_args = transform_args\n</code></pre>"},{"location":"reference/transform/#amisc.transform.Minmax","title":"<code>Minmax(transform_args)</code>","text":"<p>               Bases: <code>Transform</code></p> <p>A Minmax transform: \\(x: (lb, ub) \\mapsto (lb_{norm}, ub_{norm})\\).</p> ATTRIBUTE DESCRIPTION <code>transform_args</code> <p><code>(lb, ub, lb_norm, ub_norm)</code> the original lower and upper bounds and the normalized bounds</p> <p> </p> Source code in <code>src/amisc/transform.py</code> <pre><code>def __init__(self, transform_args: tuple):\n    self.transform_args = transform_args\n</code></pre>"},{"location":"reference/transform/#amisc.transform.Minmax.update","title":"<code>update(lb=None, ub=None, lb_norm=None, ub_norm=None)</code>","text":"<p>Update the parameters of this transform.</p> PARAMETER DESCRIPTION <code>lb</code> <p>the lower bound in the original variable space</p> <p> DEFAULT: <code>None</code> </p> <code>ub</code> <p>the upper bound in the original variable space</p> <p> DEFAULT: <code>None</code> </p> <code>lb_norm</code> <p>the lower bound of the transformed space</p> <p> DEFAULT: <code>None</code> </p> <code>ub_norm</code> <p>the upper bound of the transformed space</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/transform.py</code> <pre><code>def update(self, lb=None, ub=None, lb_norm=None, ub_norm=None):\n    \"\"\"Update the parameters of this transform.\n\n    :param lb: the lower bound in the original variable space\n    :param ub: the upper bound in the original variable space\n    :param lb_norm: the lower bound of the transformed space\n    :param ub_norm: the upper bound of the transformed space\n    \"\"\"\n    transform_args = (lb, ub, lb_norm, ub_norm)\n    self.transform_args = tuple([ele if ele is not None else self.transform_args[i]\n                                 for i, ele in enumerate(transform_args)])\n</code></pre>"},{"location":"reference/transform/#amisc.transform.Transform","title":"<code>Transform(transform_args)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for all transformations.</p> ATTRIBUTE DESCRIPTION <code>transform_args</code> <p>the arguments for the transformation</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>src/amisc/transform.py</code> <pre><code>def __init__(self, transform_args: tuple):\n    self.transform_args = transform_args\n</code></pre>"},{"location":"reference/transform/#amisc.transform.Transform.from_string","title":"<code>from_string(transform_spec)</code>  <code>classmethod</code>","text":"<p>Return a list of <code>Transforms</code> given a list of string specifications. Available transformations are:</p> <ul> <li>linear \u2014 \\(x_{norm} = mx + b\\) specified as <code>linear(m, b)</code> or <code>linear(slope=m, offset=b)</code>. <code>m=1, b=0</code> if not                specified.</li> <li>log \u2014 \\(x_{norm} = \\log_b(x)\\) specified as <code>log</code> or <code>log10</code> for the natural or common logarithms. For a             different base, use <code>log(b)</code>. Optionally, specify <code>offset</code> for <code>log(x+offset)</code>.</li> <li>minmax \u2014 \\(x_{norm} = \\frac{x - a}{b - a}(u - l) + l\\) specified as <code>minmax(a, b, l, u)</code> or                <code>minmax(lb=a, ub=b, lb_norm=l, ub_norm=u)</code>. Scales <code>x</code> from the range <code>(a, b)</code> to <code>(l, u)</code>. By                default, <code>(a, b)</code> is the Variable's domain and <code>(l, u)</code> is <code>(0, 1)</code>. Use simply as <code>minmax</code>                to use all defaults.</li> <li>zscore \u2014 \\(x_{norm} = \\frac{x - m}{s}\\) specified as <code>zscore(m, s)</code> or <code>zscore(mu=m, std=s)</code>. If the                Variable is specified as <code>distribution=normal</code>, then <code>zscore</code> defaults to the Variable's                own <code>mu, std</code>.</li> </ul> <p>Example</p> <p><pre><code>transforms = Transform.from_string(['log10', 'linear(2, 4)'])\nprint(transforms)\n</code></pre> will give <pre><code>[ Log(10), Linear(2, 4) ]  # the corresponding `Transform` objects\n</code></pre></p> <p>Warning</p> <p>You may optionally leave the <code>minmax</code> arguments blank to defer to the bounds of the parent <code>Variable</code>. You may also optionally leave the <code>zscore</code> arguments blank to defer to the <code>(mu, std)</code> of the parent <code>Variable</code>, but this will throw a runtime error if <code>Variable.distribution</code> is not <code>Normal(mu, std)</code>.</p> Source code in <code>src/amisc/transform.py</code> <pre><code>@classmethod\ndef from_string(cls, transform_spec: str | list[str]) -&gt; list[Transform] | None:\n    \"\"\"Return a list of `Transforms` given a list of string specifications. Available transformations are:\n\n    - **linear** \u2014 $x_{norm} = mx + b$ specified as `linear(m, b)` or `linear(slope=m, offset=b)`. `m=1, b=0` if not\n                   specified.\n    - **log** \u2014 $x_{norm} = \\\\log_b(x)$ specified as `log` or `log10` for the natural or common logarithms. For a\n                different base, use `log(b)`. Optionally, specify `offset` for `log(x+offset)`.\n    - **minmax** \u2014 $x_{norm} = \\\\frac{x - a}{b - a}(u - l) + l$ specified as `minmax(a, b, l, u)` or\n                   `minmax(lb=a, ub=b, lb_norm=l, ub_norm=u)`. Scales `x` from the range `(a, b)` to `(l, u)`. By\n                   default, `(a, b)` is the Variable's domain and `(l, u)` is `(0, 1)`. Use simply as `minmax`\n                   to use all defaults.\n    - **zscore** \u2014 $x_{norm} = \\\\frac{x - m}{s}$ specified as `zscore(m, s)` or `zscore(mu=m, std=s)`. If the\n                   Variable is specified as `distribution=normal`, then `zscore` defaults to the Variable's\n                   own `mu, std`.\n\n    !!! Example\n        ```python\n        transforms = Transform.from_string(['log10', 'linear(2, 4)'])\n        print(transforms)\n        ```\n        will give\n        ```shell\n        [ Log(10), Linear(2, 4) ]  # the corresponding `Transform` objects\n        ```\n\n    !!! Warning\n        You may optionally leave the `minmax` arguments blank to defer to the bounds of the parent `Variable`.\n        You may also optionally leave the `zscore` arguments blank to defer to the `(mu, std)` of the parent\n        `Variable`, but this will throw a runtime error if `Variable.distribution` is not `Normal(mu, std)`.\n    \"\"\"\n    if transform_spec is None:\n        return None\n    if isinstance(transform_spec, str | Transform):\n        transform_spec = [transform_spec]\n\n    transforms = []\n    for spec_string in transform_spec:\n        if isinstance(spec_string, Transform):\n            transforms.append(spec_string)\n            continue\n\n        name, args, kwargs = parse_function_string(spec_string)\n        if name.lower() == 'linear':\n            try:\n                slope = float(kwargs.get('slope', args[0] if len(args) &gt; 0 else 1))\n                offset = float(kwargs.get('offset', args[1] if len(args) &gt; 1 else 0))\n                transforms.append(Linear((slope, offset)))\n            except Exception as e:\n                raise ValueError(f'Linear transform spec \"{spec_string}\" is not valid: Try \"linear(m, b)\".') from e\n        elif name.lower() in ['log', 'log10']:\n            try:\n                log_base = float(kwargs.get('base', args[0] if len(args) &gt; 0 else (np.e if name.lower() == 'log'\n                                                                                   else 10)))\n                offset = float(kwargs.get('offset', args[1] if len(args) &gt; 1 else 0))\n                transforms.append(Log((log_base, offset)))\n            except Exception as e:\n                raise ValueError(f'Log transform spec \"{spec_string}\" is not valid: Try \"log(base, offset)\"') from e\n        elif name.lower() in ['minmax', 'maxabs']:\n            try:\n                # Defer bounds to the Variable by setting np.nan\n                lb = float(kwargs.get('lb', args[0] if len(args) &gt; 0 else np.nan))\n                ub = float(kwargs.get('ub', args[1] if len(args) &gt; 1 else np.nan))\n                lb_norm = float(kwargs.get('lb_norm', args[2] if len(args) &gt; 2 else 0))\n                ub_norm = float(kwargs.get('ub_norm', args[3] if len(args) &gt; 3 else 1))\n                transforms.append(Minmax((lb, ub, lb_norm, ub_norm)))\n            except Exception as e:\n                raise ValueError(f'Minmax transform spec \"{spec_string}\" is not valid: Try \"minmax(lb, ub)\"') from e\n        elif name.lower() in ['z', 'zscore']:\n            try:\n                # Defer (mu, std) to the Variable by setting np.nan\n                mu = float(kwargs.get('mu', args[0] if len(args) &gt; 0 else np.nan))\n                std = float(kwargs.get('std', args[1] if len(args) &gt; 1 else np.nan))\n                transforms.append(Zscore((mu, std)))\n            except Exception as e:\n                raise ValueError(f'Z-score normalization string \"{spec_string}\" is not valid: '\n                                 f'Try \"zscore(mu, std)\".') from e\n        else:\n            raise NotImplementedError(f'Transform method \"{name}\" is not implemented.')\n\n    return transforms\n</code></pre>"},{"location":"reference/transform/#amisc.transform.Transform.transform","title":"<code>transform(x, inverse=False, transform_args=None)</code>","text":"<p>Transform the given values <code>x</code>. This wrapper function handles the input type and tries to  return the transformed values in the same type.</p> PARAMETER DESCRIPTION <code>x</code> <p>the values to transform</p> <p> TYPE: <code>ArrayLike</code> </p> <code>inverse</code> <p>whether to do the inverse transform instead</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>transform_args</code> <p>overrides <code>Transform.transform_args</code></p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ArrayLike</code> <p>the transformed values</p> Source code in <code>src/amisc/transform.py</code> <pre><code>def transform(self, x: ArrayLike, inverse: bool = False, transform_args: tuple = None) -&gt; ArrayLike:\n    \"\"\"Transform the given values `x`. This wrapper function handles the input type and tries to\n     return the transformed values in the same type.\n\n    :param x: the values to transform\n    :param inverse: whether to do the inverse transform instead\n    :param transform_args: overrides `Transform.transform_args`\n    :return: the transformed values\n    \"\"\"\n    input_type = type(x)\n    result = self._transform(np.atleast_1d(x), inverse, transform_args)\n    if input_type in [int, float]:\n        return float(result[0])\n    elif input_type is list:\n        return result.tolist()\n    elif input_type is tuple:\n        return tuple(result.tolist())\n    else:\n        return result  # just keep as np.ndarray for everything else\n</code></pre>"},{"location":"reference/transform/#amisc.transform.Zscore","title":"<code>Zscore(transform_args)</code>","text":"<p>               Bases: <code>Transform</code></p> <p>A Zscore transform: \\(y=(x-\\mu)/\\sigma\\).</p> ATTRIBUTE DESCRIPTION <code>transform_args</code> <p><code>(mu, std)</code> the mean and standard deviation</p> <p> </p> Source code in <code>src/amisc/transform.py</code> <pre><code>def __init__(self, transform_args: tuple):\n    self.transform_args = transform_args\n</code></pre>"},{"location":"reference/transform/#amisc.transform.Zscore.update","title":"<code>update(mu=None, std=None)</code>","text":"<p>Update the parameters of this transform.</p> PARAMETER DESCRIPTION <code>mu</code> <p>the mean of the transform</p> <p> DEFAULT: <code>None</code> </p> <code>std</code> <p>the standard deviation of the transform</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>src/amisc/transform.py</code> <pre><code>def update(self, mu=None, std=None):\n    \"\"\"Update the parameters of this transform.\n\n    :param mu: the mean of the transform\n    :param std: the standard deviation of the transform\n    \"\"\"\n    transform_args = (mu, std)\n    self.transform_args = tuple([ele if ele is not None else self.transform_args[i]\n                                 for i, ele in enumerate(transform_args)])\n</code></pre>"},{"location":"reference/typing/","title":"typing","text":""},{"location":"reference/typing/#amisc.typing","title":"<code>amisc.typing</code>","text":"<p>Module with type hints for the AMISC package.</p> <p>Includes:</p> <ul> <li><code>MultiIndex</code> \u2014 tuples of integers or similar string representations</li> <li><code>Dataset</code> \u2014 a type hint for the input/output <code>dicts</code> handled by <code>Component.model</code></li> <li><code>TrainIteration</code> \u2014 the results of a single training iteration</li> <li><code>CompressionData</code> \u2014 a dictionary spec for passing data to/from <code>Variable.compress()</code></li> <li><code>LATENT_STR_ID</code> \u2014 a string identifier for latent coefficients of field quantities</li> <li><code>COORDS_STR_ID</code> \u2014 a string identifier for coordinate locations of field quantities</li> </ul>"},{"location":"reference/typing/#amisc.typing.CompressionData","title":"<code>CompressionData</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration <code>dict</code> for passing compression data to/from <code>Variable.compress()</code>.</p> <p>Field quantity shapes</p> <p>Field quantity data can take on any arbitrary shape, which we indicate with <code>qty.shape</code>. For example, a 3d structured grid might have <code>qty.shape = (10, 15, 10)</code>. Unstructured data might just have <code>qty.shape = (N,)</code> for \\(N\\) points in an unstructured grid. Regardless, <code>Variable.compress()</code> will flatten this and compress to a single latent vector of size <code>latent_size</code>. That is, <code>qty.shape</code> \u2192 <code>latent_size</code>.</p> <p>Compression coordinates</p> <p>Field quantity data must be specified along with its coordinate locations. If the coordinate locations are different from what was used when building the compression map (i.e. the SVD data matrix), then they will be interpolated to/from the SVD coordinates.</p> ATTRIBUTE DESCRIPTION <code>coords</code> <p><code>(qty.shape, dim)</code> the coordinate locations of the qty data; coordinates exist in <code>dim</code> space (e.g. <code>dim=2</code> for 2d Cartesian coordinates). Defaults to the coordinates used when building the construction map (i.e. the coordinates of the data in the SVD data matrix)</p> <p> TYPE: <code>ndarray</code> </p> <code>latent</code> <p><code>(..., latent_size)</code> array of latent space coefficients for a field quantity; this is what is returned by <code>Variable.compress()</code> and what is expected as input by <code>Variable.reconstruct()</code>.</p> <p> TYPE: <code>ndarray</code> </p> <code>qty</code> <p><code>(..., qty.shape)</code> array of uncompressed field quantity data for this qty within the <code>fields</code> list. Each qty in this list will be its own <code>key:value</code> pair in the <code>CompressionData</code> structure</p> <p> TYPE: <code>ndarray</code> </p>"},{"location":"reference/typing/#amisc.typing.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Type hint for the input/output <code>dicts</code> of a call to <code>Component.model</code>. The keys are the variable names and the values are the corresponding <code>np.ndarrays</code>. There are also a few special keys that can be returned by the model that are described below.</p> <p>The model can return additional items that are not part of <code>Component.outputs</code>. These items are returned as object arrays in the output.</p> <p>This data structure is very similar to the <code>Dataset</code> class in the <code>xarray</code> package. Later versions might consider migrating to <code>xarray</code> for more advanced data manipulation.</p> ATTRIBUTE DESCRIPTION <code>model_cost</code> <p>the computational cost (seconds of CPU time) of a single model evaluation</p> <p> TYPE: <code>float | list | ArrayLike</code> </p> <code>output_path</code> <p>the path to the output file or directory written by the model</p> <p> TYPE: <code>str | Path</code> </p> <code>errors</code> <p>a <code>dict</code> with the indices where the model evaluation failed with context about the errors</p> <p> TYPE: <code>dict</code> </p>"},{"location":"reference/typing/#amisc.typing.MultiIndex","title":"<code>MultiIndex</code>","text":"<p>               Bases: <code>tuple</code></p> <p>A multi-index is a tuple of integers, can be converted from a string.</p>"},{"location":"reference/typing/#amisc.typing.TrainIteration","title":"<code>TrainIteration</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Gives the results of a single training iteration.</p> ATTRIBUTE DESCRIPTION <code>component</code> <p>the name of the component selected for refinement at this iteration</p> <p> TYPE: <code>str</code> </p> <code>alpha</code> <p>the selected candidate model fidelity multi-index</p> <p> TYPE: <code>MultiIndex</code> </p> <code>beta</code> <p>the selected candidate surrogate fidelity multi-index</p> <p> TYPE: <code>MultiIndex</code> </p> <code>num_evals</code> <p>the number of model evaluations performed during this iteration</p> <p> TYPE: <code>int</code> </p> <code>added_cost</code> <p>the total added computational cost of the new model evaluations (CPU time in seconds)</p> <p> TYPE: <code>float</code> </p> <code>added_error</code> <p>the error/difference between the refined surrogate and the previous surrogate</p> <p> TYPE: <code>float</code> </p> <code>test_error</code> <p>the error of the refined surrogate on the test set (optional)</p> <p> TYPE: <code>Optional[dict[str, float]]</code> </p> <code>overhead_s</code> <p>the algorithmic overhead wall time in seconds for the training iteration</p> <p> TYPE: <code>float</code> </p> <code>model_s</code> <p>the total wall time in seconds for the model evaluations for the training iteration</p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/#amisc.utils","title":"<code>amisc.utils</code>","text":"<p>Provides some basic utilities for the package.</p> <p>Includes:</p> <ul> <li><code>to_model_dataset</code> \u2014 convert surrogate input/output dataset to a form usable by the true model</li> <li><code>to_surrogate_dataset</code> \u2014 convert true model input/output dataset to a form usable by the surrogate</li> <li><code>constrained_lls</code> \u2014 solve a constrained linear least squares problem</li> <li><code>search_for_file</code> \u2014 search for a file in the current working directory and additional search paths</li> <li><code>format_inputs</code> \u2014 broadcast and reshape all inputs to the same shape</li> <li><code>format_outputs</code> \u2014 reshape all outputs to a common loop shape</li> <li><code>parse_function_string</code> \u2014 convert function-like strings to arguments and keyword-arguments</li> <li><code>relative_error</code> \u2014 compute the relative L2 error between two vectors</li> <li><code>get_logger</code> \u2014 logging utility with nice formatting</li> </ul>"},{"location":"reference/utils/#amisc.utils.constrained_lls","title":"<code>constrained_lls(A, b, C, d)</code>","text":"<p>Minimize \\(||Ax-b||_2\\), subject to \\(Cx=d\\), i.e. constrained linear least squares.</p> <p>Note</p> <p>See these lecture notes for more detail.</p> PARAMETER DESCRIPTION <code>A</code> <p><code>(..., M, N)</code>, vandermonde matrix</p> <p> TYPE: <code>ndarray</code> </p> <code>b</code> <p><code>(..., M, 1)</code>, data</p> <p> TYPE: <code>ndarray</code> </p> <code>C</code> <p><code>(..., P, N)</code>, constraint operator</p> <p> TYPE: <code>ndarray</code> </p> <code>d</code> <p><code>(..., P, 1)</code>, constraint condition</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p><code>(..., N, 1)</code>, the solution parameter vector <code>x</code></p> Source code in <code>src/amisc/utils.py</code> <pre><code>def constrained_lls(A: np.ndarray, b: np.ndarray, C: np.ndarray, d: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Minimize $||Ax-b||_2$, subject to $Cx=d$, i.e. constrained linear least squares.\n\n    !!! Note\n        See [these lecture notes](http://www.seas.ucla.edu/~vandenbe/133A/lectures/cls.pdf) for more detail.\n\n    :param A: `(..., M, N)`, vandermonde matrix\n    :param b: `(..., M, 1)`, data\n    :param C: `(..., P, N)`, constraint operator\n    :param d: `(..., P, 1)`, constraint condition\n    :returns: `(..., N, 1)`, the solution parameter vector `x`\n    \"\"\"\n    M = A.shape[-2]\n    dims = len(A.shape[:-2])\n    T_axes = tuple(np.arange(0, dims)) + (-1, -2)\n    Q, R = np.linalg.qr(np.concatenate((A, C), axis=-2))\n    Q1 = Q[..., :M, :]\n    Q2 = Q[..., M:, :]\n    Q1_T = np.transpose(Q1, axes=T_axes)\n    Q2_T = np.transpose(Q2, axes=T_axes)\n    Qtilde, Rtilde = np.linalg.qr(Q2_T)\n    Qtilde_T = np.transpose(Qtilde, axes=T_axes)\n    Rtilde_T_inv = np.linalg.pinv(np.transpose(Rtilde, axes=T_axes))\n    w = np.linalg.pinv(Rtilde) @ (Qtilde_T @ Q1_T @ b - Rtilde_T_inv @ d)\n\n    return np.linalg.pinv(R) @ (Q1_T @ b - Q2_T @ w)\n</code></pre>"},{"location":"reference/utils/#amisc.utils.format_inputs","title":"<code>format_inputs(inputs, var_shape=None)</code>","text":"<p>Broadcast and reshape all inputs to the same shape. Loop shape is inferred from broadcasting the leading dims of all input arrays. Input arrays are broadcast to this shape and then flattened.</p> <p>Example</p> <pre><code>inputs = {'x': np.random.rand(10, 1, 5), 'y': np.random.rand(1, 1), 'z': np.random.rand(1, 20, 3)}\nfmt_inputs, loop_shape = format_inputs(inputs)\n# Output: {'x': np.ndarray(200, 5), 'y': np.ndarray(200,), 'z': np.ndarray(200, 3)}, (10, 20)\n</code></pre> PARAMETER DESCRIPTION <code>inputs</code> <p><code>dict</code> of input arrays</p> <p> TYPE: <code>Dataset</code> </p> <code>var_shape</code> <p><code>dict</code> of expected input variable shapes (i.e. for field quantities); assumes all inputs are 1d if None or not specified (i.e. scalar)</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Dataset, tuple[int, ...]]</code> <p>the reshaped inputs and the common loop shape</p> Source code in <code>src/amisc/utils.py</code> <pre><code>def format_inputs(inputs: Dataset, var_shape: dict = None) -&gt; tuple[Dataset, tuple[int, ...]]:\n    \"\"\"Broadcast and reshape all inputs to the same shape. Loop shape is inferred from broadcasting the leading dims\n    of all input arrays. Input arrays are broadcast to this shape and then flattened.\n\n    !!! Example\n        ```python\n        inputs = {'x': np.random.rand(10, 1, 5), 'y': np.random.rand(1, 1), 'z': np.random.rand(1, 20, 3)}\n        fmt_inputs, loop_shape = format_inputs(inputs)\n        # Output: {'x': np.ndarray(200, 5), 'y': np.ndarray(200,), 'z': np.ndarray(200, 3)}, (10, 20)\n        ```\n\n    :param inputs: `dict` of input arrays\n    :param var_shape: `dict` of expected input variable shapes (i.e. for field quantities); assumes all inputs are 1d\n                      if None or not specified (i.e. scalar)\n    :returns: the reshaped inputs and the common loop shape\n    \"\"\"\n    var_shape = var_shape or {}\n\n    def _common_shape(shape1, shape2):\n        \"\"\"Find the common leading dimensions between two shapes (with np broadcasting rules).\"\"\"\n        min_len = min(len(shape1), len(shape2))\n        common_shape = []\n        for i in range(min_len):\n            if shape1[i] == shape2[i]:\n                common_shape.append(shape1[i])\n            elif shape1[i] == 1:\n                common_shape.append(shape2[i])\n            elif shape2[i] == 1:\n                common_shape.append(shape1[i])\n            else:\n                break\n        return tuple(common_shape)\n\n    def _shorten_shape(name, array):\n        \"\"\"Remove extra variable dimensions from the end of the array shape (i.e. field quantity dimensions).\"\"\"\n        shape = var_shape.get(name, None)\n        if shape is not None and len(shape) &gt; 0:\n            if len(shape) &gt; len(array.shape):\n                raise ValueError(f\"Variable '{name}' shape {shape} is longer than input array shape {array.shape}. \"\n                                 f\"The input array for '{name}' should have at least {len(shape)} dimensions.\")\n            return array.shape[:-len(shape)]\n        else:\n            return array.shape\n\n    # Get the common \"loop\" dimensions from all input arrays\n    inputs = {name: np.atleast_1d(value) for name, value in inputs.items()}\n    name, array = next(iter(inputs.items()))\n    loop_shape = _shorten_shape(name, array)\n    for name, array in inputs.items():\n        array_shape = _shorten_shape(name, array)\n        loop_shape = _common_shape(loop_shape, array_shape)\n        if not loop_shape:\n            break\n    N = np.prod(loop_shape)\n    common_dim_cnt = len(loop_shape)\n\n    # Flatten and broadcast all inputs to the common shape\n    ret_inputs = {}\n    for var_id, array in inputs.items():\n        if common_dim_cnt &gt; 0:\n            broadcast_shape = np.broadcast_shapes(loop_shape, array.shape[:common_dim_cnt])\n            broadcast_shape += array.shape[common_dim_cnt:]\n            ret_inputs[var_id] = np.broadcast_to(array, broadcast_shape).reshape((N, *array.shape[common_dim_cnt:]))\n        else:\n            ret_inputs[var_id] = array\n\n    return ret_inputs, loop_shape\n</code></pre>"},{"location":"reference/utils/#amisc.utils.format_outputs","title":"<code>format_outputs(outputs, loop_shape)</code>","text":"<p>Reshape all outputs to the common loop shape. Loop shape is as obtained from a call to <code>format_inputs</code>. Assumes that all outputs are the same along the first dimension. This first dimension gets reshaped back into the <code>loop_shape</code>. Singleton outputs are squeezed along the last dimension. A singleton loop shape is squeezed along the first dimension.</p> <p>Example</p> <pre><code>outputs = {'x': np.random.rand(10, 1, 5), 'y': np.random.rand(10, 1), 'z': np.random.rand(10, 20, 3)}\nloop_shape = (2, 5)\nfmt_outputs = format_outputs(outputs, loop_shape)\n# Output: {'x': np.ndarray(2, 5, 1, 5), 'y': np.ndarray(2, 5), 'z': np.ndarray(200, 3)}, (2, 5, 20, 3)\n</code></pre> PARAMETER DESCRIPTION <code>outputs</code> <p><code>dict</code> of output arrays</p> <p> TYPE: <code>Dataset</code> </p> <code>loop_shape</code> <p>the common leading dimensions to reshape the output arrays to</p> <p> TYPE: <code>tuple[int, ...]</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>the reshaped outputs</p> Source code in <code>src/amisc/utils.py</code> <pre><code>def format_outputs(outputs: Dataset, loop_shape: tuple[int, ...]) -&gt; Dataset:\n    \"\"\"Reshape all outputs to the common loop shape. Loop shape is as obtained from a call to `format_inputs`.\n    Assumes that all outputs are the same along the first dimension. This first dimension gets reshaped back into\n    the `loop_shape`. Singleton outputs are squeezed along the last dimension. A singleton loop shape is squeezed\n    along the first dimension.\n\n    !!! Example\n        ```python\n        outputs = {'x': np.random.rand(10, 1, 5), 'y': np.random.rand(10, 1), 'z': np.random.rand(10, 20, 3)}\n        loop_shape = (2, 5)\n        fmt_outputs = format_outputs(outputs, loop_shape)\n        # Output: {'x': np.ndarray(2, 5, 1, 5), 'y': np.ndarray(2, 5), 'z': np.ndarray(200, 3)}, (2, 5, 20, 3)\n        ```\n\n    :param outputs: `dict` of output arrays\n    :param loop_shape: the common leading dimensions to reshape the output arrays to\n    :returns: the reshaped outputs\n    \"\"\"\n    output_dict = {}\n    for key, val in outputs.items():\n        val = np.atleast_1d(val)\n        output_shape = val.shape[1:]  # Assumes (N, ...) output shape to start with\n        val = val.reshape(loop_shape + output_shape)\n        if output_shape == (1,):\n            val = np.atleast_1d(np.squeeze(val, axis=-1))  # Squeeze singleton outputs\n        if loop_shape == (1,):\n            val = np.atleast_1d(np.squeeze(val, axis=0))  # Squeeze singleton loop dimensions\n        output_dict[key] = val\n    return output_dict\n</code></pre>"},{"location":"reference/utils/#amisc.utils.get_logger","title":"<code>get_logger(name, stdout=True, log_file=None, level=logging.INFO)</code>","text":"<p>Return a file/stdout logger with the given name.</p> PARAMETER DESCRIPTION <code>name</code> <p>the name of the logger to return</p> <p> TYPE: <code>str</code> </p> <code>stdout</code> <p>whether to add a stdout stream handler to the logger</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>log_file</code> <p>add file logging to this file (optional)</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>None</code> </p> <code>level</code> <p>the logging level to set</p> <p> TYPE: <code>int</code> DEFAULT: <code>INFO</code> </p> RETURNS DESCRIPTION <code>Logger</code> <p>the logger</p> Source code in <code>src/amisc/utils.py</code> <pre><code>def get_logger(name: str, stdout: bool = True, log_file: str | Path = None,\n               level: int = logging.INFO) -&gt; logging.Logger:\n    \"\"\"Return a file/stdout logger with the given name.\n\n    :param name: the name of the logger to return\n    :param stdout: whether to add a stdout stream handler to the logger\n    :param log_file: add file logging to this file (optional)\n    :param level: the logging level to set\n    :returns: the logger\n    \"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    logger.handlers.clear()\n    if stdout:\n        std_handler = logging.StreamHandler(sys.stdout)\n        std_handler.setFormatter(LOG_FORMATTER)\n        logger.addHandler(std_handler)\n    if log_file is not None:\n        f_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')\n        f_handler.setLevel(level)\n        f_handler.setFormatter(LOG_FORMATTER)\n        logger.addHandler(f_handler)\n\n    return logger\n</code></pre>"},{"location":"reference/utils/#amisc.utils.parse_function_string","title":"<code>parse_function_string(call_string)</code>","text":"<p>Convert a function signature like <code>func(a, b, key=value)</code> to name, args, kwargs.</p> PARAMETER DESCRIPTION <code>call_string</code> <p>a function-like string to parse</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>tuple[str, list, dict]</code> <p>the function name, positional arguments, and keyword arguments</p> Source code in <code>src/amisc/utils.py</code> <pre><code>def parse_function_string(call_string: str) -&gt; tuple[str, list, dict]:\n    \"\"\"Convert a function signature like `func(a, b, key=value)` to name, args, kwargs.\n\n    :param call_string: a function-like string to parse\n    :returns: the function name, positional arguments, and keyword arguments\n    \"\"\"\n    # Regex pattern to match function name and arguments\n    pattern = r\"(\\w+)(?:\\((.*)\\))?\"\n    match = re.match(pattern, call_string.strip())\n\n    if not match:\n        raise ValueError(f\"Function string '{call_string}' is not valid.\")\n\n    # Extracting name and arguments section\n    name = match.group(1)\n    args_str = match.group(2)\n\n    # Regex to split arguments respecting parentheses and quotes\n    # arg_pattern = re.compile(r'''((?:[^,'\"()\\[\\]{}*]+|'[^']*'|\"(?:\\\\.|[^\"\\\\])*\"|\\([^)]*\\)|\\[[^\\]]*\\]|\\{[^{}]*\\}|\\*)+|,)''')  # noqa: E501\n    # pieces = [piece.strip() for piece in arg_pattern.findall(args_str) if piece.strip() != ',']\n    pieces = _tokenize(args_str)\n\n    args = []\n    kwargs = {}\n    keyword_only = False\n\n    for piece in pieces:\n        if piece == '/':\n            continue\n        elif piece == '*':\n            keyword_only = True\n        elif '=' in piece and (piece.index('=') &lt; piece.find('{') or piece.find('{') == -1):\n            key, val = piece.split('=', 1)\n            kwargs[key.strip()] = ast.literal_eval(val.strip())\n            keyword_only = True\n        else:\n            if keyword_only:\n                raise ValueError(\"Positional arguments cannot follow keyword arguments.\")\n            args.append(ast.literal_eval(piece))\n\n    return name, args, kwargs\n</code></pre>"},{"location":"reference/utils/#amisc.utils.relative_error","title":"<code>relative_error(pred, targ, axis=None, skip_nan=False)</code>","text":"<p>Compute the relative L2 error between two vectors along the given axis.</p> PARAMETER DESCRIPTION <code>pred</code> <p>the predicted values</p> <p> </p> <code>targ</code> <p>the target values</p> <p> </p> <code>axis</code> <p>the axis along which to compute the error</p> <p> DEFAULT: <code>None</code> </p> <code>skip_nan</code> <p>whether to skip NaN values in the error calculation</p> <p> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <p>the relative L2 error</p> Source code in <code>src/amisc/utils.py</code> <pre><code>def relative_error(pred, targ, axis=None, skip_nan=False):\n    \"\"\"Compute the relative L2 error between two vectors along the given axis.\n\n    :param pred: the predicted values\n    :param targ: the target values\n    :param axis: the axis along which to compute the error\n    :param skip_nan: whether to skip NaN values in the error calculation\n    :returns: the relative L2 error\n    \"\"\"\n    with np.errstate(divide='ignore', invalid='ignore'):\n        sum_func = np.nansum if skip_nan else np.sum\n        err = np.sqrt(sum_func((pred - targ)**2, axis=axis) / sum_func(targ**2, axis=axis))\n    return np.nan_to_num(err, nan=np.nan, posinf=np.nan, neginf=np.nan)\n</code></pre>"},{"location":"reference/utils/#amisc.utils.search_for_file","title":"<code>search_for_file(filename, search_paths=None)</code>","text":"<p>Search for the given filename in the current working directory and any additional search paths provided.</p> PARAMETER DESCRIPTION <code>filename</code> <p>the filename to search for</p> <p> TYPE: <code>str | Path</code> </p> <code>search_paths</code> <p>paths to try and find the file in</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>the full path to the file if found, otherwise the original <code>filename</code></p> Source code in <code>src/amisc/utils.py</code> <pre><code>def search_for_file(filename: str | Path, search_paths=None):\n    \"\"\"Search for the given filename in the current working directory and any additional search paths provided.\n\n    :param filename: the filename to search for\n    :param search_paths: paths to try and find the file in\n    :returns: the full path to the file if found, otherwise the original `filename`\n    \"\"\"\n    if not isinstance(filename, str | Path):\n        return filename\n\n    search_paths = search_paths or []\n    search_paths.append('.')\n\n    save_file = Path(filename)\n    need_to_search = True\n    try:\n        need_to_search = ((len(save_file.parts) == 1 and len(save_file.suffix) &gt; 0) or\n                          (len(save_file.parts) &gt; 1 and not save_file.exists()))\n    except Exception:\n        need_to_search = False\n\n    # Search for the save file if it was a valid path and does not exist\n    if need_to_search:\n        found_file = False\n        name = save_file.name\n        for path in search_paths:\n            if (pth := Path(path) / name).exists():\n                filename = pth.resolve().as_posix()\n                found_file = True\n                break\n        if not found_file:\n            pass  # Let the caller handle the error (just return the original filename back to caller)\n            # raise FileNotFoundError(f\"Could not find save file '{filename}' in paths: {search_paths}.\")\n\n    return filename\n</code></pre>"},{"location":"reference/utils/#amisc.utils.to_model_dataset","title":"<code>to_model_dataset(dataset, variables, del_latent=True, **field_coords)</code>","text":"<p>Convert surrogate input/output dataset to a form usable by the true model. Primarily, reconstruct field quantities and denormalize.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>the dataset to convert</p> <p> TYPE: <code>Dataset</code> </p> <code>variables</code> <p>the <code>VariableList</code> containing the variable objects used in <code>dataset</code> -- these objects define the normalization and compression methods to use for each variable</p> <p> TYPE: <code>'amisc.variable.VariableList'</code> </p> <code>del_latent</code> <p>whether to delete the latent variables from the dataset after reconstruction</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>field_coords</code> <p>pass in extra field qty coords as f'{var}_coords' for reconstruction (optional)</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple[Dataset, Dataset]</code> <p>the reconstructed/denormalized dataset and any field coordinates used during reconstruction</p> Source code in <code>src/amisc/utils.py</code> <pre><code>def to_model_dataset(dataset: Dataset, variables: 'amisc.variable.VariableList', del_latent: bool = True,\n                     **field_coords) -&gt; tuple[Dataset, Dataset]:\n    \"\"\"Convert surrogate input/output dataset to a form usable by the true model. Primarily, reconstruct\n    field quantities and denormalize.\n\n    :param dataset: the dataset to convert\n    :param variables: the `VariableList` containing the variable objects used in `dataset` -- these objects define\n                      the normalization and compression methods to use for each variable\n    :param del_latent: whether to delete the latent variables from the dataset after reconstruction\n    :param field_coords: pass in extra field qty coords as f'{var}_coords' for reconstruction (optional)\n    :returns: the reconstructed/denormalized dataset and any field coordinates used during reconstruction\n    \"\"\"\n    dataset = copy.deepcopy(dataset)\n    _combine_latent_arrays(dataset)\n\n    ret_coords = {}\n    for var in variables:\n        if var in dataset:\n            if var.compression is not None:\n                # coords = self.model_kwargs.get(f'{var.name}_coords', None)\n                coords = field_coords.get(f'{var}{COORDS_STR_ID}', None)\n                field = var.reconstruct({'latent': dataset[var]}, coords=coords)\n                if del_latent:\n                    del dataset[var]\n                coords = field.pop('coords')\n                ret_coords[f'{var.name}{COORDS_STR_ID}'] = copy.deepcopy(coords)\n                dataset.update(field)\n            else:\n                dataset[var] = var.denormalize(dataset[var])\n\n    return dataset, ret_coords\n</code></pre>"},{"location":"reference/utils/#amisc.utils.to_surrogate_dataset","title":"<code>to_surrogate_dataset(dataset, variables, del_fields=True, **field_coords)</code>","text":"<p>Convert true model input/output dataset to a form usable by the surrogate. Primarily, compress field quantities and normalize.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>the dataset to convert</p> <p> TYPE: <code>Dataset</code> </p> <code>variables</code> <p>the <code>VariableList</code> containing the variable objects used in <code>dataset</code> -- these objects define the normalization and compression methods to use for each variable</p> <p> TYPE: <code>'amisc.variable.VariableList'</code> </p> <code>del_fields</code> <p>whether to delete the original field quantities from the dataset after compression</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>field_coords</code> <p>pass in extra field qty coords as f'{var}_coords' for compression (optional)</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple[Dataset, list[str]]</code> <p>the compressed/normalized dataset and a list of variable names to pass to surrogate</p> Source code in <code>src/amisc/utils.py</code> <pre><code>def to_surrogate_dataset(dataset: Dataset, variables: 'amisc.variable.VariableList', del_fields: bool = True,\n                         **field_coords) -&gt; tuple[Dataset, list[str]]:\n    \"\"\"Convert true model input/output dataset to a form usable by the surrogate. Primarily, compress field\n    quantities and normalize.\n\n    :param dataset: the dataset to convert\n    :param variables: the `VariableList` containing the variable objects used in `dataset` -- these objects define\n                      the normalization and compression methods to use for each variable\n    :param del_fields: whether to delete the original field quantities from the dataset after compression\n    :param field_coords: pass in extra field qty coords as f'{var}_coords' for compression (optional)\n    :returns: the compressed/normalized dataset and a list of variable names to pass to surrogate\n    \"\"\"\n    surr_vars = []\n    dataset = copy.deepcopy(dataset)\n    for var in variables:\n        # Only grab scalars in the dataset or field qtys if all fields are present\n        if var in dataset or (var.compression is not None and all([f in dataset for f in var.compression.fields])):\n            if var.compression is not None:\n                coords = dataset.get(f'{var}{COORDS_STR_ID}', field_coords.get(f'{var}{COORDS_STR_ID}', None))\n                latent = var.compress({field: dataset[field] for field in\n                                       var.compression.fields}, coords=coords)['latent']  # all fields must be present\n                for i in range(latent.shape[-1]):\n                    dataset[f'{var.name}{LATENT_STR_ID}{i}'] = latent[..., i]\n                    surr_vars.append(f'{var.name}{LATENT_STR_ID}{i}')\n                if del_fields:\n                    for field in var.compression.fields:\n                        del dataset[field]\n                    if dataset.get(f'{var}{COORDS_STR_ID}', None) is not None:\n                        del dataset[f'{var}{COORDS_STR_ID}']\n            else:\n                dataset[var.name] = var.normalize(dataset[var.name])\n                surr_vars.append(f'{var.name}')\n\n    return dataset, surr_vars\n</code></pre>"},{"location":"reference/variable/","title":"variable","text":""},{"location":"reference/variable/#amisc.variable","title":"<code>amisc.variable</code>","text":"<p>Provides an object-oriented interface for model inputs/outputs, random variables, scalars, and field quantities.</p> <p>Includes:</p> <ul> <li><code>Variable</code> \u2014 an object that stores information about a variable and includes methods for sampling, pdf evaluation,                normalization, compression, loading from file, etc. Variables can mostly be treated as strings                that have some additional information and utilities attached to them.</li> <li><code>VariableList</code> \u2014 a container for <code>Variables</code> that provides dict-like access of <code>Variables</code> by <code>name</code> along with normal                    indexing and slicing.</li> </ul> <p>The preferred serialization of <code>Variable</code> and <code>VariableList</code> is to/from yaml. This is done by default with the <code>!Variable</code> and <code>!VariableList</code> yaml tags.</p>"},{"location":"reference/variable/#amisc.variable.Variable","title":"<code>Variable(name=None, **kwargs)</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Serializable</code></p> <p>Object for storing information about variables and providing methods for pdf evaluation, sampling, etc. All fields will undergo pydantic validation and conversion to the correct types.</p> <p>A simple variable object can be created with <code>var = Variable()</code>. All initialization options are optional and will be given good defaults. You should probably at the very least give a memorable <code>name</code> and a <code>domain</code>. Variables can mostly be treated as strings with some extra information/utilities attached.</p> <p>With the <code>pyyaml</code> library installed, all <code>Variable</code> objects can be saved or loaded directly from a <code>.yml</code> file by using the <code>!Variable</code> yaml tag (which is loaded by default with <code>amisc</code>).</p> <ul> <li>Use <code>Variable.distribution</code> to specify PDFs, such as for random variables. See the <code>Distribution</code> classes.</li> <li>Use <code>Variable.norm</code> to specify a transformed-space that is more amenable to surrogate construction   (e.g. mapping to the range (0,1)). See the <code>Transform</code> classes.</li> <li>Use <code>Variable.compression</code> to specify high-dimensional, coordinate-based field quantities,   such as from the output of many simulation software programs. See the <code>Compression</code> classes.</li> <li>Use <code>Variable.category</code> as an additional layer for using Variable's in different ways (e.g. set a \"calibration\"   category for Bayesian inference).</li> </ul> <p>Example</p> <pre><code># Random variable\ntemp = Variable(name='T', description='Temperature', units='K', distribution='Uniform(280, 320)')\nsamples = temp.sample(100)\npdf = temp.pdf(samples)\n\n# Field quantity\nvel = Variable(name='u', description='Velocity', units='m/s', compression={'fields': ['ux', 'uy', 'uz']})\nvel_data = ...  # from a simulation\nreduced_vel = vel.compress(vel_data)\n</code></pre> <p>Warning</p> <p>Changes to collection fields (like <code>Variable.norm</code>) should completely reassign the whole collection to trigger the correct validation, rather than editing particular entries. For example, reassign <code>norm=['log', 'linear(2, 2)']</code> rather than editing norm via <code>norm.append('linear(2, 2)')</code>.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>an identifier for the variable, can compare variables directly with strings for indexing purposes</p> <p> TYPE: <code>Optional[str]</code> </p> <code>nominal</code> <p>a typical value for this variable</p> <p> TYPE: <code>Optional[float]</code> </p> <code>description</code> <p>a lengthier description of the variable</p> <p> TYPE: <code>Optional[str]</code> </p> <code>units</code> <p>assumed units for the variable (if applicable)</p> <p> TYPE: <code>Optional[str]</code> </p> <code>category</code> <p>an additional descriptor for how this variable is used, e.g. calibration, operating, design, etc.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>tex</code> <p>latex format for the variable, i.e. \"\\(x_i\\)\"</p> <p> TYPE: <code>Optional[str]</code> </p> <code>compression</code> <p>specifies field quantities and links to relevant compression data</p> <p> TYPE: <code>Optional[str | dict | Compression]</code> </p> <code>distribution</code> <p>a string specifier of a probability distribution function (see the <code>Distribution</code> types)</p> <p> TYPE: <code>Optional[str | Distribution]</code> </p> <code>domain</code> <p>the explicit domain bounds of the variable (limits of where you expect to use it); for field quantities, this is a list of domains for each latent dimension</p> <p> TYPE: <code>Optional[str | tuple[float, float] | list]</code> </p> <code>norm</code> <p>specifier of a map to a transformed-space for surrogate construction (see the <code>Transform</code> types)</p> <p> TYPE: <code>Optional[_TransformLike]</code> </p> Source code in <code>src/amisc/variable.py</code> <pre><code>def __init__(self, /, name=None, **kwargs):\n    # Try to set the variable name if instantiated as \"x = Variable()\"\n    if name is None:\n        name = _inspect_assignment('Variable')\n    name = name or \"X_\" + \"\".join(random.choices(string.digits, k=3))\n    super().__init__(name=name, **kwargs)\n</code></pre>"},{"location":"reference/variable/#amisc.variable.Variable.get_tex","title":"<code>get_tex(units=False, symbol=True)</code>","text":"<p>Return a raw string that is well-formatted for plotting (with latex).</p> PARAMETER DESCRIPTION <code>units</code> <p>whether to include the units in the string</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>symbol</code> <p>just latex symbol if true, otherwise the full description</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>str</code> <p>the latex formatted string</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def get_tex(self, units: bool = False, symbol: bool = True) -&gt; str:\n    \"\"\"Return a raw string that is well-formatted for plotting (with latex).\n\n    :param units: whether to include the units in the string\n    :param symbol: just latex symbol if true, otherwise the full description\n    :returns: the latex formatted string\n    \"\"\"\n    s = (self.tex if symbol else self.description) or self.name\n    return r'{} [{}]'.format(s, self.units or '-') if units else r'{}'.format(s)\n</code></pre>"},{"location":"reference/variable/#amisc.variable.Variable.get_nominal","title":"<code>get_nominal()</code>","text":"<p>Return the nominal value of the variable. Defaults to the mean for a normal distribution or the center of the domain if <code>var.nominal</code> is not specified. Returns a list of nominal values for each latent dimension if this is a field quantity with compression.</p> RETURNS DESCRIPTION <code>float | list | None</code> <p>the nominal value(s)</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def get_nominal(self) -&gt; float | list | None:\n    \"\"\"Return the nominal value of the variable. Defaults to the mean for a normal distribution or the\n    center of the domain if `var.nominal` is not specified. Returns a list of nominal values for each latent\n    dimension if this is a field quantity with compression.\n\n    :returns: the nominal value(s)\n    \"\"\"\n    nominal = self.nominal\n    if nominal is None:\n        if dist := self.distribution:\n            nominal = float(dist.nominal())\n        elif domain := self.get_domain():\n            nominal = [np.mean(d) for d in domain] if isinstance(domain, list) else float(np.mean(domain))\n\n    return nominal\n</code></pre>"},{"location":"reference/variable/#amisc.variable.Variable.get_domain","title":"<code>get_domain()</code>","text":"<p>Return a tuple of the defined domain of this variable. Returns a list of domains for each latent dimension if this is a field quantity with compression.</p> RETURNS DESCRIPTION <code>tuple | list | None</code> <p>the domain(s) of this variable</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def get_domain(self) -&gt; tuple | list | None:\n    \"\"\"Return a tuple of the defined domain of this variable. Returns a list of domains for each latent dimension\n    if this is a field quantity with compression.\n\n    :returns: the domain(s) of this variable\n    \"\"\"\n    if self.domain is None:\n        return None\n    elif isinstance(self.domain, list):\n        return self.domain\n    elif self.compression is not None:\n        # Try to infer a list of domains from compression latent size\n        try:\n            return [self.domain] * self.compression.latent_size()\n        except Exception as e:\n            raise ValueError(f'Variables with `compression` data should return a list of domains, one '\n                             f'for each latent coefficient. Could not infer domain for \"{self.name}\".') from e\n    else:\n        return self.domain\n</code></pre>"},{"location":"reference/variable/#amisc.variable.Variable.sample_domain","title":"<code>sample_domain(shape)</code>","text":"<p>Return an array of the given <code>shape</code> for uniform samples over the domain of this variable. Returns samples for each latent dimension if this is a field quantity with compression.</p> <p>Will always sample uniformly over the normalized surrogate domain if <code>norm</code> is specified, and will return samples in the original unnormalized domain.</p> <p>Note</p> <p>The last dim of the returned samples will be the latent space size for field quantities.</p> PARAMETER DESCRIPTION <code>shape</code> <p>the shape of samples to return</p> <p> TYPE: <code>tuple | int</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>the random samples over the domain of the variable</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def sample_domain(self, shape: tuple | int) -&gt; np.ndarray:\n    \"\"\"Return an array of the given `shape` for uniform samples over the domain of this variable. Returns\n    samples for each latent dimension if this is a field quantity with compression.\n\n    Will always sample uniformly over the normalized surrogate domain if `norm` is specified, and will return\n    samples in the original unnormalized domain.\n\n    !!! Note\n        The last dim of the returned samples will be the latent space size for field quantities.\n\n    :param shape: the shape of samples to return\n    :returns: the random samples over the domain of the variable\n    \"\"\"\n    if isinstance(shape, int):\n        shape = (shape, )\n    if domain := self.get_domain():\n        if isinstance(domain, list):\n            lb = np.atleast_1d([d[0] for d in domain])\n            ub = np.atleast_1d([d[1] for d in domain])\n            return np.random.rand(*shape, 1) * (ub - lb) + lb\n        else:\n            lb, ub = self.normalize(domain)\n            norm_samples = np.random.rand(*shape) * (ub - lb) + lb\n            return self.denormalize(norm_samples)\n    else:\n        raise RuntimeError(f'Variable \"{self.name}\" does not have a domain specified.')\n</code></pre>"},{"location":"reference/variable/#amisc.variable.Variable.update_domain","title":"<code>update_domain(domain, override=False)</code>","text":"<p>Update the domain of this variable by taking the minimum or maximum of the new domain with the current domain for the lower and upper bounds, respectively. Will attempt to update the domain of each latent dimension if this is a field quantity with compression. If the variable has a <code>Uniform</code> distribution, this will update the distribution's bounds too.</p> PARAMETER DESCRIPTION <code>domain</code> <p>the new domain(s) to update with</p> <p> TYPE: <code>tuple[float, float] | list[tuple]</code> </p> <code>override</code> <p>will simply set the domain to the new values rather than update against the current domain; (default <code>False</code>)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amisc/variable.py</code> <pre><code>def update_domain(self, domain: tuple[float, float] | list[tuple], override: bool = False):\n    \"\"\"Update the domain of this variable by taking the minimum or maximum of the new domain with the current domain\n    for the lower and upper bounds, respectively. Will attempt to update the domain of each latent dimension\n    if this is a field quantity with compression. If the variable has a `Uniform` distribution, this will\n    update the distribution's bounds too.\n\n    :param domain: the new domain(s) to update with\n    :param override: will simply set the domain to the new values rather than update against the current domain;\n                     (default `False`)\n    \"\"\"\n    def _update_domain(domain, curr_domain):\n        lb, ub = domain\n        ret = (lb, ub) if override else (min(lb, curr_domain[0]) if curr_domain is not None else lb,\n                                         max(ub, curr_domain[1]) if curr_domain is not None else ub)\n        return tuple(map(float, ret))\n\n    curr_domain = self.get_domain()\n    if isinstance(domain, list):\n        if not isinstance(curr_domain, list):\n            curr_domain = [curr_domain] * len(domain)\n        self.domain = [_update_domain(d, curr_domain[i]) for i, d in enumerate(domain)]\n    elif isinstance(curr_domain, list):\n        if not isinstance(domain, list):\n            domain = [domain] * len(curr_domain)\n        self.domain = [_update_domain(d, curr_domain[i]) for i, d in enumerate(domain)]\n    else:\n        self.domain = _update_domain(domain, curr_domain)\n        if (dist := self.distribution) is not None and isinstance(dist, Uniform | LogUniform):\n            dist.dist_args = self.domain  # keep Uniform dist in sync\n</code></pre>"},{"location":"reference/variable/#amisc.variable.Variable.pdf","title":"<code>pdf(x)</code>","text":"<p>Compute the PDF of the Variable at the given <code>x</code> locations. Will just return one's if the variable does not have a distribution.</p> PARAMETER DESCRIPTION <code>x</code> <p>locations to compute the PDF at</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>the PDF evaluations at <code>x</code></p> Source code in <code>src/amisc/variable.py</code> <pre><code>def pdf(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute the PDF of the Variable at the given `x` locations. Will just return one's if the variable\n    does not have a distribution.\n\n    :param x: locations to compute the PDF at\n    :returns: the PDF evaluations at `x`\n    \"\"\"\n    if dist := self.distribution:\n        return dist.pdf(x)\n    else:\n        return np.ones(x.shape)  # No pdf if no dist is specified\n</code></pre>"},{"location":"reference/variable/#amisc.variable.Variable.sample","title":"<code>sample(shape, nominal=None)</code>","text":"<p>Draw samples from this <code>Variable's</code> distribution. Just returns the nominal value of the given shape if this <code>Variable</code> has no distribution.</p> PARAMETER DESCRIPTION <code>shape</code> <p>the shape of the returned samples</p> <p> TYPE: <code>tuple | int</code> </p> <code>nominal</code> <p>a nominal value to use if applicable (i.e. a center for relative, tolerance, or normal)</p> <p> TYPE: <code>float | ndarray</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>samples from the PDF of this <code>Variable's</code> distribution</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def sample(self, shape: tuple | int, nominal: float | np.ndarray = None) -&gt; np.ndarray:\n    \"\"\"Draw samples from this `Variable's` distribution. Just returns the nominal value of the given shape if\n    this `Variable` has no distribution.\n\n    :param shape: the shape of the returned samples\n    :param nominal: a nominal value to use if applicable (i.e. a center for relative, tolerance, or normal)\n    :returns: samples from the PDF of this `Variable's` distribution\n    \"\"\"\n    if isinstance(shape, int):\n        shape = (shape, )\n    if nominal is None:\n        nominal = self.get_nominal()\n\n    if dist := self.distribution:\n        return dist.sample(shape, nominal)\n    else:\n        # Variable's with no distribution\n        if nominal is None:\n            raise ValueError(f'Cannot sample \"{self.name}\" with no dist or nominal value specified.')\n        elif isinstance(nominal, list | np.ndarray):\n            return np.ones(shape + (len(nominal),)) * np.atleast_1d(nominal)  # For field quantities\n        else:\n            return np.ones(shape) * nominal\n</code></pre>"},{"location":"reference/variable/#amisc.variable.Variable.normalize","title":"<code>normalize(values, denorm=False)</code>","text":"<p>Normalize <code>values</code> based on this <code>Variable's</code> <code>norm</code> method(s). See <code>Transform</code> for available norm methods.</p> <p>Note</p> <p>If this Variable's <code>self.norm</code> was specified as a list of norm methods, then each will be applied in sequence in the original order (and in reverse for <code>denorm=True</code>). When <code>self.distribution</code> is involved in the transforms (only for <code>minmax</code> and <code>zscore</code>), the <code>dist_args</code> will get normalized too at each transform before applying the next transform.</p> PARAMETER DESCRIPTION <code>values</code> <p>the values to normalize (array-like)</p> <p> TYPE: <code>ArrayLike</code> </p> <code>denorm</code> <p>whether to denormalize instead using the inverse of the original normalization method</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ArrayLike | None</code> <p>the normalized (or unnormalized) values</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def normalize(self, values: ArrayLike, denorm: bool = False) -&gt; ArrayLike | None:\n    \"\"\"Normalize `values` based on this `Variable's` `norm` method(s). See `Transform` for available norm methods.\n\n    !!! Note\n        If this Variable's `self.norm` was specified as a list of norm methods, then each will be applied in\n        sequence in the original order (and in reverse for `denorm=True`). When `self.distribution` is involved in\n        the transforms (only for `minmax` and `zscore`), the `dist_args` will get normalized too at each\n        transform before applying the next transform.\n\n    :param values: the values to normalize (array-like)\n    :param denorm: whether to denormalize instead using the inverse of the original normalization method\n    :returns: the normalized (or unnormalized) values\n    \"\"\"\n    if not self.norm or values is None:\n        return values\n    if dist := self.distribution:\n        normal_dist = isinstance(dist, Normal)\n    else:\n        normal_dist = False\n\n    def _normalize_single(values, transform, inverse, domain, dist_args):\n        \"\"\"Do a single transform. Might need to override transform_args depending on the transform.\"\"\"\n        transform_args = None\n        if isinstance(transform, Minmax) and domain:\n            transform_args = domain + transform.transform_args[2:]  # Update minmax bounds\n        elif isinstance(transform, Zscore) and dist_args:\n            transform_args = dist_args                              # Update N(mu, std)\n\n        return transform.transform(values, inverse=inverse, transform_args=transform_args)\n\n    domain = self.get_domain() or ()\n    dist_args = self.distribution.dist_args if normal_dist else []\n    if isinstance(domain, list):\n        domain = ()  # For field quantities, domain is not used in normalization\n\n    if denorm:\n        # First, send domain and dist_args through the forward norm list (up until the last norm)\n        hyperparams = [np.hstack((domain, dist_args))]\n        for i, transform in enumerate(self.norm):\n            domain, dist_args = tuple(hyperparams[i][:2]), tuple(hyperparams[i][2:])\n            hyperparams.append(_normalize_single(hyperparams[i], transform, False, domain, dist_args))\n\n        # Now denormalize in reverse\n        hp_idx = -2\n        for transform in reversed(self.norm):\n            domain, dist_args = tuple(hyperparams[hp_idx][:2]), tuple(hyperparams[hp_idx][2:])\n            values = _normalize_single(values, transform, True, domain, dist_args)\n            hp_idx -= 1\n    else:\n        # Normalize values and hyperparams through the forward norm list\n        hyperparams = np.hstack((domain, dist_args))\n        for transform in self.norm:\n            domain, dist_args = tuple(hyperparams[:2]), tuple(hyperparams[2:])\n            values = _normalize_single(values, transform, denorm, domain, dist_args)\n            hyperparams = _normalize_single(hyperparams, transform, denorm, domain, dist_args)\n\n    return values\n</code></pre>"},{"location":"reference/variable/#amisc.variable.Variable.denormalize","title":"<code>denormalize(values)</code>","text":"<p>Alias for <code>normalize(denorm=True)</code>. See <code>normalize</code> for more details.</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def denormalize(self, values):\n    \"\"\"Alias for `normalize(denorm=True)`. See `normalize` for more details.\"\"\"\n    return self.normalize(values, denorm=True)\n</code></pre>"},{"location":"reference/variable/#amisc.variable.Variable.compress","title":"<code>compress(values, coords=None, reconstruct=False)</code>","text":"<p>Compress or reconstruct field quantity values using this <code>Variable's</code> compression info.</p> <p>Specifying compression values</p> <p>If only one field quantity is associated with this variable, then specify <code>values</code> as <code>dict(coords=..., name=...)</code> for this Variable's <code>name</code>. If <code>coords</code> is not specified, then this assumes the locations are the same as the reconstruction data (and skips interpolation).</p> <p>Compression workflow</p> <p>Generally, compression follows <code>interpolate -&gt; normalize -&gt; compress</code> to take raw values into the compressed \"latent\" space. The interpolation step is required to make sure <code>values</code> align with the coordinates used when building the compression map in the first place (such as through SVD).</p> PARAMETER DESCRIPTION <code>values</code> <p>a <code>dict</code> with a key for each field qty of shape <code>(..., qty.shape)</code> and a <code>coords</code> key of shape <code>(qty.shape, dim)</code> that gives the coordinates of each point. Only a single <code>latent</code> key should be given instead if <code>reconstruct=True</code>.</p> <p> TYPE: <code>CompressionData</code> </p> <code>coords</code> <p>the coordinates of each point in <code>values</code> if <code>values</code> did not contain a <code>coords</code> key; defaults to the compression grid coordinates</p> <p> TYPE: <code>ndarray</code> DEFAULT: <code>None</code> </p> <code>reconstruct</code> <p>whether to reconstruct values instead of compress</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>CompressionData</code> <p>the compressed values with key <code>latent</code> and shape <code>(..., latent_size)</code>; if <code>reconstruct=True</code>, then the reconstructed values with shape <code>(..., qty.shape)</code> for each <code>qty</code> key are returned. The return <code>dict</code> also has a <code>coords</code> key with shape <code>(qty.shape, dim)</code>.</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def compress(self, values: CompressionData, coords: np.ndarray = None,\n             reconstruct: bool = False) -&gt; CompressionData:\n    \"\"\"Compress or reconstruct field quantity values using this `Variable's` compression info.\n\n    !!! Note \"Specifying compression values\"\n        If only one field quantity is associated with this variable, then\n        specify `values` as `dict(coords=..., name=...)` for this Variable's `name`. If `coords` is not specified,\n        then this assumes the locations are the same as the reconstruction data (and skips interpolation).\n\n    !!! Info \"Compression workflow\"\n        Generally, compression follows `interpolate -&gt; normalize -&gt; compress` to take raw values into the compressed\n        \"latent\" space. The interpolation step is required to make sure `values` align with the coordinates used\n        when building the compression map in the first place (such as through SVD).\n\n    :param values: a `dict` with a key for each field qty of shape `(..., qty.shape)` and a `coords` key of shape\n                  `(qty.shape, dim)` that gives the coordinates of each point. Only a single `latent` key should\n                  be given instead if `reconstruct=True`.\n    :param coords: the coordinates of each point in `values` if `values` did not contain a `coords` key;\n                   defaults to the compression grid coordinates\n    :param reconstruct: whether to reconstruct values instead of compress\n    :returns: the compressed values with key `latent` and shape `(..., latent_size)`; if `reconstruct=True`,\n              then the reconstructed values with shape `(..., qty.shape)` for each `qty` key are returned.\n              The return `dict` also has a `coords` key with shape `(qty.shape, dim)`.\n    \"\"\"\n    if not self.compression:\n        raise ValueError(f'Compression is not supported for variable \"{self.name}\". Please specify a compression'\n                         f' method for this variable.')\n    if not self.compression.map_exists:\n        raise ValueError(f'Compression map not computed yet for \"{self.name}\".')\n\n    # Default field coordinates to the compression coordinates if they are not provided\n    field_coords = values.pop('coords', coords)\n    if field_coords is None:\n        field_coords = self.compression.coords\n    ret_dict = {'coords': field_coords}\n\n    # For reconstruction: decompress -&gt; denormalize -&gt; interpolate\n    if reconstruct:\n        try:\n            states = np.atleast_1d(values['latent'])    # (..., rank)\n        except KeyError as e:\n            raise ValueError('Must pass values[\"latent\"] in for reconstruction.') from e\n        states = self.compression.reconstruct(states)   # (..., dof)\n        states = self.denormalize(states)               # (..., dof)\n        states = self.compression.interpolate_from_grid(states, field_coords)\n        ret_dict.update(states)\n\n    # For compression: interpolate -&gt; normalize -&gt; compress\n    else:\n        states = self.compression.interpolate_to_grid(field_coords, values)\n        states = self.normalize(states)                 # (..., dof)\n        states = self.compression.compress(states)      # (..., rank)\n        ret_dict['latent'] = states\n\n    return ret_dict\n</code></pre>"},{"location":"reference/variable/#amisc.variable.Variable.reconstruct","title":"<code>reconstruct(values, coords=None)</code>","text":"<p>Alias for <code>compress(reconstruct=True)</code>. See <code>compress</code> for more details.</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def reconstruct(self, values, coords=None):\n    \"\"\"Alias for `compress(reconstruct=True)`. See `compress` for more details.\"\"\"\n    return self.compress(values, coords=coords, reconstruct=True)\n</code></pre>"},{"location":"reference/variable/#amisc.variable.Variable.serialize","title":"<code>serialize(save_path='.')</code>","text":"<p>Convert a <code>Variable</code> to a <code>dict</code> with only standard Python types (i.e. convert custom objects like <code>dist</code> and <code>norm</code> to strings and save <code>compression</code> to a <code>.pkl</code>).</p> PARAMETER DESCRIPTION <code>save_path</code> <p>the path to save the compression data to (defaults to current directory)</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>'.'</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>the serialized <code>dict</code> of the <code>Variable</code> object</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def serialize(self, save_path: str | Path = '.') -&gt; dict:\n    \"\"\"Convert a `Variable` to a `dict` with only standard Python types\n    (i.e. convert custom objects like `dist` and `norm` to strings and save `compression` to a `.pkl`).\n\n    :param save_path: the path to save the compression data to (defaults to current directory)\n    :returns: the serialized `dict` of the `Variable` object\n    \"\"\"\n    d = {}\n    for key, value in self.__dict__.items():\n        if value is not None and not key.startswith('_'):\n            if key == 'domain':\n                d[key] = [str(v) for v in value] if isinstance(value, list) else str(value)\n            elif key == 'distribution':\n                d[key] = str(value)\n            elif key == 'norm':\n                d[key] = [str(transform) for transform in value]\n            elif key == 'compression':\n                fname = f'{self.name}_compression.pkl'\n                d[key] = value.serialize(save_path=Path(save_path) / fname)\n            else:\n                d[key] = value\n    return d\n</code></pre>"},{"location":"reference/variable/#amisc.variable.Variable.deserialize","title":"<code>deserialize(data, search_paths=None)</code>  <code>classmethod</code>","text":"<p>Convert a <code>dict</code> to a <code>Variable</code> object. Let <code>pydantic</code> handle validation and conversion of fields.</p> PARAMETER DESCRIPTION <code>data</code> <p>the <code>dict</code> to convert to a <code>Variable</code></p> <p> TYPE: <code>dict</code> </p> <code>search_paths</code> <p>the paths to search for compression files (if necessary)</p> <p> TYPE: <code>list[str | Path]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Variable</code> <p>the <code>Variable</code> object</p> Source code in <code>src/amisc/variable.py</code> <pre><code>@classmethod\ndef deserialize(cls, data: dict, search_paths: list[str | Path] = None) -&gt; Variable:\n    \"\"\"Convert a `dict` to a `Variable` object. Let `pydantic` handle validation and conversion of fields.\n\n    :param data: the `dict` to convert to a `Variable`\n    :param search_paths: the paths to search for compression files (if necessary)\n    :returns: the `Variable` object\n    \"\"\"\n    if isinstance(data, Variable):\n        return data\n    elif isinstance(data, str):\n        return cls(name=data)\n    else:\n        if (compression := data.get('compression', None)) is not None:\n            if isinstance(compression, str):\n                data['compression'] = search_for_file(compression, search_paths=search_paths)\n        return cls(**data)\n</code></pre>"},{"location":"reference/variable/#amisc.variable.VariableList","title":"<code>VariableList(data=None, **kwargs)</code>","text":"<p>               Bases: <code>OrderedDict</code>, <code>Serializable</code></p> <p>Store <code>Variables</code> as <code>str(var) : Variable</code> in the order they were passed in. You can:</p> <ul> <li>Initialize/update from a single <code>Variable</code> or a list of <code>Variables</code></li> <li>Get/set a <code>Variable</code> directly or by name via <code>my_vars[var]</code> or <code>my_vars[str(var)]</code> etc.</li> <li>Retrieve the original order of insertion by <code>list(my_vars.items())</code></li> <li>Access/delete elements by order of insertion using integer/slice indexing (i.e. <code>my_vars[1:3]</code>)</li> <li>Save/load from yaml file using the <code>!VariableList</code> tag</li> </ul> <p>Initialize a collection of <code>Variable</code> objects.</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def __init__(self, data: list[Variable] | Variable | OrderedDict | dict = None, **kwargs):\n    \"\"\"Initialize a collection of `Variable` objects.\"\"\"\n    super().__init__()\n    self.update(data, **kwargs)\n</code></pre>"},{"location":"reference/variable/#amisc.variable.VariableList.get_domains","title":"<code>get_domains(norm=True)</code>","text":"<p>Get normalized variable domains (expand latent coefficient domains for field quantities). Assume a domain of <code>(0, 1)</code> for variables if their domain is not specified.</p> PARAMETER DESCRIPTION <code>norm</code> <p>whether to normalize the domains using <code>Variable.norm</code> (useful for getting bds for surrogate); latent coefficient domains do not get normalized</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <p>a <code>dict</code> of variables to their normalized domains; field quantities return a domain for each of their latent coefficients</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def get_domains(self, norm: bool = True):\n    \"\"\"Get normalized variable domains (expand latent coefficient domains for field quantities). Assume a\n    domain of `(0, 1)` for variables if their domain is not specified.\n\n    :param norm: whether to normalize the domains using `Variable.norm` (useful for getting bds for surrogate);\n                 latent coefficient domains do not get normalized\n    :returns: a `dict` of variables to their normalized domains; field quantities return a domain for each\n              of their latent coefficients\n    \"\"\"\n    domains = {}\n    for var in self:\n        var_domain = var.get_domain()\n        if isinstance(var_domain, list):  # only field qtys return a list of domains, one for each latent coeff\n            for i, domain in enumerate(var_domain):\n                domains[f'{var.name}{LATENT_STR_ID}{i}'] = domain\n        elif var_domain is None:\n            domains[var.name] = (0, 1)\n        else:\n            domains[var.name] = var.normalize(var_domain) if norm else var_domain\n    return domains\n</code></pre>"},{"location":"reference/variable/#amisc.variable.VariableList.get_pdfs","title":"<code>get_pdfs(norm=True)</code>","text":"<p>Get callable pdfs for all variables (skipping field quantities for now)</p> PARAMETER DESCRIPTION <code>norm</code> <p>whether values passed to the pdf functions are normalized and should be denormed first before pdf evaluation (useful for surrogate construction where samples are gathered in the normalized space)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <p>a <code>dict</code> of variables to callable pdf functions; field quantities are skipped.</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def get_pdfs(self, norm: bool = True):\n    \"\"\"Get callable pdfs for all variables (skipping field quantities for now)\n\n    :param norm: whether values passed to the pdf functions are normalized and should be denormed first\n                 before pdf evaluation (useful for surrogate construction where samples are gathered in the\n                 normalized space)\n    :returns: a `dict` of variables to callable pdf functions; field quantities are skipped.\n    \"\"\"\n    def _get_pdf(var, norm):\n        return lambda z: var.pdf(var.denormalize(z) if norm else z)\n\n    pdf_fcns = {}\n    for var in self:\n        var_domain = var.get_domain()\n        if isinstance(var_domain, list):  # only field qtys return a list of domains, one for each latent coeff\n            # for i, domain in enumerate(var_domain):\n                # pdf_fcns[f'{var.name}{LATENT_STR_ID}{i}'] = var.latent_pdfs[i]  TODO: Implement latent pdfs\n            pass\n        else:\n            pdf_fcns[var.name] = _get_pdf(var, norm)\n    return pdf_fcns\n</code></pre>"},{"location":"reference/variable/#amisc.variable.VariableList.update","title":"<code>update(data=None, **kwargs)</code>","text":"<p>Update from a list or dict of <code>Variable</code> objects, or from <code>key=value</code> pairs.</p> Source code in <code>src/amisc/variable.py</code> <pre><code>def update(self, data: list[Variable | str] | str | Variable | OrderedDict | dict = None, **kwargs):\n    \"\"\"Update from a list or dict of `Variable` objects, or from `key=value` pairs.\"\"\"\n    if data:\n        if isinstance(data, OrderedDict | dict):\n            for key, value in data.items():\n                self.__setitem__(key, value)\n        else:\n            data = [data] if not isinstance(data, list | tuple) else data\n            for variable in data:\n                self.__setitem__(str(variable), variable)\n    if kwargs:\n        for key, value in kwargs.items():\n            self.__setitem__(key, value)\n</code></pre>"},{"location":"reference/variable/#amisc.variable.VariableList.get","title":"<code>get(key, default=None)</code>","text":"<p>Make sure this passes through <code>__getitem__()</code></p> Source code in <code>src/amisc/variable.py</code> <pre><code>def get(self, key, default=None):\n    \"\"\"Make sure this passes through `__getitem__()`\"\"\"\n    try:\n        return self.__getitem__(key)\n    except Exception:\n        return default\n</code></pre>"},{"location":"reference/variable/#amisc.variable.VariableList.serialize","title":"<code>serialize(save_path='.')</code>","text":"<p>Convert to a list of <code>dict</code> objects for each <code>Variable</code> in the list.</p> PARAMETER DESCRIPTION <code>save_path</code> <p>the path to save the compression data to (defaults to current directory)</p> <p> DEFAULT: <code>'.'</code> </p> Source code in <code>src/amisc/variable.py</code> <pre><code>def serialize(self, save_path='.') -&gt; list[dict]:\n    \"\"\"Convert to a list of `dict` objects for each `Variable` in the list.\n\n    :param save_path: the path to save the compression data to (defaults to current directory)\n    \"\"\"\n    return [var.serialize(save_path=save_path) for var in self.values()]\n</code></pre>"},{"location":"reference/variable/#amisc.variable.VariableList.merge","title":"<code>merge(*variable_lists)</code>  <code>classmethod</code>","text":"<p>Merge multiple sets of variables into a single <code>VariableList</code> object.</p> <p>Note</p> <p>Variables with the same name will be merged by keeping the one with the most information provided.</p> PARAMETER DESCRIPTION <code>variable_lists</code> <p>the variables/lists to merge</p> <p> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>VariableList</code> <p>the merged <code>VariableList</code> object</p> Source code in <code>src/amisc/variable.py</code> <pre><code>@classmethod\ndef merge(cls, *variable_lists) -&gt; VariableList:\n    \"\"\"Merge multiple sets of variables into a single `VariableList` object.\n\n    !!! Note\n        Variables with the same name will be merged by keeping the one with the most information provided.\n\n    :param variable_lists: the variables/lists to merge\n    :returns: the merged `VariableList` object\n    \"\"\"\n    merged_vars = cls()\n\n    def _get_best_variable(var1, var2):\n        var1_dict = {key: value for key, value in var1.__dict__.items() if value is not None}\n        var2_dict = {key: value for key, value in var2.__dict__.items() if value is not None}\n        return var1 if len(var1_dict) &gt;= len(var2_dict) else var2\n\n    for var_list in variable_lists:\n        for var in cls(var_list):\n            if var.name in merged_vars:\n                merged_vars[var.name] = _get_best_variable(merged_vars[var.name], var)\n            else:\n                merged_vars[var.name] = var\n\n    return merged_vars\n</code></pre>"},{"location":"reference/variable/#amisc.variable.VariableList.deserialize","title":"<code>deserialize(data, search_paths=None)</code>  <code>classmethod</code>","text":"<p>Convert a <code>dict</code> or list of <code>dict</code> objects to a <code>VariableList</code> object. Let <code>pydantic</code> handle validation.</p> Source code in <code>src/amisc/variable.py</code> <pre><code>@classmethod\ndef deserialize(cls, data: dict | list[dict], search_paths=None) -&gt; VariableList:\n    \"\"\"Convert a `dict` or list of `dict` objects to a `VariableList` object. Let `pydantic` handle validation.\"\"\"\n    if not isinstance(data, list):\n        data = [data]\n    return cls([Variable.deserialize(d, search_paths=search_paths) for d in data])\n</code></pre>"},{"location":"theory/feedback_cycles/","title":"Feedback cycles","text":"<p>The <code>amisc</code> library handles multidisciplinary systems with feedback coupling using a fixed-point iteration (FPI) routine. This approach is crucial for solving systems where the outputs of some components are inputs to others, and vice versa, forming cycles of dependencies.</p>"},{"location":"theory/feedback_cycles/#feedback-coupling","title":"Feedback coupling","text":"<p>In multidisciplinary systems, feedback coupling occurs when the outputs of one component influence the inputs of another and vice versa, forming cycles. These cycles can be represented as strongly-connected components (SCCs) in a graph-theoretic representation of the system. Each SCC represents a group of components that are interdependent.</p> <p>Example</p> <p>In the following system of 3 nonlinear equations: \\(f_1, f_2, f_3\\), the \\((f_2, f_3)\\) group forms an SCC with coupling variables \\((y_2, y_3)\\). Viewing the condensation of the system graph using the <code>networkx</code> library highlights this feedback cycle. <pre><code>from amisc import System\nimport networkx as nx\n\ndef f1(x):\n    y1 = x ** 2\n    return y1\n\ndef f2(y1, y3):\n    y2 = y1 + np.sin(y3)\n    return y2\n\ndef f3(y2):\n    y3 = np.cos(y2)\n    return y3\n\nsystem = System(f1, f2, f3)\ndag = nx.condensation(system.graph())  # Group SCCs to form a directed-acyclic-graph\n\nfor idx, node in dag.nodes.items():\n    print(node['members'])\n\n# gives 'f1' and ['f2', 'f3']\n</code></pre></p>"},{"location":"theory/feedback_cycles/#fixed-point-iteration-fpi","title":"Fixed-Point Iteration (FPI)","text":"<p>An FPI routine is employed to solve for the coupling variables within each SCC. The process involves iteratively updating the coupling variables until convergence is achieved. The steps are as follows:</p> <ol> <li> <p>Initialization - Begin with an initial guess for the coupling variables \\(\\xi^{(0)}\\) within the SCC.</p> </li> <li> <p>Iterative update - For each iteration \\( i \\), update the coupling variables \\(\\xi^{(i)}\\) using the function \\( F(\\xi^{(i-1)}) \\), which evaluates the component models in the SCC using the current estimates of the coupling variables:         \\begin{equation}         \\xi^{(i)} = F(\\xi^{(i-1)}).         \\end{equation}</p> </li> <li>Convergence check - Continue iterating until the change in the coupling variables is below a specified tolerance, i.e., \\( \\lVert\\xi^{(i)} - \\xi^{(i-1)}\\rVert &lt; \\eta \\), where \\(\\eta\\) is the convergence tolerance.</li> </ol>"},{"location":"theory/feedback_cycles/#usage-in-amisc","title":"Usage in <code>amisc</code>","text":"<p>When using <code>amisc</code> surrogates for prediction, the system's graph structure is divided into SCCs, and each SCC is solved independently using the FPI routine with the surrogate models. The feedback dependencies are automatically managed under-the-hood during prediction, and <code>amisc</code> allows the user to specify some keywords to fine-tune the FPI routine, including the convergence tolerance \\(\\eta\\) and the maximum number of iterations.</p> <p>Example</p> <p>Using the three-component system of \\((f_1, f_2, f_3)\\) defined above, predictions are obtained with the surrogate <code>predict()</code> method, with FPI options passed as keywords. <pre><code>system = System(f1, f2, f3)  # defined in previous example\n\nsystem.predict({'x': 0.5}, max_fpi_iter=100, fpi_tol=1e-10, andersom_mem=5)\n</code></pre></p> <p>Note that the <code>System</code> implements the Anderson acceleration algorithm for enhancing the convergence of FPI.</p>"},{"location":"theory/overview/","title":"Overview","text":"<p>The <code>amisc</code> Python library provides tools for constructing multi-fidelity surrogates for multidisciplinary systems, based on methodologies outlined in the paper titled</p> <p>\"Adaptive Experimental Design for Multi-Fidelity Surrogate Modeling of Multi-Disciplinary Systems\" </p> <p>by Jakeman et al. This library leverages the Multi-Index Stochastic Collocation (MISC) approach to efficiently approximate complex system models by combining evaluations of varying fidelity. </p> <p>A \"multi-index\" is a tuple of whole numbers that defines a set of fidelity levels for a model, ranging from the lowest fidelity \\((0, 0, \\dots)\\) to the highest fidelity \\((N_1, N_2, \\dots)\\). \"Stochastic collocation\" refers to the process of choosing collocation points (i.e. knots, grid points, training data, etc.) for a set of random variable inputs for the purposes of function approximation or moment estimation.</p> <p>Model fidelity is collectively defined by a pair of multi-indices:</p> <ul> <li>\\(\\alpha\\) - Specifies the deterministic fidelity of a model. It controls the mesh size, time step, and other hyper-parameters that influence the physical model's resolution and accuracy.</li> <li>\\(\\beta\\) - Specifies the parametric fidelity of a surrogate. It dictates the number of samples used to construct the surrogate and the complexity of the surrogate itself, impacting its computational cost and accuracy.</li> </ul>"},{"location":"theory/overview/#key-ideas","title":"Key Ideas","text":"<ul> <li>Multi-Index Stochastic Collocation (MISC) - The library implements MISC to create multi-fidelity surrogates by combining multiple model fidelities. The surrogate model for each component \\(k\\) in a multidisciplinary system is expressed as a linear combination of multiple surrogates:</li> </ul> <p>\\begin{equation}   f_{k}(x) \\approx \\sum_{(\\alpha, \\beta)\\in\\mathcal{I_k}} c_{k, (\\alpha, \\beta)} f_{k, (\\alpha, \\beta)}(x).   \\end{equation}</p> <p>Here, \\(f_{k, (\\alpha, \\beta)}(x)\\) represents the single-fidelity surrogate for a given fidelity level \\((\\alpha, \\beta)\\), and \\(\\mathcal{I}_k\\) is a set of concatenated multi-indices specifying different fidelities.</p> <ul> <li> <p>Downward-closed index set - The set \\(\\mathcal{I}_k\\) must be downward-closed to ensure that if \\((\\gamma, \\delta) \\leq (\\alpha, \\beta)\\) and \\((\\alpha, \\beta) \\in \\mathcal{I}_k\\), then \\((\\gamma, \\delta) \\in \\mathcal{I}_k\\).</p> </li> <li> <p>Combination coefficients - The coefficients for the MISC approximation are calculated using:</p> </li> </ul> <p>\\begin{equation}   c_{k, (\\alpha, \\beta)} = \\sum_{(\\alpha + i, \\beta + j)\\in \\mathcal{I}_k} (-1)^{\\lVert i, j\\rVert _1},   \\end{equation}</p> <p>for all \\((i, j)\\in(0, 1)^{\\mathrm{len}(\\alpha, \\beta)}\\). That is, we sum over every \\((\\alpha, \\beta)\\) in the index set for which the lower neighbor is also in the index set (i.e. subtract one from every index number and check if it is in \\(\\mathcal{I}_k\\)).</p> <p>This formula ensures efficient computation of the surrogate model by balancing deterministic and parametric errors.</p> <ul> <li> <p>Adaptive training - The adaptive training procedure involves refining the surrogate model by activating indices and searching for new refinement directions:</p> <ul> <li>Activated Indices - These are the indices in the set \\(\\mathcal{I}_k\\) that have been selected for inclusion in the surrogate model. The refinement process begins by identifying the index with the largest error indicator from the set of candidate indices.</li> <li>Candidate Indices - These indices are the nearest forward neighbors of the active indices and represent potential directions for refinement. The algorithm searches over these indices to identify new refinement directions that can improve the surrogate model's accuracy. Each candidate index is evaluated based on its potential to reduce the error in the system-level quantities of interest.</li> <li>Refinement - Once a candidate index is selected, it is added to the activated set, and new training samples are generated. This iterative process continues, dynamically allocating computational resources to the most impactful components for minimizing prediction error.</li> </ul> </li> </ul>"},{"location":"theory/overview/#further-reading","title":"Further Reading","text":"<p>For a comprehensive understanding of the methodologies and theoretical underpinnings, please refer to the original paper by Jakeman et al., available at DOI: 10.1002/nme.6958.</p>"},{"location":"theory/polynomials/","title":"Lagrange polynomials","text":"<p>Lagrange polynomials are used in <code>amisc</code> for constructing a smooth interpolation surface over a set of training data. The Lagrange polynomial is a fundamental tool in numerical interpolation, allowing for polynomial approximation of functions based on known data points.</p>"},{"location":"theory/polynomials/#univariate-lagrange-polynomials","title":"Univariate Lagrange polynomials","text":"<p>For a set of \\( n+1 \\) distinct data points \\((x_0, y_0), (x_1, y_1), \\ldots, (x_n, y_n)\\), the Lagrange polynomial \\( L(x) \\) is defined as:</p> \\[ L(x) = \\sum_{i=0}^{n} y_i \\ell_i(x) \\] <p>where each basis polynomial \\(\\ell_i(x)\\) is given by:</p> \\[ \\ell_i(x) = \\prod_{\\substack{0 \\le j \\le n \\\\ j \\neq i}} \\frac{x - x_j}{x_i - x_j} \\] <p>This formula ensures that \\( \\ell_i(x_j) = \\delta_{ij} \\), where \\(\\delta_{ij}\\) is the Kronecker delta, making \\(\\ell_i(x)\\) equal to 1 at \\( x_i \\) and 0 at all other \\( x_j \\).</p>"},{"location":"theory/polynomials/#multivariate-lagrange-polynomials","title":"Multivariate Lagrange polynomials","text":"<p>In higher dimensions, the interpolation of a function over a grid of points can be achieved using a tensor-product extension of the univariate Lagrange polynomials. For a multi-dimensional input \\( \\mathbf{x} = (x_1, x_2, \\ldots, x_d) \\), the tensor-product Lagrange polynomial \\( L(\\mathbf{x}) \\) is constructed as follows:</p> \\[ L(\\mathbf{x}) = \\sum_{i_1=0}^{n_1} \\sum_{i_2=0}^{n_2} \\cdots \\sum_{i_d=0}^{n_d} f_{i_1,i_2,\\ldots,i_d} \\prod_{j=1}^{d} \\ell_{i_j}(x_j) \\] <p>where each \\(\\ell_{i_j}(x_j)\\) is the univariate Lagrange basis polynomial for the \\( j \\)-th dimension, and \\( f_{i_1,i_2,\\ldots,i_d} \\) are the values of the function at the grid points in the multi-dimensional space.</p>"},{"location":"theory/polynomials/#usage-in-amisc","title":"Usage in <code>amisc</code>","text":"<p>In the context of <code>amisc</code>, Lagrange polynomials are used to construct single-fidelity surrogates \\(f_{k, (\\alpha, \\beta)}(x)\\) by evaluating the \\(\\alpha\\)-fidelity model at a Cartesian grid defined on the parametric domain. This approach efficiently interpolates data in multiple dimensions, taking advantage of the structure provided by tensor-product grids.</p>"}]}